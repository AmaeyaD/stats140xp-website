{
  "hash": "8c1de42627cb36c68d7589bb5f34e7ed",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stats 102C, Homework 2 - Intro to Bayesian Statistics\"\nauthor: \"Amaeya Deshpande\"\noutput:\n  pdf_document: default\n  html_document: default\n---\n\n\n\n\n\nHomework Questions and text copyright Miles Chen. For personal use only. Do not post or distribute without permission.\n\n**Do not post your solutions online on a site like github. Violations will be reported.**\n\n# Homework 2 Requirements\n\nThere is no separate instruction file. This file is the instructions and you will modify it for your submission.\n\nYou will submit two files.\n\n1.  `102c_hw2_output_First_Last.pdf` Your output file. You will submit the output to Gradescope. This is the primary file that will be graded. **Make sure all requested output is visible in the output file.**\n\n2.  `102c_hw2_output_First_Last.Rmd` Take this R Markdown file and make the necessary edits so that it generates the requested output. You will submit the Rmd file to CCLE.\n\n## Academic Integrity\n\nModifying the following statement with your name.\n\n\"By including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students.\"\n\n## Reading\n\nReading is important!\n\n-   Chapter 2 section 2 in Introducing Monte Carlo Methods with R\n-   Chapter 3 sections 1-3 in Introducing Monte Carlo Methods with R\n\n## Problem 1 - Estimate pi (poorly)\n\nA simple Monte Carlo Exercise ... Get an estimate of pi by using random uniform numbers. (It won't be a very good estimate.)\n\nIn this first exercise, we can see how a simple source of randomness (in our case, R's `runif()` function) can be used to estimate tough quantities.\n\nWe will find an estimate of pi by estimating the ratio between the area of a circle and its encompassing square.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- seq(-1, 1, by = 0.001)\nposf <- sqrt(1 - s ^ 2)\nplot(s, posf, type = \"l\", asp = 1, ylim = c(-1, 1))\nlines(s, -1 * posf)\nsegments(-1, -1, -1, 1)\nsegments(-1, -1, 1, -1)\nsegments(1, 1, -1, 1)\nsegments(1, 1, 1, -1)\n```\n\n::: {.cell-output-display}\n![](102c_hw2_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nTo calculate the area of the circle analytically, we would need to integrate the function drawing the upper semi-circle and then multiply that by 2. This process requires the use of trig substitutions, and while doable, can illustrate a time where the analytic solution is not easy.\n\n$$Area = 2 \\times \\int_{-1}^1 \\sqrt{1 - x^2} dx$$\n\nFor the Monte-Carlo approach, we will use `runif(n, min = -1, max=1)` to generate a bunch of random pairs of x and y coordinates. We will see how many of those random uniform points fall within the circle. This is easy - just see if $x^2 + y^2 \\le 1$. The total area of the square is 4. The total area of the circle is pi. Thus, the proportion of coordinates that satisfy the inequality $x^2 + y^2 \\le 1 \\approx \\pi/4$.\n\nInstructions:\n\n-   create a vector x of n random values between -1 and 1. I suggest starting with n = 500\n-   create a vector y of n random values between -1 and 1. Use the two vectors to make coordinate pairs.\n-   calculate which of points satisfy the inequality for falling inside the circle.\n-   Print out your estimate of pi by multiplying the proportion by 4.\n-   plot each of those (x,y) coordinate pairs. Use pch = 20. Color the points based on whether they fall in the circle or not.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nx <- runif(500, min = -1, max = 1)\ny <- runif(500, min = -1, max = 1)\ninside <- x^2 + y^2 <= 1\nprop_inside <- sum(inside) / 500\npi_est <- prop_inside * 4\nprint(paste(\"Estimate for pi: \", pi_est))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for pi:  3.2\"\n```\n\n\n:::\n\n```{.r .cell-code}\ns <- seq(-1, 1, by = 0.001)\nposf <- sqrt(1 - s ^ 2)\nplot(s, posf, type = \"l\", asp = 1, ylim = c(-1, 1))\nlines(s, -1 * posf)\nsegments(-1, -1, -1, 1)\nsegments(-1, -1, 1, -1)\nsegments(1, 1, -1, 1)\nsegments(1, 1, 1, -1)\npoints(x, y, pch = 20, col = ifelse(inside, \"green\", \"red\"))\n```\n\n::: {.cell-output-display}\n![](102c_hw2_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n# Monte Carlo Integration\n\n## Problem 2\n\nEstimate the integral of the following function by using Monte Carlo integration.\n\n$$I = \\int_0^5 h(x) dx = \\int_0^5 \\exp(-0.5 (x-2)^2 - 0.1 |\\sin(2x)|) dx$$\n\nWe want to estimate the value of I, the area under the curve from 0 to 5. We can say that area is equal to the average value of $h(x)$ times the width:\n\n$$I = 5 \\cdot \\mathbb{E}_f[h(X)]$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# what the function looks like\nh <- function(x) {exp(-0.5 * (x - 2) ^ 2 - 0.1 * abs( sin(2 * x) ) )}\nv <- seq(0, 5, by = 0.01)\nplot(v, h(v), type = \"l\", xlim = c(-0.5, 5.5))\npolygon(c(0, v, 5), c(0, h(v), 0), col = 'khaki')\nrect_ht = 0.458394 # height of rectangle is the Expected value of h(x)\npolygon(c(0, 0, 5, 5), c(0, rect_ht, rect_ht, 0), col = rgb(0, 0, 1, .2))\n```\n\n::: {.cell-output-display}\n![](102c_hw2_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nFor MC integration, we estimate\n\n$$\\mathbb{E}_f[h(X)] = \\int_\\mathcal{X} h(x)f(x) dx$$\n\nFor this first problem, we will use the uniform distribution on the interval (0,5) to draw samples of x. Thus, the PDF, $f(x)$ is\n\n$$f(x) = 1/5$$\n\nThus,\n\n$$ I  = 5 \\cdot \\mathbb{E}_f[h(X)] = 5 \\cdot \\int_0^5 h(x) f(x) dx \\approx \\frac{5}{N}\\sum_{j = 1}^N h(x_j) = \\hat I$$\n\nWhere $x_j \\sim \\text{Unif}(0,5)$\n\nGenerate a sample of 5000 values of x to estimate $\\hat{I}$. Use `runif()` to generate the random uniform values.\n\nCreate a plot using the cumulative mean (aka running mean) of the first 500 values in the sample to show how the estimate of the integral 'settles' over the course of the sampling.\n\nDo this two more times (each with a different starting seed), for a total of three samples of 5000 values each. Plot the cumulative mean of the first 500 values of each sample. There will be three different lines on this plot.\n\nIn your plot also include the 'true' value of the integral, 2.29583, as a horizontal line.\n\nPrint out your three estimates of $I$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\nh <- function(x) {exp(-0.5 * (x - 2) ^ 2 - 0.1 * abs( sin(2 * x) ) )}\n\n## Monte Carlo Integration using uniform random sampling\nn = 5000 # total number of values to generate\nk = 500  # number of values to plot\n\n# First series\nset.seed(1)\nx1 <- runif(n, min = 0, max = 5)\nh_vals1 <- h(x1)\nI_est1 <- (5 / n) * sum(h_vals1)\ncum_mean1 <- cummean(h_vals1[1:k]) * 5\n\n# second series\nset.seed(2)\nx2 <- runif(n, min = 0, max = 5)\nh_vals2 <- h(x2)\nI_est2 <- (5 / n) * sum(h_vals2)\ncum_mean2 <- cummean(h_vals2[1:k]) * 5\n\n# third series\nset.seed(3)\nx3 <- runif(n, min = 0, max = 5)\nh_vals3 <- h(x3)\nI_est3 <- (5 / n) * sum(h_vals3)\ncum_mean3 <- cummean(h_vals3[1:k]) * 5\n\nprint(paste(\"Estimate for I with seed 1:\", I_est1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for I with seed 1: 2.27626784048365\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Estimate for I with seed 2:\", I_est2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for I with seed 2: 2.26402222241221\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Estimate for I with seed 3:\", I_est3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for I with seed 3: 2.29369518890943\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x = c(1, 500), y = c(2.29583, 2.29583), type = \"l\", ylim = c(0, 5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:500, y = cum_mean1, col = \"blue\")\nlines(x = 1:500, y = cum_mean2, col = \"red\")\nlines(x = 1:500, y = cum_mean3, col = \"green\")\nlegend(\"topright\", legend = c(\"Estimate 1\", \"Estimate 2\", \"Estimate 3\", \n                  \"True Value\"), col = c(\"blue\", \"red\", \"green\", \"black\"), lty = 1)\n```\n\n::: {.cell-output-display}\n![](102c_hw2_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## Problem 2b\n\nWe can estimate the variance of $\\bar h_n$ with $v_n = \\frac{1}{n^2}\\sum_{j=1}^N [ h(x_j) - \\bar h_n]^2$.\n\nWhat is variance of the estimate $\\hat I$?\n\n*Write the formula for estimated variance of I-hat with latex:*\n\n$$Var(\\hat I) = \\frac{25}{n^2}\\sum_{j=1}^N [ h(x_j) - \\bar h_n]^2$$\n\nLook at your first Monte Carlo series and estimate the running variance of $\\hat I$ as n goes from 1 to 500.\n\nCreate a plot of the $\\hat I$ with 95% confidence bounds above and below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncum_est <- cum_mean1[1:500]\ncum_var <- numeric(500)\nfor (i in 1:500) {\n  cum_var[i] <- var(h_vals1[1:i]) * 25\n}\nupper_bound <- cum_est + qnorm(0.975) * sqrt(cum_var / 1:500)\nlower_bound <- cum_est + qnorm(0.025) * sqrt(cum_var / 1:500)\nplot(x = 1:500, y = cum_est, type = \"l\", ylim = c(0, 5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:500, y = upper_bound, col = \"blue\")\nlines(x = 1:500, y = lower_bound, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](102c_hw2_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n# Importance Sampling\n\nWith importance sampling, we won't draw from the distribution $f(x)$, but from a proposal distribution $g(x)$. This can be useful if we don't know how to draw samples directly from $f(x)$.\n\nWe revisit the previous problem with importance sampling. Even though it is easy to draw from $f(x)$, which is the uniform distribution, we may see that it can be advantageous to draw from a different distribution that more closely resembles the target function.\n\nWe will use something that looks like the normal distribution N(2, 1) as the trial distribution to estimate the same integral by importance sampling.\n\n$$I = \\int_0^5 \\exp(-0.5 (x-2)^2 - 0.1 |\\sin(2x)|) dx$$\n\n$$I = 5 \\cdot  \\mathbb{E}_f[h(X)]  = 5 \\cdot \\int_0^5 h(x) f(x) dx = 5 \\cdot  \\int_0^5 h(x) \\frac{f(x)}{g(x)} g(x)dx = 5 \\cdot  \\mathbb{E}_g\\left[h(X)\\frac{f(x)}{g(x)}\\right] \\approx \\frac{5}{N}\\sum_{j = 1}^N h(x_j)\\frac{f(x_j)}{g(x_j)}$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# what the function looks like, along with the normal distribution\nh <- function(x) {exp(-0.5 * (x - 2) ^ 2 - 0.1 * abs( sin(2 * x) ) )}\nv <- seq(0, 5, by = 0.01)\nnorm_pdf <- dnorm(v, mean = 2, sd = 1)\n\noptimize(f = function(x){  dnorm(x,2,1) / h(x) }, interval = c(0, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$minimum\n[1] 3.141592\n\n$objective\n[1] 0.3989423\n```\n\n\n:::\n\n```{.r .cell-code}\n# The function inside optimize is the proposal g(x) divided by the function h(x).\n# Optimize gives the location where the ratio between the proposal pdf and target function is smallest.\n# If we multiply the proposal distribution by 1/ratio, then the proposal distribution will always be >= \n# target distribution. So we use this value as our constant M.\n\nm <- optimize(f = function(x){  dnorm(x, 2, 1) / h(x) }, interval = c(0, 5))$objective\n# Technically, this does not matter at all for importance sampling, but it makes it easier visually to\n# see that the trial distribution matches the desired function quite well.\n\nplot(v, norm_pdf * (1 / m), type = \"l\", col = \"blue\")  # trial distribution\nlines(v, h(v), type = \"l\", col = \"black\")  # desired function\n```\n\n::: {.cell-output-display}\n![](102c_hw2_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# we see a pretty good match between the trial distribution and desired function.\n```\n:::\n\n\n\nWe will use `rnorm()` to generate random normal values and use importance sampling to estimate $\\hat{I}$.\n\nKeep in mind that $f(x) = 1/5$.\n\nLet $g(x)$ be the proposal distribution we will use. $g(x)$ looks very much like the normal density with mean 2 and sd 1. However, it is slighlty different because we will throw away any values outside of the range (0, 5).\n\nIf we use the normal PDF directly, then $g(x)$ is not a probability density because it does not integrate to 1.\n\n$$\\int_{0}^5 \\text{Normal PDF (2,1)} \\ne \\int_{-\\infty}^\\infty \\text{Normal PDF (2,1)} = 1$$\n\nTo fix this, we need to find a normalizing constant.\n\n$$g(x) = \\frac{\\text{Normal PDF (2,1)}}{Z_r}$$\n\nSuch that:\n\n$$\\int_{0}^5 g(x) = 1$$\n\n## Problem 3a\n\nFind $Z_r$ so that $\\int_{0}^5 \\frac{1}{Z_r} \\text{Normal PDF (2,1)} = 1$. Hint: figure out how much of the distribution is 'cut off' at 0 and 5, and find $Z_r$ accordingly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz_upper <- (5 - 2) / 1\nz_lower <- (0 - 2) / 1\nz_r <- pnorm(z_upper) - pnorm(z_lower)\nz_r\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9759\n```\n\n\n:::\n:::\n\n\n\n## Problem 3b\n\nPerform Importance sampling.\n\nGenerate a sample of 5000 values of x to estimate $\\hat{I}$.\n\nThis time, do not sample values from `runif()`, but rather from `rnorm()` with mean 2 and standard deviation 1. Make sure you remove values of x below 0 and above 5. Adjust how the estimate of the integral is calculated according to importance sampling as well as using the normalizing constant with the normal density.\n\nCreate a plot using the cumulative mean (aka running mean) of the first 500 values in the sample to show how the estimate of the integral 'settles' over the course of the sampling.\n\nDo this two more times (each with a different starting seed), for a total of three samples of 5000 values each. Plot the cumulative mean of the first 500 values of each sample.\n\nIn your plot also include the 'true' value of the integral, 2.29583, as a horizontal line.\n\nWhen you create your plot, also adjust the axes to fit the samples better.\n\nFinally, print out your three estimates of $I$, and also comment on how quickly the method using importance sampling converges to the expected value versus the uniform sampling method.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Monte Carlo Integration using importance sampling\nn = 5000\nk = 500\n\n# First series\nset.seed(1)\nx1 <- rnorm(n, 2, 1)\nx1 <- x1[x1 > 0 & x1 < 5]\nh_vals1 <- h(x1)\nweights1 <- (1 / 5) / (dnorm(x1, 2, 1) / z_r)\nI_est1 <- (5 / length(x1)) * sum(h_vals1 * weights1)\ncum_mean1 <- 5 * cumsum(h_vals1[1:k] * weights1[1:k]) / seq_along(x1[1:k])\n\n# Second series\nset.seed(2)\nx2 <- rnorm(n, 2, 1)\nx2 <- x2[x2 > 0 & x2 < 5]\nh_vals2 <- h(x2)\nweights2 <- (1 / 5) / (dnorm(x2, 2, 1) / z_r)\nI_est2 <- (5 / length(x2)) * sum(h_vals2 * weights2)\ncum_mean2 <- 5 * cumsum(h_vals2[1:k] * weights2[1:k]) / seq_along(x2[1:k])\n\n# Third series\nset.seed(3)\nx3 <- rnorm(n, 2, 1)\nx3 <- x3[x3 > 0 & x3 < 5]\nh_vals3 <- h(x3)\nweights3 <- (1 / 5) / (dnorm(x3, 2, 1) / z_r)\nI_est3 <- (5 / length(x3)) * sum(h_vals3 * weights3)\ncum_mean3 <- 5 * cumsum(h_vals3[1:k] * weights3[1:k]) / seq_along(x3[1:k])\n\nprint(paste(\"Estimate for I with seed 1:\", I_est1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for I with seed 1: 2.29700774779758\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Estimate for I with seed 2:\", I_est2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for I with seed 2: 2.29365541728644\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Estimate for I with seed 3:\", I_est3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for I with seed 3: 2.2941144034467\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x = c(1, 500), y = c(2.29583, 2.29583), type = \"l\", ylim = c(2, 2.5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:k, y = cum_mean1, col = \"blue\")\nlines(x = 1:k, y = cum_mean2, col = \"red\")\nlines(x = 1:k, y = cum_mean3, col = \"green\")\nlegend(\"bottomright\", legend = c(\"Estimate 1\", \"Estimate 2\", \"Estimate 3\", \n              \"True Value\"), col = c(\"blue\", \"red\", \"green\", \"black\"), lty = 1)\n```\n\n::: {.cell-output-display}\n![](102c_hw2_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nWhat we find is that the values converge much faster. The reason for this is that $h(x)$ and $g(x)$ are nearly proportional to each other. That is to say that the variance of $(h(x)/g(x))$ is very low.\n\n## Self-Normalizing Importance Sampling\n\nIn the previous problem, we had to find the normalizing constant $Z_C$, so that the density from which we drew values would still integrate to 1. We drew values from the normal density, which is well understood, so calculating this constant $Z_c$ was not too difficult.\n\nIn some situations, however, it is not always possible to calculate the normalizing constants for our densities.\n\nWe may only know a function $q(x)$ that is proportional to the target density $f(x)$. If is equal to $q(x)$ divided by some unknown normalizing constant $Z_q$:\n\n$$f(x) = q(x)/Z_q$$\n\nSimilarly, there may be a function $r(x)$ that is proportional to the proposal distribution $g(x)$. In this scenario, we assume we are able to generate values from $g(x)$, but we just don't know the exact function equation for $g(x)$. Instead, we know the function $r(x)$ is proportional to $g(x)$, and the normalizing constant $Z_r$ is unknown.\n\n$$g(x) = r(x)/Z_r$$\n\n### As it applies to the current example\n\nIn our scenario, we can generate values from the proposal distribution $g(x)$. $g(x)$ is proportional to the normal distribution with mean 2 and standard deviation 1. We will call the PDF of the normal distribution $r(x)$, keeping in mind that $g(x) = r(x)/Z_r$. We can generate random values easily using `rnorm()` and then throwing away any value larger than 5 or less than 0. We can also easily find the value of the function $r(x)$ by using `dnorm()`.\n\nUnlike the previous problem, we will not calculate the normalizing constant $Z_r$ for $g(x)$. Instead, we will perform self-normalizing Importance Sampling.\n\nIn self-normalizing importance sampling, we do not bother with calculating these normalizing constants.\n\nWe can estimate\n\n$$\\mathbb{E}_f[h(X)] \\approx \\frac{\\sum_{j = 1}^N h(x_j) w(x_j)}{\\sum_{j = 1}^N w(x_j)}$$\n\nWhere\n\n$$w(x_j) = \\frac{q(x_j)}{r(x_j)}$$\n\n## Problem 4\n\nPerform Self-normalizing Importance sampling.\n\nGenerate a sample of 5000 values of x to estimate $\\hat{I}$.\n\nLet $f(x)$ be the uniform distribution from 0 to 5. You can let $q(x)$ be 0.2, and $Z_q = 1$.\n\nLet $g(x)$ be proportional to the normal distribution with mean 2 and sd 1. We can let $r(x)$ be the normal PDF, and leave $Z_r$ unknown.\n\nAdjust how the estimate of the integral is calculated according to self-normalized importance sampling.\n\nCreate a plot using the cumulative mean (aka running mean) of the first 500 values in the sample to show how the estimate of the integral 'settles' over the course of the sampling.\n\nDo this two more times (each with a different starting seed), for a total of three samples of 5000 values each. Plot the cumulative mean of the first 500 values of each sample.\n\nIn your plot also include the 'true' value of the integral, 2.29583, as a horizontal line.\n\nWhen you create your plot, also adjust the axes to fit the samples better.\n\nFinally, print out your three estimates of $I$, and also comment on how quickly the method using importance sampling converges to the expected value versus the uniform sampling method.\n\nKeep in mind that because we are estimating the normalizing constant $Z_r$ using our samples, the performance of the self-normalizing method will not be as good as when we knew $Z_r$ directly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Monte Carlo Integration using self-normalizing importance sampling\nn = 5000\nk = 500\nq <- 0.2\n\n# First series\nset.seed(1)\nx1 <- rnorm(n, 2, 1)\nx1 <- x1[x1 > 0 & x1 < 5]\nh_vals1 <- h(x1)\nr1 <- dnorm(x1, 2, 1)\nweights1 <- q / r1\nI_est1 <- 5 * sum(h_vals1 * weights1) / sum(weights1)\ncum_mean1 <- 5 * cumsum(h_vals1[1:k] * weights1[1:k]) / cumsum(weights1[1:k])\n\n# Second series\nset.seed(2)\nx2 <- rnorm(n, 2, 1)\nx2 <- x2[x2 > 0 & x2 < 5]\nh_vals2 <- h(x2)\nr2 <- dnorm(x2, 2, 1)\nweights2 <- q / r2\nI_est2 <- 5 * sum(h_vals2 * weights2) / sum(weights2)\ncum_mean2 <- 5 * cumsum(h_vals2[1:k] * weights2[1:k]) / cumsum(weights2[1:k])\n\n# Third series\nset.seed(3)\nx3 <- rnorm(n, 2, 1)\nx3 <- x3[x3 > 0 & x3 < 5]\nh_vals3 <- h(x3)\nr3 <- dnorm(x3, 2, 1)\nweights3 <- q / r3\nI_est3 <- 5 * sum(h_vals3 * weights3) / sum(weights3)\ncum_mean3 <- 5 * cumsum(h_vals3[1:k] * weights3[1:k]) / cumsum(weights3[1:k])\n\nprint(paste(\"Estimate for I with seed 1:\", I_est1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for I with seed 1: 2.2834276456526\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Estimate for I with seed 2:\", I_est2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for I with seed 2: 2.24511503235841\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Estimate for I with seed 3:\", I_est3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Estimate for I with seed 3: 2.33700377077445\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x = c(1, 500), y = c(2.29583, 2.29583), type = \"l\", ylim = c(0, 5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:k, y = cum_mean1, col = \"blue\")\nlines(x = 1:k, y = cum_mean2, col = \"red\")\nlines(x = 1:k, y = cum_mean3, col = \"green\")\nlegend(\"topright\", legend = c(\"Estimate 1\", \"Estimate 2\", \"Estimate 3\", \n          \"True Value\"), col = c(\"blue\", \"red\", \"green\", \"black\"), lty = 1)\n```\n\n::: {.cell-output-display}\n![](102c_hw2_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "102c_hw2_output_Amaeya_Deshpande_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}