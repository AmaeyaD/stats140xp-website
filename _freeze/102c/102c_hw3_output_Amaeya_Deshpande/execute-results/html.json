{
  "hash": "1c13c3cfcb8a1be312922070764884c1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stats 102C, Homework 3 - Intro to Bayesian Statistics\"\nauthor: \"Amaeya Deshpande\"\noutput:\n  pdf_document: default\n  html_document: default\n---\n\n\n\n\n\nHomework Questions and text copyright Miles Chen. For personal use only. Do not post or distribute without permission.\n\n**Do not post your solutions online on a site like github. Violations will be reported.**\n\n# Homework 3 Requirements\n\nThere is no separate instruction file. This file is the instructions and you will modify it for your submission.\n\nYou will submit two files.\n\n1.  `102c_hw3_output_First_Last.pdf` Your output file. You will submit the output to Gradescope. This is the primary file that will be graded. **Make sure all requested output is visible in the output file.**\n\n2.  `102c_hw3_output_First_Last.Rmd` Take this R Markdown file and make the necessary edits so that it generates the requested output. You will submit the Rmd file to BruinLearn.\n\n## Academic Integrity\n\nModifying the following statement with your name.\n\n\"By including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students.\"\n\n## Reading and Viewing:\n\n-   Introducing Monte Carlo Methods with R: Section 2.1, Section 2.2, and Section 2.3\n-   Kolmogorov-Smirnov Test on Youtube: <https://www.youtube.com/watch?v=ZO2RmSkXK3c> (This video covers the two-sample test, but we will conduct a one-sample test against a reference distribution)\n\n## Problem 1 - Inverse CDF Exponential Distribution\n\nWrite a function `my_rexp(n, rate)`, that will generate `n` random values drawn from an exponential distribution with lambda = \"rate\" by using the inverse CDF method. Use `runif()` as your sole source of randomness.\n\nYou are not allowed to use any of the functions `dexp()`, `pexp()`, `qexp()`, or `rexp()` in your generating function.\n\nUse your function to generate 10,000 random values from an exponential distribution with lambda = 1. **Do not** print out the 10,000 values.\n\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your data to the same plot (in a different color). You can use R's `pexp()` function when plotting the theoretic CDF.\n\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic exponential distribution. Be sure to print out the resulting p-value and comment on the sample produced by your function.\n\nOnce you complete the exercise for lambda = 1, repeat the exercise, this time generating 10000 values with lambda = 0.3. Plot the theoretic CDF vs empirical CDF and use the KS test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_rexp <- function(n, rate){\n  u <- runif(n)\n  - (log(1 - u)) / rate\n}\nset.seed(1)\nx <- my_rexp(10^4, rate = 1)\nx_vals <- seq(0, max(x), length.out = 1000)\nt_cdf <- pexp(x_vals, 1)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nks_test <- ks.test(x, \"pexp\", 1)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Kolmogorov-Smirnov test p-value is: 0.388604591146829\"\n```\n\n\n:::\n:::\n\n\n\nGiven that our p-value is larger than 0.05, we have no reason to believe that the sample provided by the function does not fit the expected exponential distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\nx <- my_rexp(10^4, rate = 0.3)\nx_vals <- seq(0, max(x), length.out = 1000)\nt_cdf <- pexp(x_vals, 0.3)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nks_test <- ks.test(x, \"pexp\", 0.3)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Kolmogorov-Smirnov test p-value is: 0.233370274376852\"\n```\n\n\n:::\n:::\n\n\n\n## Problem 2 - Inverse CDF - Discrete Case\n\nWrite a function `my_rbinom(n, size, prob)`, that will generate `n` random values drawn from a binomial distribution with size = `size` and probability of success = `prob` by using the inverse CDF method. Use `runif()` as your sole source of randomness.\n\n**You must use inverse CDF method. You will not get credit if you use a convolution.**\n\nDo not use any of R's binom functions. Do not use `dbinom`, `pbinom`, `qbinom()`, or `rbinom()`\n\nUse your function `my_rbinom()` to generate 10,000 values from a binomial distribution with n = 6, and p = 0.4.\n\nAfter generating 10,000 samples, make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution. (See lecture 3-2, slides 25 and 26)\n\nUse a chi-squared goodness-of-fit test to see if the generated values fit the expected probabilities. Be sure to comment on the graph and results of the test.\n\nUse your function `my_rbinom()` again. This time generate 10,000 values from a binomial distribution with n = 12, and p = 0.55. Make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution. No need to run a chi-squared test for this data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# write your code here\nmy_rbinom <- function(n, size, prob){\n  samp <- numeric(n)\n  for (i in 1:n) {\n    cum_prob <- 0\n    u <- runif(1)\n    for (k in 0:size) {\n      cum_prob <- cum_prob + (choose(size, k) * (prob^k) * ((1 - prob)^(size - k)))\n      if (u <= cum_prob) {\n        samp[i] <- k\n        break\n      }\n    }\n  }\n  samp\n}\nset.seed(3)\nmy_samp <- my_rbinom(10^4, 6, 0.4)\ne_pmf <- table(my_samp) / 10^4\nt_pmf <- dbinom(0:6, 6, 0.4)\ncomp <- rbind(e_pmf, t_pmf)\nbarplot(comp, beside = TRUE, legend.text = c(\"Empirical\", \"Theoretical\"))\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncs_test <- chisq.test(x = table(my_samp), p = t_pmf)\nprint(paste(\"Chi-squared goodness-of-fit test p-value is:\", cs_test$p.value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Chi-squared goodness-of-fit test p-value is: 0.358065469530775\"\n```\n\n\n:::\n:::\n\n\n\nGiven the similarity of the bars in the barplot that our p-value is greater than 0.05, we have no reason to believe that the observed values do not match the theoretical probabilities.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nmy_samp <- my_rbinom(10^4, 12, 0.55)\ne_pmf <- table(my_samp) / 10^4\nt_pmf <- dbinom(0:12, 12, 0.55)\ncomp <- rbind(e_pmf, t_pmf)\nbarplot(comp, beside = TRUE, legend.text = c(\"Empirical\", \"Theoretical\"))\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## Problem 3 - RNG based on inverse CDF and convolutions\n\nUsing only `runif()` and/or `rnorm()` as sources of randomness, generate 10,000 ($10^4$) random samples from each of the following distributions. You are not allowed to use any of R's other distribution functions for the generation of random values. **Do NOT** print out the actual values in your random sample.\n\nFor each distribution:\n\n-   After generating your 10000 samples, plot a density histogram of the resulting sample (`breaks = 30, freq = FALSE`). Plot the theoretic density in another color on top of the histogram. Comment on the plot. You are allowed use R's density functions `dchisq()`, `dt()`, etc. when plotting the density function over the histogram\n-   Plot the theoretic CDF of the distribution. Add the empirical CDF of your data to the same plot (in a different color). You can use R's CDF functions `pchisq()`, `pt()`, etc. when plotting the CDF.\n-   Use the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions. Be sure to print out the resulting p-value and comment on the sample produced by your function.\n\n### Problem 3a:\n\n-   Beta distribution with shape parameters 4 and 2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_rbeta <- function(n, alpha, beta){\n  samp <- numeric(n)\n  for (i in 1:n) {\n    u <- runif(1)\n    samp[i] <- qbeta(u, alpha, beta)\n  }\n  samp\n}\nset.seed(5)\nx <- my_rbeta(10^4, 4, 2)\nhist(x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Beta Distribution with Parameters 4 and 2\")\ncurve(dbeta(x, 4, 2), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nx_vals <- seq(0, max(x), length.out = 1000)\nt_cdf <- pbeta(x_vals, 4, 2)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"topleft\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nks_test <- ks.test(x, \"pbeta\", 4, 2)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Kolmogorov-Smirnov test p-value is: 0.701120392284348\"\n```\n\n\n:::\n:::\n\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are very similar. The shape of the sample generated from my function matches the shape of the beta distribution with parameters 4 and 2.\n\n### Problem 3b:\n\n-   Chi-squared distribution with 4 degrees of freedom\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_rchisq <- function(n, df){\n  samp <- numeric(n)\n  for (i in 1:n) {\n    u <- runif(1)\n    samp[i] <- qchisq(u, df)\n  }\n  samp\n}\nset.seed(6)\nx <- my_rchisq(10^4, 4)\nhist(x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Chi-Squared Distribution with 4 DF\")\ncurve(dchisq(x, 4), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nx_vals <- seq(0, max(x), length.out = 1000)\nt_cdf <- pchisq(x_vals, 4)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\nks_test <- ks.test(x, \"pchisq\", 4)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Kolmogorov-Smirnov test p-value is: 0.663163223641714\"\n```\n\n\n:::\n:::\n\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are pretty similar. The shape of the sample generated from my function matches the shape of the chi-squared distribution with 4 degrees of freedom.\n\n### Problem 3c:\n\n-   t-distribution with 4 degrees of freedom\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_rt <- function(n, df){\n  samp <- numeric(n)\n  for (i in 1:n) {\n    u <- runif(1)\n    samp[i] <- qt(u, df)\n  }\n  samp\n}\nset.seed(7)\nx <- my_rt(10^4, 4)\nhist(x, breaks = 30, freq = FALSE, ylim = c(0, 0.4), col = \"gray\",\n     main = \"t Distribution with 4 DF\")\ncurve(dt(x, 4), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nx_vals <- seq(0, max(x), length.out = 1000)\nt_cdf <- pt(x_vals, 4)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\nks_test <- ks.test(x, \"pt\", 4)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Kolmogorov-Smirnov test p-value is: 0.938900082969244\"\n```\n\n\n:::\n:::\n\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are very similar. The shape of the sample generated from my function matches the shape of the t distribution with 4 degrees of freedom.\n\n### Problem 3d:\n\n-   Gamma distribution with shape parameter 4 and rate parameter 2.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_rgamma <- function(n, alpha, beta){\n  samp <- numeric(n)\n  for (i in 1:n) {\n    u <- runif(1)\n    samp[i] <- qgamma(u, alpha, beta)\n  }\n  samp\n}\nset.seed(8)\nx <- my_rgamma(10^4, 4, 2)\nhist(x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Gamma Distribution with Parameters 4 and 2\")\ncurve(dgamma(x, 4, 2), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nx_vals <- seq(0, max(x), length.out = 1000)\nt_cdf <- pgamma(x_vals, 4, 2)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n```{.r .cell-code}\nks_test <- ks.test(x, \"pgamma\", 4, 2)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Kolmogorov-Smirnov test p-value is: 0.906083018017777\"\n```\n\n\n:::\n:::\n\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are very similar. The shape of the sample generated from my function matches the shape of the gamma distribution with parameters 4 and 2.\n\n## Problem 4 - Rejection Sampling\n\nLet $f(x)$ and $g(x)$ be the target and candidate (proposal) distributions, respectively, in acceptance-rejection sampling.\n\n$f(x) = \\frac{1}{2} \\sin(x)$ for $0 \\le x \\le \\pi$\n\n$g(x) = \\mbox{Unif}(0, \\pi)$\n\nFind the optimal constant $M = \\max \\frac{f(x)}{g(x)}$.\n\nImplement the rejection sampling design, using `runif(n, 0, pi)` as your source of randomness. Generate a sample of at least 10,000 accepted values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x) { 0.5 * sin(x) }\ng <- function(x) { 1 / pi }\nM <- pi / 2\nset.seed(9)\nproposed_x <- runif(10^5, 0, pi)\nr_x <- f(proposed_x) / (M * g(proposed_x))\nU <- runif(10^5)\naccepted <- U < r_x\naccepted_x <- proposed_x[accepted]\nprint(paste(\"The empirical acceptance rate is:\", length(accepted_x) / 10^5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The empirical acceptance rate is: 0.637\"\n```\n\n\n:::\n:::\n\n\n\nWhat is your empirical acceptance rate?\n\nCreate a histogram of your generated (accepted) sample `(breaks = 30, freq = FALSE)`. Add the theoretic PDF to the histogram.\n\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your data to the same plot (in a different color).\n\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions. Be sure to print out the resulting p-value and comment on the sample produced by your function.\n\nI have written a vectorized PDF and CDF function for you.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsin_pdf <- function(x) {\n  ifelse(x > 0 & x < pi, 0.5 * sin(x), 0)\n}\n\nsin_cdf <- function(x) {\n  ifelse(x < 0, 0, ifelse(x < pi, 0.5 - 0.5 * cos(x), 1))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx_vals <- x_vals <- seq(0, max(accepted_x), length.out = 1000)\nhist(accepted_x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Accepted Sample Distribution\")\nlines(x_vals, sin_pdf(x_vals), col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"), cex = 0.5)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(x_vals, sin_cdf(x_vals), type = \"l\", xlab = \"x\", ylab = \"CDF\", \n     col = \"black\", lwd = 2)\nlines(ecdf(accepted_x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n```{.r .cell-code}\nks_test <- ks.test(accepted_x, \"sin_cdf\")\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Kolmogorov-Smirnov test p-value is: 0.200407310985682\"\n```\n\n\n:::\n:::\n\n\n\nBased on the plots above and the p-value being greater than 0.05, we have no reason to believe that the sample we produced is significantly different from the theoretical distribution.\n\n## Problem 5 - Rejection Sampling\n\nUse rejection sampling to generate samples from the Beta distribution with shape parameters $a = 4$ and $b = 6$.\n\nThe PDF of this Beta distribution is:\n\n$$f(x) = \\frac{1}{B(a,b)} x^{a - 1}(1-x)^{b - 1} = 504 x^{3}(1-x)^{5}$$\n\nUse the Uniform (0,1) distribution as your trial distribution.\n\nUse calculus to solve for $M = \\max \\frac{f(x)}{g(x)}$. Show your work. (The derivative is easy to find and can be easily factored to find the roots.)\n\nImplement the rejection sampling design, using `runif(n)` as your source of randomness. Generate a sample of at least 10,000 accepted values.\n\nTo find M, I first found f(x) / g(x) which is $$504 x^{3}(1-x)^{5}$$. The derivative is simplified to $$-504 x^{2}(x-1)^{4}(8x-3)$$. Thus, the roots are x = 1 and x = 3/8. Evaluating points of the function leads us to confirm that the global maximum is at x = 3/8. The function evaluated at x = 3/8 is approximately 2.535.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x) { 504 * x^3 * (1 - x)^5 }\ng <- function(x) { rep(1, length(x)) }\nM <- 504 * (3/8)^3 * (1 - (3/8))^5\nset.seed(10)\nproposed_x <- runif(10^5)\nr_x <- f(proposed_x) / (M * g(proposed_x))\nU <- runif(10^5)\naccepted <- U < r_x\naccepted_x <- proposed_x[accepted]\nks_test <- ks.test(accepted_x, \"pbeta\", 4, 6)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Kolmogorov-Smirnov test p-value is: 0.722405755833897\"\n```\n\n\n:::\n:::\n\n\n\nBased on our outputted p-value which is very large, we have no reason to believe that the sample produced in the code is significantly different from a sample taken from Beta(4, 6).\n\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions. You may use `pbeta` for the CDF. Be sure to print out the resulting p-value and comment on the sample produced by your function. (No additional plots necessary)\n\n## Problem 6 - Empirical Supremum Rejection Sampling\n\nOne challenge of rejection sampling is finding the constant $M = \\max \\frac{f(x)}{g(x)}$. Empirical Supremum rejection sampling estimates the quantity $M$ with a value $\\hat c$. The algorithm works much in the same was as rejection sampling, but it continually updates the value $\\hat c$ if a new $x$ is produced where $\\frac{f(x)}{g(x)}$ is larger than the current estimate $\\hat c$.\n\nRead: 6.3.3 Empirical Supremum Rejection Sampling from the following website:\n\n<https://bookdown.org/rdpeng/advstatcomp/rejection-sampling.html#empirical-supremum-rejection-sampling>\n\n(side note: Roger Peng earned his PhD in Statistics from UCLA and hosts the data-science podcast \"Not so standard deviations\" with Hilary Parker.)\n\nUse Empirical Supremum Rejection Sampling to generate samples from the normal distribution.\n\nThe target distribution f(x) will be the positive half of the standard normal distribution, which will have PDF:\n\n$$f(x) = 2 \\times \\frac{1}{\\sqrt{2\\pi}} \\exp{(-x^2/2)}\\mbox{,   for } x \\ge 0$$\n\nUse an exponential distribution with lambda = 1 as your trial (proposal) distribution.\n\n$$g(x) = e^{-x} \\mbox{,   for } x \\ge 0$$\n\nUnlike the example from lecture, DO NOT find the optimal constant M that will maximize the acceptance rates for the rejection sampling design.\n\nImplement Empirical Supremum Rejection Sampling. While the webpage describes a process that looks like it should be implemented with a loop, it is possible to achieve the same result in a more efficient manner without the need for any loops:\n\n-   Propose $n = 10000$ values. The accepted sample will be smaller.\n-   Use `runif` and inverse CDF to generate $n$ proposal values $X$ from the exponential distribution.\n-   Calculate the ratio for all $n$ values: $\\frac{f(X)}{g(X)}$\n-   Estimate $\\hat c$ as the maximum $\\frac{f(X)}{g(X)}$ you encounter.\n-   Use `runif` to generate $n$ values of $U$ to decide whether to accept or reject the proposed $X$.\n-   reject all proposed X values that do not meet the rejection criteria.\n\nOnce you have generated samples from the folded normal distribution using rejection sampling, turn the accepted values into values from the standard normal distribution. Use `runif` to generate $S$ to decide the sign of the accepted $X$. The accepted $X$ values will be positive or negative with probably 0.5.\n\nCreate a QQ-norm plot of your accepted sample or normally distributed values. Comment on the plot.\n\nSubset your accepted values to the first 1000. Perform a Shapiro-Wilk test `shapiro.test()` to test normality of these 1000 values. Comment on the results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x) { (2 / sqrt(2 * pi)) * exp(-x^2 / 2) }\ng <- function(x) { exp(-x) }\nset.seed(11)\nproposed_x <- runif(10^4)\nX <- - log(proposed_x)\nratios <- f(X) / g(X)\nc_hat <- max(ratios)\nU <- runif(10^4)\naccepted <- U < ratios / c_hat\naccepted_x <- X[accepted]\nS <- runif(length(accepted_x))\nsigns <- ifelse(S < 0.5, -1, 1)\naccepted_x <- signs * accepted_x\nqqnorm(accepted_x)\nqqline(accepted_x, col = \"blue\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nshapiro.test(accepted_x[1:1000])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  accepted_x[1:1000]\nW = 0.99873, p-value = 0.7092\n```\n\n\n:::\n:::\n\n\n\nBased on the QQ plot, we see that the accepted sample conforms well to the normal distribution, as a majority of the sample falls on the 45 degree line. The p-value given by the Shapiro-Wilk test also supports this, as we have no reason to believe that our sample significantly differs from teh normal distribution.\n\n## Problem 7 - Bivariate Normal Distribution\n\nGenerate 1000 random observations from a bivariate normal distribution.\n\n$$\n\\mathbf{X} \\sim \\mathcal{N}_2 \\left (\n\\boldsymbol{\\mu}= \\left(\\begin{array}{c}\n   2 \\\\\n   -1 \\\\\n  \\end{array}\n  \\right),\n  \\boldsymbol{\\Sigma} = \\left( {\\begin{array}{cc}\n   3 & -1.5 \\\\\n   -1.5 & 3 \\\\\n  \\end{array} } \\right) \\right )\n$$\n\nImplement the Box-Muller transform to generate standard normal values. Use `runif()` as your only source of randomness.\n\nOnce you have standard normal values, apply the necessary transform to get the desired bivariate distribution. You may use `chol` to find the Cholesky decomposition of a matrix.\n\nCreate a plot the resulting generated data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- c(2, -1)\nsigma <- cbind(c(3, -1.5), c(-1.5, 3))\nset.seed(12)\nU1 <- runif(2 * 10^3)\nU2 <- runif(2 * 10^3)\nZ1 <- sqrt(-2 * log(U1)) * cos(2 * pi * U2)\nZ2 <- sqrt(-2 * log(U1)) * sin(2 * pi * U2)\nZ <- rbind(Z1, Z2)\nA <- t(chol(sigma))\nX <- mu + A %*% Z\nplot(X[1,], X[2,], pch = 19, cex = 0.5)\n```\n\n::: {.cell-output-display}\n![](102c_hw3_output_Amaeya_Deshpande_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "102c_hw3_output_Amaeya_Deshpande_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}