---
title: "Stats 102C, Homework 1 - Intro to Bayesian Statistics"
author: "Amaeya Deshpande"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Homework Questions and text copyright Miles Chen. For personal use only. Do not post or distribute without permission.

**Do not post your solutions online on a site like github. Violations will be reported.**

# Homework 1 Requirements

There is no separate instruction file. This file is the instructions and you will modify it for your submission.

You will submit two files.

1.  `102c_hw1_output_First_Last.pdf` Your output file. You will submit the output to Gradescope. This is the primary file that will be graded. **Make sure all requested output is visible in the output file.**

2.  `102c_hw1_output_First_Last.Rmd` Take this R Markdown file and make the necessary edits so that it generates the requested output. You will submit the Rmd file to CCLE.

## Academic Integrity

Modifying the following statement with your name.

"By including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students."

## Reading

Reading is important!

Doing Bayesian Data Analysis Textbook is available at: <https://www.sciencedirect.com/science/book/9780124058880>

-   Chapter 2 of Doing Bayesian Data Analysis
-   Skim Chapter 5 of Doing Bayesian Data Analysis
-   Chapter 6 of Doing Bayesian Data Analysis
-   <http://varianceexplained.org/statistics/beta_distribution_and_baseball/>
-   <http://varianceexplained.org/r/credible_intervals_baseball/>

# Bayesian Thinking

\pagebreak

## Problem 1: Doing Bayesian Data Analysis: Exercise 5.1 [Iterative application of Bayes' Rule]

The textbook already gives us the probability of having the disease given a positive test. We can continue using Bayes' Theorem with this probability as the prior probability, now considering a false negative test.

```{r}
disease_given_pos <- (0.99 * 0.001) / (0.99 * 0.001 + 0.05 * 0.999)
disease_given_pos_neg <- (0.01 * disease_given_pos) / (0.95 * 0.999 + 0.01 * 0.001)
disease_given_pos_neg
```

\pagebreak

## Problem 2: Doing Bayesian Data Analysis: Exercise 5.3 [data-order invariance]

## (A)

```{r}
disease_given_neg <- (0.01 * 0.001) / (0.95 * 0.999 + 0.01 * 0.001)
disease_given_neg
```

## (B)

```{r}
disease_given_neg_pos <- (0.99 * disease_given_neg) / (0.99 * 0.001 + 0.05 * 0.999)
disease_given_neg_pos
```

This is equivalent to the probability from Exercise 5.1, demonstrating that the probability of having the disease is the same given one positive and one negative test, regardless of the order.

\pagebreak

# Modeling the Beta-Binomial Model

In Bayesian inference, we often write the posterior distribution of some parameter $\theta$ based on the data $y$ as follows:

$$P(\theta | y) = \frac{P(y | \theta)P(\theta)}{P(y)}$$

We label $P(y | \theta)$ the *likelihood* of the data given the value of the parameter $\theta$.

$P(\theta)$ represents our *prior* distribution of the possible parameter values of $\theta$.

$P(y)$ is the *marginal* distribution of the observed data $y$. This is generally found by summing or integrating the joint probability of the data $y$ and parameter $\theta$ across all possible values of $\theta$. In many cases, this integral is intractable. The good news is that it is just a constant.

As such, we often say that the posterior distribution is proportional to the numerator.

$$P(\theta | y) \propto P(y | \theta)P(\theta)$$

## Summary of Ch 6 of Doing Bayesian Data Analysis

If the Beta distribution prior has distribution $\text{Beta}(\alpha, \beta)$

And our data has $z$ successes, and $N - z$ failures, the posterior distribution will have distribution:

$$\text{Beta}(z + \alpha, N - z + \beta)$$

Let's further explore the relationship between the prior, the likelihood, and the posterior distributions.

## The beta prior for baseball batting average

Read: <http://varianceexplained.org/statistics/beta_distribution_and_baseball/>

As seen in the blog article, we will model the prior distribution of baseball batters' batting average as $\text{Beta}(81, 219)$. These values have been arbitrarily selected, but were chosen because the author of the article 'felt like' batters generally have a batting average between 0.2 and 0.35.

```{r}
s <- seq(0.0, 1, by = 0.005)
plot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density')
arrows(qbeta(0.025, 81, 219), 0.5, qbeta(0.975, 81, 219), 0.5, col = 'red', code = 3, angle = 90, length = 0.05) # adding an 'arrow' to display a credibility interval at the level y = 0.5
```

Credibility interval:

```{r}
print( c( qbeta(0.025, 81, 219), qbeta(0.975, 81, 219) ) )  # equal tailed Credibility interval
```

Before seeing any data, my prior distribution tells me that there is a 95% probability that the batter's batting average is between 0.2213 and 0.3216.

\pagebreak

## Problem 3

### On gradescope begin tagging Problem 3 here

Let's say you observe a player who had 10 at bats and has 4 base hits (batting average = 0.400).

Plot the likelihood of the data for values of p between 0.0 and 1. Use the same vector `s` for the locations.

```{r}
likelihood <- choose(10, 4) * s^4 * (1 - s)^(6)
plot(s, likelihood, type = 'l', ylab = 'probability')
```

Use the known results for the posterior distribution: $\text{Beta}(z + \alpha, N - z + \beta)$. Plot the posterior distribution of p after considering the data. Use red for the posterior. Also plot the prior distribution in black. You will see just a slight shift between the prior and the posterior.

```{r}
cred_int <- c(qbeta(0.025, 85, 225), qbeta(0.975, 85, 225))
conf_int <- prop.test(4, 10, conf.level = 0.95)$conf.int[1:2]
plot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density')
lines(s, dbeta(s, 85, 225), col = "red")
arrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)
arrows(conf_int[1], 0.6, conf_int[2], 0.6, col = 'blue', code = 3, angle = 90, length = 0.05)
```

Use `qbeta()` to create a 95% credibility interval based on the posterior distribution.

Use classical statistics to create a 95% confidence interval for p based on the fact that you had 4 successful hits out of 10 trials. (Even though the large sample condition is not met, assume you can use the central limit theorem for the creation of the confidence interval.)

Using the function `arrow()`, add both the credibility interval (in red at the level y = 0.5) and the confidence interval (in blue at the level y = 0.6) to the plot so you can make a visual comparison.

\pagebreak

## Problem 4a

Let's say you observe a player who had 100 at bats and has 35 base hits (batting average = 0.350). Use a prior distribution of Beta(81, 219).

Plot the posterior distribution of p after considering the data (in red). Also plot the prior (in black). Comment on the difference between the prior and the posterior.

Find a 95% credibility interval based on the posterior. Create a classical 95% confidence interval. Compare the two intervals.

Add both the credibility interval (in red, at y = 0.5) and the confidence interval (in blue, at y = 0.6) to the plot so you can make a visual comparison.

```{r}
cred_int <- c( qbeta(0.025, 116, 284), qbeta(0.975, 116, 284))
conf_int <- prop.test(35, 100, conf.level = 0.95)$conf.int[1:2]
plot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density', ylim = c(0, 20))
lines(s, dbeta(s, 116, 284), col = "red")
arrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)
arrows(conf_int[1], 0.6, conf_int[2], 0.6, col = 'blue', code = 3, angle = 90, length = 0.05)
```

The prior and posterior distributions have a notable and visible difference unlike in question 3. The posterior distribution is shifted more to teh right towards a higher batting average. The posterior distribution is also narrower as it considers more data. Both distributions flatten out on the right tail around the same proportion. The credibility interval and confidence interval are also notably different. The confidence interval is much wider and spans to much higher batting averages.

## Problem 4b

Let's say you observe a player who had 500 at bats and has 175 base hits (batting average = 0.350).

Plot the posterior distribution of p after considering the data. Also plot the prior (use a prior distribution of Beta(81, 219)). Comment on the difference between the prior and the posterior.

Find a 95% credibility interval based on the posterior. Create a classical 95% confidence interval. Compare the two intervals.

Add both the credibility interval (in red, at y = 0.5) and the confidence interval (in blue, at y = 0.8) to the plot so you can make a visual comparison.

```{r}
cred_int <- c( qbeta(0.025, 256, 544), qbeta(0.975, 256, 544))
conf_int <- prop.test(175, 500, conf.level = 0.95)$conf.int[1:2]
plot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density', ylim = c(0, 25))
lines(s, dbeta(s, 256, 544), col = "red")
arrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)
arrows(conf_int[1], 0.8, conf_int[2], 0.8, col = 'blue', code = 3, angle = 90, length = 0.05)
```

The prior and posterior distributions are now very different from one another. Their modes are at least 0.05 away from one another. The posterior distribution is much narrower due to the added data. The credibility and confidence intervals are now very similar in range, but the confidence interval still spans to higher batting avergaes.

## Problem 4c

Finally, let's say you observe a player who had 5000 at bats and has 1750 base hits.

Plot the posterior distribution of p after considering the data. Also plot the prior Beta(81, 219).

Add both the credibility interval (in red, at y = 0.5) and the confidence interval (in blue, at y = 1) to the plot so you can make a visual comparison.

```{r}
cred_int <- c( qbeta(0.025, 1831, 3469), qbeta(0.975, 256, 544))
conf_int <- prop.test(1831, 5300, conf.level = 0.95)$conf.int[1:2]
plot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density', ylim = c(0, 60))
lines(s, dbeta(s, 1831, 3469), col = "red")
arrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)
arrows(conf_int[1], 1, conf_int[2], 1, col = 'blue', code = 3, angle = 90, length = 0.05)
```

### As the amount of data increases, how do the results of the Bayesian credibility interval compare to the results of the classical confidence interval?

As the amount of data increases, the results of the Bayesian credibility interval and the classical confidence interval become more and more similar. This can be seen in the red and blue intervals above. This happens because as the amount of data increases, the posterior data used for the credibility distribution becomes more and more similar to the sampled data used for the confidence interval.

\pagebreak

## Problem 5: Application of Monte Carlo Integration: Bayesian Statistics

In problem 4b, the player's data that we observed was 175 hits out of 500 at-bats. In the problem, you updated the posterior distribution. The posterior distribution is: Beta(256, 544)

Someone asks, if this player has four at-bats in the next game, what is the probability that the player will get at least one hit?

The answer to this question is 1 - Probability(0 hits in 4 at bats). So we need to find the probability of 0 hits in 4 at bats.

If we knew the value of p, it's easy:

$$P(x = 0) = {4 \choose 0} p^0 (1-p)^4 = (1-p)^4$$

In Bayesian statistics, however, we do not know the value of p. It can be one of the infinite values between 0 and 1. After seeing our data, p is described by the distribution Beta(256, 544).

Thus, the answer to our question will be found by integrating the probability of 0 hits multiplied by the probability of p, considered across every possible value of p.

$$\int_0^1 (1 - p)^4 f(p) dp$$

where f(p) is the pdf of the distribution Beta(256, 544).

Using Monte Carlo Estimation (use n = 50,000), estimate the value of the above integral. (hint: it's already in the form $E_f[h(X)] = \int_\mathcal{X} h(x)f(x) dx$)

You are allowed to use `rbeta()`.

```{r}
set.seed(1)
samp <- rbeta(50000, 256, 544)
g_samp <- (1 - samp)^4
1 - mean(g_samp)
```

\pagebreak

## Problem 6: Bayesian Inference for the rate parameter of the Exponential distribution

The gamma distribution has a PDF of the form:

$$f(x; \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha - 1} e^{-\beta x}$$

It has two shape parameters $\alpha$ and $\beta$. Many distributions (including the exponential and chi-squared distributions) can be written in the form of a gamma distribution. We will take a look at the gamma distribution because it serves as a conjugate-prior for many distributions.

When looking at the PDF of the gamma distribution, you can ignore the scary looking constant in the front $\frac{\beta^\alpha}{\Gamma(\alpha)}$, as its only purpose is to make sure the PDF integrates to 1.

The exponential distribution has the following PDF, defined by the rate parameter $\lambda$.

$$f(x;\lambda) = \lambda e^{-\lambda x}$$

The exponential distribution can be used to model the time between events, such as the time between customers entering a store. The $\lambda$ parameter is the rate (the number of arrivals per time block). Let's say we are trying to model customers entering a store each hour. If $\lambda = 2$, that means the average rate is two arrivals per hour. The expected time between customers is $1/\lambda = 0.5$, meaning the mean time between customers is half an hour.

In this problem, we are trying to model customers entering a small business and will use Bayesian inference to create a distribution for the rate parameter. You talk to the business owner who tells you that sometimes the business gets busy and will see 20 customers in an hour. Other times, it's slow, and maybe only 3 or 4 customers come. But overall, the owner estimates the average is something like 8 customers per hour, give or take a few.

Taking this into account, you decide to use a Gamma distribution with shape parameters $\alpha = 8$ and $\beta = 1$ as the prior distribution for the rate $\lambda$.

```{r}
s <- seq(0, 30, by = 0.01)
plot(s, dgamma(s, 8, 1), type = 'l')
```

You decide to collect data by timing how long you wait between customer arrivals.

You gather the following values, measured in fractions of an hour:

```{r}
y <- c(0.131, 0.078, 0.297, 0.024, 0.016, 0.057, 0.070, 0.148, 0.070, 0.109)
# after you started the stop watch, the first customer arrived after 7 minutes and 52 seconds (0.131 of an hour)
# the next customer came 4 minutes and 41 seconds after that (0.078 of an hour). etc. etc.
# You gathered values for 10 customers total.
# Conveniently, they add up to exactly one hour!
```

I have written a simple function `l()` to calculate the likelihood of the data for a given lambda. It simply takes the pdf of each data point and returns the product.

```{r}
s <- seq(0, 30, by = 0.01)
l <- function(lambda){
  y <- c(0.131, 0.078, 0.297, 0.024, 0.016, 0.057, 0.070, 0.148, 0.070, 0.109)
  prod(lambda * exp(-lambda * y))
}

res <- rep(NA, length(s))
for(i in 1:length(s)){
  res[i] <- l(s[i])
}

plot(s, res, type = 'l', main = 'likelihood of given data as a function of lambda')
```

Calculate the likelihood function for lambda mathematically. The total likelihood of the data (which is assumed to be iid) is the product of each point's probability. You can take advantage of the fact that the sum of the y's is 1.

Write down your equation of the likelihood function.

The equation for the likelihood function of lambda is

$$L(\lambda) = \lambda^{10} e^{-\lambda}$$

This was found by using the PDF of the exponential distribution and the fact that n = 10 in our data and that the sum of all y's is 1.

Create a plot of your mathematical likelihood function for values of lambda between 0 and 30. Is it identical to the plot I have provided above?

```{r}
L <- function(lambda){
  prod(lambda^10 * exp(-lambda))
}

res <- rep(NA, length(s))
for(i in 1:length(s)){
  res[i] <- L(s[i])
}

plot(s, res, type = 'l', main = 'likelihood of given data as a function of lambda')
```

The plot is seemingly identical to the plot provided using the function l().

Mathematically, find the posterior distribution of lambda given the data.

Hints: We know that the posterior distribution is proportional to the likelihood times the prior. We also know that the gamma distribution is the conjugate prior for the exponential distribution. This means that the posterior distribution of lambda will be a gamma distribution.

$$p(\lambda | y) \propto p(y | \lambda) p(\lambda)$$

Start by multiplying the likelihood by the prior (a gamma distribution). Then, using algebra, rearrange terms so that the posterior is in the form of a gamma distribution with parameters $\alpha$ and $\beta$. If you temporarily ignore the normalizing constant in the gamma distribution, it is in the form $x^{\text{constant1}}e^{\text{-constant2}\cdot x}$

Your answer: The posterior distribution of lambda given the data is a gamma distribution with parameters $\alpha = 18$ and $\beta = 2$. This was determined using the information above and ultimately reaching the form $x^{\text{17}}e^{\text{-2}\cdot x}$

Graph the posterior distribution.

```{r}
plot(s, dgamma(s, 18, 2), type = 'l')
```

\pagebreak

## Problem 7: Bayesian Inference for the mean of a normal distribution

Let's say X comes from a normal distribution with mean $\mu$ and variance $\sigma^2$. The variance $\sigma^2$ is known and is equal to 9. The mean, however, is unknown and has its own probability distribution.

The prior belief for the mean of the population is that $\mu$ comes from a normal distribution with mean $\mu_0$ and variance $\sigma^2_0$. Both $\mu_0$ and $\sigma^2_0$ are given.

$$\mu \sim N(\mu_0, \sigma^2_0)$$

Derive the posterior distribution of $\mu$ given that we have observed a single observation x.

That is:

$$X \sim N(\mu, \sigma^2)$$

$$f(x | \mu) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{\frac{-(x - \mu)^2}{2 \sigma^2}}$$

Where $\mu$ itself has the pdf:

$$f(\mu) = \frac{1}{\sqrt{2 \pi \sigma_0^2}} \exp{\frac{-(\mu - \mu_0)^2}{2 \sigma_0^2}}$$

To get you started, keep in mind Bayes' Rule. We also remember that $f(x)$ is a constant that exists only to ensure that $f(\mu|x)$ integrates to 1.

$$f(\mu | x) = \frac{\text{likelihood}\times\text{prior}}{\text{marginal}} = \frac{f(x|\mu)f(\mu)}{f(x)} \propto f(x|\mu)f(\mu)$$

Thus:

$$f(\mu | x) \propto f(x|\mu)f(\mu) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{\frac{-(x - \mu)^2}{2 \sigma^2}} \cdot \frac{1}{\sqrt{2 \pi \sigma_0^2}} \exp{\frac{-(\mu - \mu_0)^2}{2 \sigma_0^2}}$$

Your job: combine and rearrange terms as necessary to get the result to be in the form of the normal PDF.

That is find $\mu_1$ and $\sigma_1$ so that the above product can be expressed as:

$$f(\mu | x) = \frac{1}{\sqrt{2 \pi \sigma_1^2}} \exp{\frac{-(\mu - \mu_1)^2}{2 \sigma_1^2}}$$

Your answer: The posterior distribution of $\mu$ given the data is a normal distribution with parameters $$\mu_1 = \frac{\sigma_0^2 x + \sigma^2 \mu_0} {\sigma_0^2 + \sigma^2}$$ and

$$\sigma_1^2 = \frac{\sigma_0^2 \sigma^2} {\sigma_0^2 + \sigma^2}$$

This was found by expanding and combining the terms in the equation provided above and then rearranging terms to return to the form of the normal distribution. From there, I was able to identify the parameters of the distribution.

## Problem 7b:

Using your result from above, give the posterior distribution for the mean height of adult males in the US.

Prior to seeing any data, it is believed that the mean height of adult males in the US is about 69 inches. We express our prior beliefs by saying $\mu$ random variable that follows a Normal distribution with mean 69 and sd = 0.5.

We randomly select one adult male in the US, and find his height to be 71 inches.

With this observation, what is the posterior distribution of the mean $\mu$?

The posterior distribution of the mean $\mu$ is $$\mu \sim N(69.054, 0.243)$$

This was found by inputting the known values into the answers we found in question 7a. By solving for the parameters, I was able to find the posterior distribution given the single observation.
