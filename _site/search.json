[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Amaeya Deshpande",
    "section": "",
    "text": "Amaeya Deshpande is a statistics and data science student at UCLA. He is finalizing his fourth years of undergraduate studies and pursuing a Master’s program in data science. He currently works for the customer workflow provider ServiceNow as a software engineer. His interests include machine learning, computational statistics, and sports analytics."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Amaeya Deshpande",
    "section": "Education",
    "text": "Education\nUniversity of California, Los Angeles | Los Angeles, CA M.S in Statistics and Data Science | Sept 2021 - Present"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Amaeya Deshpande",
    "section": "Experience",
    "text": "Experience\nServiceNow, Inc. | Software Engineer | Jun 2023 - present\nDisruptive Sports Agency | Data Research Intern | Dec 2022 - Jul 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "Stats 102C Assignments\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nStats 102C, Homework 1 - Intro to Bayesian Statistics\n\n\n\n\nStats 102C, Homework 2 - Intro to Bayesian Statistics\n\n\n\n\nStats 102C, Homework 3 - Intro to Bayesian Statistics\n\n\n\n\nStats 102C, Homework 4 - Intro to MCMC\n\n\n\n\nStats 102C, Homework 5 - Multivariate MCMC\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#about",
    "href": "about.html#about",
    "title": "Amaeya Deshpande",
    "section": "",
    "text": "Amaeya Deshpande is a statistics and data science student at UCLA. He is finalizing his fourth years of undergraduate studies and pursuing a Master’s program in data science. He currently works for the customer workflow provider ServiceNow as a software engineer. His interests include machine learning, computational statistics, and sports analytics."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "",
    "text": "Homework Questions and text copyright Miles Chen. For personal use only. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#academic-integrity",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#academic-integrity",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nModifying the following statement with your name.\n“By including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students.”"
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#reading",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#reading",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Reading",
    "text": "Reading\nReading is important!\nDoing Bayesian Data Analysis Textbook is available at: https://www.sciencedirect.com/science/book/9780124058880\n\nChapter 2 of Doing Bayesian Data Analysis\nSkim Chapter 5 of Doing Bayesian Data Analysis\nChapter 6 of Doing Bayesian Data Analysis\nhttp://varianceexplained.org/statistics/beta_distribution_and_baseball/\nhttp://varianceexplained.org/r/credible_intervals_baseball/"
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-1-doing-bayesian-data-analysis-exercise-5.1-iterative-application-of-bayes-rule",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-1-doing-bayesian-data-analysis-exercise-5.1-iterative-application-of-bayes-rule",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 1: Doing Bayesian Data Analysis: Exercise 5.1 [Iterative application of Bayes’ Rule]",
    "text": "Problem 1: Doing Bayesian Data Analysis: Exercise 5.1 [Iterative application of Bayes’ Rule]\nThe textbook already gives us the probability of having the disease given a positive test. We can continue using Bayes’ Theorem with this probability as the prior probability, now considering a false negative test.\n\ndisease_given_pos &lt;- (0.99 * 0.001) / (0.99 * 0.001 + 0.05 * 0.999)\ndisease_given_pos_neg &lt;- (0.01 * disease_given_pos) / (0.95 * 0.999 + 0.01 * 0.001)\ndisease_given_pos_neg\n\n[1] 0.0002047777"
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-2-doing-bayesian-data-analysis-exercise-5.3-data-order-invariance",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-2-doing-bayesian-data-analysis-exercise-5.3-data-order-invariance",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 2: Doing Bayesian Data Analysis: Exercise 5.3 [data-order invariance]",
    "text": "Problem 2: Doing Bayesian Data Analysis: Exercise 5.3 [data-order invariance]"
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#a",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#a",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "(A)",
    "text": "(A)\n\ndisease_given_neg &lt;- (0.01 * 0.001) / (0.95 * 0.999 + 0.01 * 0.001)\ndisease_given_neg\n\n[1] 1.053674e-05"
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#b",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#b",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "(B)",
    "text": "(B)\n\ndisease_given_neg_pos &lt;- (0.99 * disease_given_neg) / (0.99 * 0.001 + 0.05 * 0.999)\ndisease_given_neg_pos\n\n[1] 0.0002047777\n\n\nThis is equivalent to the probability from Exercise 5.1, demonstrating that the probability of having the disease is the same given one positive and one negative test, regardless of the order."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#summary-of-ch-6-of-doing-bayesian-data-analysis",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#summary-of-ch-6-of-doing-bayesian-data-analysis",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Summary of Ch 6 of Doing Bayesian Data Analysis",
    "text": "Summary of Ch 6 of Doing Bayesian Data Analysis\nIf the Beta distribution prior has distribution \\(\\text{Beta}(\\alpha, \\beta)\\)\nAnd our data has \\(z\\) successes, and \\(N - z\\) failures, the posterior distribution will have distribution:\n\\[\\text{Beta}(z + \\alpha, N - z + \\beta)\\]\nLet’s further explore the relationship between the prior, the likelihood, and the posterior distributions."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#the-beta-prior-for-baseball-batting-average",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#the-beta-prior-for-baseball-batting-average",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "The beta prior for baseball batting average",
    "text": "The beta prior for baseball batting average\nRead: http://varianceexplained.org/statistics/beta_distribution_and_baseball/\nAs seen in the blog article, we will model the prior distribution of baseball batters’ batting average as \\(\\text{Beta}(81, 219)\\). These values have been arbitrarily selected, but were chosen because the author of the article ‘felt like’ batters generally have a batting average between 0.2 and 0.35.\n\ns &lt;- seq(0.0, 1, by = 0.005)\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density')\narrows(qbeta(0.025, 81, 219), 0.5, qbeta(0.975, 81, 219), 0.5, col = 'red', code = 3, angle = 90, length = 0.05) # adding an 'arrow' to display a credibility interval at the level y = 0.5\n\n\n\n\n\n\n\n\nCredibility interval:\n\nprint( c( qbeta(0.025, 81, 219), qbeta(0.975, 81, 219) ) )  # equal tailed Credibility interval\n\n[1] 0.2213490 0.3215554\n\n\nBefore seeing any data, my prior distribution tells me that there is a 95% probability that the batter’s batting average is between 0.2213 and 0.3216."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-3",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-3",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 3",
    "text": "Problem 3\n\nOn gradescope begin tagging Problem 3 here\nLet’s say you observe a player who had 10 at bats and has 4 base hits (batting average = 0.400).\nPlot the likelihood of the data for values of p between 0.0 and 1. Use the same vector s for the locations.\n\nlikelihood &lt;- choose(10, 4) * s^4 * (1 - s)^(6)\nplot(s, likelihood, type = 'l', ylab = 'probability')\n\n\n\n\n\n\n\n\nUse the known results for the posterior distribution: \\(\\text{Beta}(z + \\alpha, N - z + \\beta)\\). Plot the posterior distribution of p after considering the data. Use red for the posterior. Also plot the prior distribution in black. You will see just a slight shift between the prior and the posterior.\n\ncred_int &lt;- c(qbeta(0.025, 85, 225), qbeta(0.975, 85, 225))\nconf_int &lt;- prop.test(4, 10, conf.level = 0.95)$conf.int[1:2]\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density')\nlines(s, dbeta(s, 85, 225), col = \"red\")\narrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)\narrows(conf_int[1], 0.6, conf_int[2], 0.6, col = 'blue', code = 3, angle = 90, length = 0.05)\n\n\n\n\n\n\n\n\nUse qbeta() to create a 95% credibility interval based on the posterior distribution.\nUse classical statistics to create a 95% confidence interval for p based on the fact that you had 4 successful hits out of 10 trials. (Even though the large sample condition is not met, assume you can use the central limit theorem for the creation of the confidence interval.)\nUsing the function arrow(), add both the credibility interval (in red at the level y = 0.5) and the confidence interval (in blue at the level y = 0.6) to the plot so you can make a visual comparison."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-4a",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-4a",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 4a",
    "text": "Problem 4a\nLet’s say you observe a player who had 100 at bats and has 35 base hits (batting average = 0.350). Use a prior distribution of Beta(81, 219).\nPlot the posterior distribution of p after considering the data (in red). Also plot the prior (in black). Comment on the difference between the prior and the posterior.\nFind a 95% credibility interval based on the posterior. Create a classical 95% confidence interval. Compare the two intervals.\nAdd both the credibility interval (in red, at y = 0.5) and the confidence interval (in blue, at y = 0.6) to the plot so you can make a visual comparison.\n\ncred_int &lt;- c( qbeta(0.025, 116, 284), qbeta(0.975, 116, 284))\nconf_int &lt;- prop.test(35, 100, conf.level = 0.95)$conf.int[1:2]\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density', ylim = c(0, 20))\nlines(s, dbeta(s, 116, 284), col = \"red\")\narrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)\narrows(conf_int[1], 0.6, conf_int[2], 0.6, col = 'blue', code = 3, angle = 90, length = 0.05)\n\n\n\n\n\n\n\n\nThe prior and posterior distributions have a notable and visible difference unlike in question 3. The posterior distribution is shifted more to teh right towards a higher batting average. The posterior distribution is also narrower as it considers more data. Both distributions flatten out on the right tail around the same proportion. The credibility interval and confidence interval are also notably different. The confidence interval is much wider and spans to much higher batting averages."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-4b",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-4b",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 4b",
    "text": "Problem 4b\nLet’s say you observe a player who had 500 at bats and has 175 base hits (batting average = 0.350).\nPlot the posterior distribution of p after considering the data. Also plot the prior (use a prior distribution of Beta(81, 219)). Comment on the difference between the prior and the posterior.\nFind a 95% credibility interval based on the posterior. Create a classical 95% confidence interval. Compare the two intervals.\nAdd both the credibility interval (in red, at y = 0.5) and the confidence interval (in blue, at y = 0.8) to the plot so you can make a visual comparison.\n\ncred_int &lt;- c( qbeta(0.025, 256, 544), qbeta(0.975, 256, 544))\nconf_int &lt;- prop.test(175, 500, conf.level = 0.95)$conf.int[1:2]\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density', ylim = c(0, 25))\nlines(s, dbeta(s, 256, 544), col = \"red\")\narrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)\narrows(conf_int[1], 0.8, conf_int[2], 0.8, col = 'blue', code = 3, angle = 90, length = 0.05)\n\n\n\n\n\n\n\n\nThe prior and posterior distributions are now very different from one another. Their modes are at least 0.05 away from one another. The posterior distribution is much narrower due to the added data. The credibility and confidence intervals are now very similar in range, but the confidence interval still spans to higher batting avergaes."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-4c",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-4c",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 4c",
    "text": "Problem 4c\nFinally, let’s say you observe a player who had 5000 at bats and has 1750 base hits.\nPlot the posterior distribution of p after considering the data. Also plot the prior Beta(81, 219).\nAdd both the credibility interval (in red, at y = 0.5) and the confidence interval (in blue, at y = 1) to the plot so you can make a visual comparison.\n\ncred_int &lt;- c( qbeta(0.025, 1831, 3469), qbeta(0.975, 256, 544))\nconf_int &lt;- prop.test(1831, 5300, conf.level = 0.95)$conf.int[1:2]\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density', ylim = c(0, 60))\nlines(s, dbeta(s, 1831, 3469), col = \"red\")\narrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)\narrows(conf_int[1], 1, conf_int[2], 1, col = 'blue', code = 3, angle = 90, length = 0.05)\n\n\n\n\n\n\n\n\n\nAs the amount of data increases, how do the results of the Bayesian credibility interval compare to the results of the classical confidence interval?\nAs the amount of data increases, the results of the Bayesian credibility interval and the classical confidence interval become more and more similar. This can be seen in the red and blue intervals above. This happens because as the amount of data increases, the posterior data used for the credibility distribution becomes more and more similar to the sampled data used for the confidence interval."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-5-application-of-monte-carlo-integration-bayesian-statistics",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-5-application-of-monte-carlo-integration-bayesian-statistics",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 5: Application of Monte Carlo Integration: Bayesian Statistics",
    "text": "Problem 5: Application of Monte Carlo Integration: Bayesian Statistics\nIn problem 4b, the player’s data that we observed was 175 hits out of 500 at-bats. In the problem, you updated the posterior distribution. The posterior distribution is: Beta(256, 544)\nSomeone asks, if this player has four at-bats in the next game, what is the probability that the player will get at least one hit?\nThe answer to this question is 1 - Probability(0 hits in 4 at bats). So we need to find the probability of 0 hits in 4 at bats.\nIf we knew the value of p, it’s easy:\n\\[P(x = 0) = {4 \\choose 0} p^0 (1-p)^4 = (1-p)^4\\]\nIn Bayesian statistics, however, we do not know the value of p. It can be one of the infinite values between 0 and 1. After seeing our data, p is described by the distribution Beta(256, 544).\nThus, the answer to our question will be found by integrating the probability of 0 hits multiplied by the probability of p, considered across every possible value of p.\n\\[\\int_0^1 (1 - p)^4 f(p) dp\\]\nwhere f(p) is the pdf of the distribution Beta(256, 544).\nUsing Monte Carlo Estimation (use n = 50,000), estimate the value of the above integral. (hint: it’s already in the form \\(E_f[h(X)] = \\int_\\mathcal{X} h(x)f(x) dx\\))\nYou are allowed to use rbeta().\n\nset.seed(1)\nsamp &lt;- rbeta(50000, 256, 544)\ng_samp &lt;- (1 - samp)^4\n1 - mean(g_samp)\n\n[1] 0.7854513"
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-6-bayesian-inference-for-the-rate-parameter-of-the-exponential-distribution",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-6-bayesian-inference-for-the-rate-parameter-of-the-exponential-distribution",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 6: Bayesian Inference for the rate parameter of the Exponential distribution",
    "text": "Problem 6: Bayesian Inference for the rate parameter of the Exponential distribution\nThe gamma distribution has a PDF of the form:\n\\[f(x; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\\]\nIt has two shape parameters \\(\\alpha\\) and \\(\\beta\\). Many distributions (including the exponential and chi-squared distributions) can be written in the form of a gamma distribution. We will take a look at the gamma distribution because it serves as a conjugate-prior for many distributions.\nWhen looking at the PDF of the gamma distribution, you can ignore the scary looking constant in the front \\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\), as its only purpose is to make sure the PDF integrates to 1.\nThe exponential distribution has the following PDF, defined by the rate parameter \\(\\lambda\\).\n\\[f(x;\\lambda) = \\lambda e^{-\\lambda x}\\]\nThe exponential distribution can be used to model the time between events, such as the time between customers entering a store. The \\(\\lambda\\) parameter is the rate (the number of arrivals per time block). Let’s say we are trying to model customers entering a store each hour. If \\(\\lambda = 2\\), that means the average rate is two arrivals per hour. The expected time between customers is \\(1/\\lambda = 0.5\\), meaning the mean time between customers is half an hour.\nIn this problem, we are trying to model customers entering a small business and will use Bayesian inference to create a distribution for the rate parameter. You talk to the business owner who tells you that sometimes the business gets busy and will see 20 customers in an hour. Other times, it’s slow, and maybe only 3 or 4 customers come. But overall, the owner estimates the average is something like 8 customers per hour, give or take a few.\nTaking this into account, you decide to use a Gamma distribution with shape parameters \\(\\alpha = 8\\) and \\(\\beta = 1\\) as the prior distribution for the rate \\(\\lambda\\).\n\ns &lt;- seq(0, 30, by = 0.01)\nplot(s, dgamma(s, 8, 1), type = 'l')\n\n\n\n\n\n\n\n\nYou decide to collect data by timing how long you wait between customer arrivals.\nYou gather the following values, measured in fractions of an hour:\n\ny &lt;- c(0.131, 0.078, 0.297, 0.024, 0.016, 0.057, 0.070, 0.148, 0.070, 0.109)\n# after you started the stop watch, the first customer arrived after 7 minutes and 52 seconds (0.131 of an hour)\n# the next customer came 4 minutes and 41 seconds after that (0.078 of an hour). etc. etc.\n# You gathered values for 10 customers total.\n# Conveniently, they add up to exactly one hour!\n\nI have written a simple function l() to calculate the likelihood of the data for a given lambda. It simply takes the pdf of each data point and returns the product.\n\ns &lt;- seq(0, 30, by = 0.01)\nl &lt;- function(lambda){\n  y &lt;- c(0.131, 0.078, 0.297, 0.024, 0.016, 0.057, 0.070, 0.148, 0.070, 0.109)\n  prod(lambda * exp(-lambda * y))\n}\n\nres &lt;- rep(NA, length(s))\nfor(i in 1:length(s)){\n  res[i] &lt;- l(s[i])\n}\n\nplot(s, res, type = 'l', main = 'likelihood of given data as a function of lambda')\n\n\n\n\n\n\n\n\nCalculate the likelihood function for lambda mathematically. The total likelihood of the data (which is assumed to be iid) is the product of each point’s probability. You can take advantage of the fact that the sum of the y’s is 1.\nWrite down your equation of the likelihood function.\nThe equation for the likelihood function of lambda is\n\\[L(\\lambda) = \\lambda^{10} e^{-\\lambda}\\]\nThis was found by using the PDF of the exponential distribution and the fact that n = 10 in our data and that the sum of all y’s is 1.\nCreate a plot of your mathematical likelihood function for values of lambda between 0 and 30. Is it identical to the plot I have provided above?\n\nL &lt;- function(lambda){\n  prod(lambda^10 * exp(-lambda))\n}\n\nres &lt;- rep(NA, length(s))\nfor(i in 1:length(s)){\n  res[i] &lt;- L(s[i])\n}\n\nplot(s, res, type = 'l', main = 'likelihood of given data as a function of lambda')\n\n\n\n\n\n\n\n\nThe plot is seemingly identical to the plot provided using the function l().\nMathematically, find the posterior distribution of lambda given the data.\nHints: We know that the posterior distribution is proportional to the likelihood times the prior. We also know that the gamma distribution is the conjugate prior for the exponential distribution. This means that the posterior distribution of lambda will be a gamma distribution.\n\\[p(\\lambda | y) \\propto p(y | \\lambda) p(\\lambda)\\]\nStart by multiplying the likelihood by the prior (a gamma distribution). Then, using algebra, rearrange terms so that the posterior is in the form of a gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\). If you temporarily ignore the normalizing constant in the gamma distribution, it is in the form \\(x^{\\text{constant1}}e^{\\text{-constant2}\\cdot x}\\)\nYour answer: The posterior distribution of lambda given the data is a gamma distribution with parameters \\(\\alpha = 18\\) and \\(\\beta = 2\\). This was determined using the information above and ultimately reaching the form \\(x^{\\text{17}}e^{\\text{-2}\\cdot x}\\)\nGraph the posterior distribution.\n\nplot(s, dgamma(s, 18, 2), type = 'l')"
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-7-bayesian-inference-for-the-mean-of-a-normal-distribution",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-7-bayesian-inference-for-the-mean-of-a-normal-distribution",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 7: Bayesian Inference for the mean of a normal distribution",
    "text": "Problem 7: Bayesian Inference for the mean of a normal distribution\nLet’s say X comes from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The variance \\(\\sigma^2\\) is known and is equal to 9. The mean, however, is unknown and has its own probability distribution.\nThe prior belief for the mean of the population is that \\(\\mu\\) comes from a normal distribution with mean \\(\\mu_0\\) and variance \\(\\sigma^2_0\\). Both \\(\\mu_0\\) and \\(\\sigma^2_0\\) are given.\n\\[\\mu \\sim N(\\mu_0, \\sigma^2_0)\\]\nDerive the posterior distribution of \\(\\mu\\) given that we have observed a single observation x.\nThat is:\n\\[X \\sim N(\\mu, \\sigma^2)\\]\n\\[f(x | \\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\frac{-(x - \\mu)^2}{2 \\sigma^2}}\\]\nWhere \\(\\mu\\) itself has the pdf:\n\\[f(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma_0^2}} \\exp{\\frac{-(\\mu - \\mu_0)^2}{2 \\sigma_0^2}}\\]\nTo get you started, keep in mind Bayes’ Rule. We also remember that \\(f(x)\\) is a constant that exists only to ensure that \\(f(\\mu|x)\\) integrates to 1.\n\\[f(\\mu | x) = \\frac{\\text{likelihood}\\times\\text{prior}}{\\text{marginal}} = \\frac{f(x|\\mu)f(\\mu)}{f(x)} \\propto f(x|\\mu)f(\\mu)\\]\nThus:\n\\[f(\\mu | x) \\propto f(x|\\mu)f(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\frac{-(x - \\mu)^2}{2 \\sigma^2}} \\cdot \\frac{1}{\\sqrt{2 \\pi \\sigma_0^2}} \\exp{\\frac{-(\\mu - \\mu_0)^2}{2 \\sigma_0^2}}\\]\nYour job: combine and rearrange terms as necessary to get the result to be in the form of the normal PDF.\nThat is find \\(\\mu_1\\) and \\(\\sigma_1\\) so that the above product can be expressed as:\n\\[f(\\mu | x) = \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp{\\frac{-(\\mu - \\mu_1)^2}{2 \\sigma_1^2}}\\]\nYour answer: The posterior distribution of \\(\\mu\\) given the data is a normal distribution with parameters \\[\\mu_1 = \\frac{\\sigma_0^2 x + \\sigma^2 \\mu_0} {\\sigma_0^2 + \\sigma^2}\\] and\n\\[\\sigma_1^2 = \\frac{\\sigma_0^2 \\sigma^2} {\\sigma_0^2 + \\sigma^2}\\]\nThis was found by expanding and combining the terms in the equation provided above and then rearranging terms to return to the form of the normal distribution. From there, I was able to identify the parameters of the distribution."
  },
  {
    "objectID": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-7b",
    "href": "projects/102c_hw1_output_Amaeya_Deshpande.html#problem-7b",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 7b:",
    "text": "Problem 7b:\nUsing your result from above, give the posterior distribution for the mean height of adult males in the US.\nPrior to seeing any data, it is believed that the mean height of adult males in the US is about 69 inches. We express our prior beliefs by saying \\(\\mu\\) random variable that follows a Normal distribution with mean 69 and sd = 0.5.\nWe randomly select one adult male in the US, and find his height to be 71 inches.\nWith this observation, what is the posterior distribution of the mean \\(\\mu\\)?\nThe posterior distribution of the mean \\(\\mu\\) is \\[\\mu \\sim N(69.054, 0.243)\\]\nThis was found by inputting the known values into the answers we found in question 7a. By solving for the parameters, I was able to find the posterior distribution given the single observation."
  },
  {
    "objectID": "projects/102C_hw5_Amaeya_Deshpande.html",
    "href": "projects/102C_hw5_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 5 - Multivariate MCMC",
    "section": "",
    "text": "Homework questions copyright Miles Chen. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported to the Dean of Students.\nModify this file with your answers and responses."
  },
  {
    "objectID": "projects/102C_hw5_Amaeya_Deshpande.html#academic-integrity-statement",
    "href": "projects/102C_hw5_Amaeya_Deshpande.html#academic-integrity-statement",
    "title": "Stats 102C, Homework 5 - Multivariate MCMC",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nBy including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students."
  },
  {
    "objectID": "projects/102C_hw5_Amaeya_Deshpande.html#results",
    "href": "projects/102C_hw5_Amaeya_Deshpande.html#results",
    "title": "Stats 102C, Homework 5 - Multivariate MCMC",
    "section": "Results",
    "text": "Results\nFor each coded message, write what you believe to be the correct deciphering of the text.\nYou can use your best_decode result and the power of the Internet (these are somewhat famous quotes).\n\nImportant: Answer Prohibition\nDo not post the answer to this question on Campuswire. Do not ask what the answer is. The problem does not ask you to include the code here, so I cannot verify if students actually did or did not attempt the problem on their own machine. As such, do not share what the answer is to this problem. Plus, it’s fun to see the code run and I want you to experience that.\n\n\nYour answers:\nMessage 1\ntrue message: WILBUR DIDNT WANT FOOD HE WANTED LOVE HE WANTED A FRIEND SOMEONE WHO WOULD PLAY WITH HIM\nMessage 2\ntrue message: LIVE TO SEE SUCH TIMES BUT THAT IS NOT FOR THEM TO DECIDE ALL WE HAVE TO DECIDE IS WHAT TO DO WITH THE TIME THAT IS GIVEN TO US\nMessage 3\ntrue message: FOR INSTANCE, ON THE PLANET EARTH, MAN HAD ALWAYS ASSUMED THAT HE WAS MORE INTELLIGENT THAN DOLPHINS BECAUSE HE HAD ACHIEVED SO MUCH-THE WHEEL, NEW YORK, WARS AND SO ON-WHILST ALL THE DOLPHINS HAD EVER DONE WAS MUCK ABOUT IN THE WATER HAVING A GOOD TIME. BUT CONVERSELY, THE DOLPHINS HAD ALWAYS BELIEVED THAT THEY WERE FAR MORE INTELLIGENT THAN MAN-FOR PRECISELY THE SAME REASONS"
  },
  {
    "objectID": "projects/102c_hw4_output_Amaeya_Deshpande.html",
    "href": "projects/102c_hw4_output_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "",
    "text": "Homework Questions, copyright Miles Chen. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported to the Dean of Students.\nModify this file with your answers and responses."
  },
  {
    "objectID": "projects/102c_hw4_output_Amaeya_Deshpande.html#academic-integrity-statement",
    "href": "projects/102c_hw4_output_Amaeya_Deshpande.html#academic-integrity-statement",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nBy including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students."
  },
  {
    "objectID": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-1---transition-matrix-and-stationary-distribution-two-state-case",
    "href": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-1---transition-matrix-and-stationary-distribution-two-state-case",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 1 - Transition Matrix and Stationary Distribution (Two state case)",
    "text": "Problem 1 - Transition Matrix and Stationary Distribution (Two state case)\nImagine a two-state Markov chain. With state 1 representing CA and state 2 representing TX.\nLet’s pretend that each year, 9% of Californians move to TX and that 12% of Texans move to CA.\nCreate and display a 2x2 transition matrix \\(\\mathbb{P}\\) in R to represent the transition probabilities.\nUsing algebra, find the stationary distribution \\(\\boldsymbol{\\pi}\\), so that \\(\\boldsymbol{\\pi}\\mathbb{P} = \\boldsymbol{\\pi}\\).\n\nP &lt;- rbind(c(0.91, 0.09), c(0.12, 0.88))\nw &lt;- c(4/7, 3/7)\n\nWe can use algebra to find the stationary distribution by creating a system of equations:\n0.91w1 + 0.12w2 = w1 0.09w1 + 0.88w2 = w2 -&gt; 0.09w1 = 0.12w2 -&gt; w2 = 0.75w1\nWe also know that w1 + w2 = 1 so…\n0.91w1 + 0.12(0.75w1) + 0.09w1 + 0.88(0.75w1) = 1 -&gt; 1.75w1 = 1\nWhen we divide, we get w1 = 4/7 so knowing that w1 + w2 = 1, we get that w2 = 3/7\nFind the left eigenvector of \\(\\mathbb{P}\\) and normalize it (so it sums to 1). Does it match the stationary distribution you found?\n\neig_vectors &lt;- eigen(t(P))$vectors\nstationary &lt;- t(eig_vectors[,1])\nstationary / (sum(stationary))\n\n          [,1]      [,2]\n[1,] 0.5714286 0.4285714\n\n\nYes, this matches the stationary distribution found using algebra."
  },
  {
    "objectID": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-2---transition-matrix-and-stationary-distribution-island-example",
    "href": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-2---transition-matrix-and-stationary-distribution-island-example",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 2 - Transition Matrix and Stationary Distribution (island example)",
    "text": "Problem 2 - Transition Matrix and Stationary Distribution (island example)\nLook at the example with the politician visiting the island chain in chapter 7 of the textbook, Doing Bayesian Data Analysis. Also see Lecture 5-3.\nImagine another chain of 7 islands where the target distribution is equal to the probabilities of a binomial distribution with n = 6 and p = 0.6.\nThis ‘nation’ has 7 islands in a chain numbered from 0 to 6. Island 0 has prob = \\(\\binom{6}{0}(.6)^0(.4)^6\\) = dbinom(0, 6, 0.6), Island 1 has prob = \\(\\binom{6}{1}(.6)^1(.4)^5\\) = dbinom(1, 6, 0.6), Island 6 has prob = dbinom(6, 6, 0.6), etc.\nUse the same algorithm as the politician to figure out the transition probabilities. Create and print out the full 7 x 7 transition matrix \\(\\mathbb{P}\\). Populate the matrix with actual decimal values, and not symbols (round to 4 decimal places for display purposes).\nStart with the initial distribution: \\(\\boldsymbol{\\pi}^{(1)}\\) = c(0, 0, 0, 1, 0, 0, 0)\nMultiply \\(\\boldsymbol{\\pi}^{(n)}\\) by \\(\\mathbb{P}\\) 6 times and print the results after each iteration. (Print the distribution of \\(\\boldsymbol{\\pi}^{(2)}\\), \\(\\boldsymbol{\\pi}^{(3)}\\), … \\(\\boldsymbol{\\pi}^{(7)}\\))\nFind the stationary distribution of the chain by finding the left eigenvector of the transition matrix and normalizing it. Check (using all.equal()) to see if it is equal to the target distribution (a binomial distribution with \\(n = 6\\) and \\(p = 0.6\\))\n\ntarget &lt;- dbinom(0:6, 6, 0.6)\nP &lt;- matrix(rep(0, 49), nrow = 7)\nfor (i in 1:7) {\n  if (i &lt; 7) {\n    if (target[i] &lt; target[i + 1]) {\n      P[i, i + 1] &lt;- 0.5\n    }\n    else {\n      P[i, i + 1] &lt;- 0.5 * (target[i + 1] / target[i])\n    }\n  }\n  if (i &gt; 1) {\n    if (target[i] &lt; target[i - 1]) {\n      P[i, i - 1] &lt;- 0.5\n    }\n    else {\n      P[i, i - 1] &lt;- 0.5 * (target[i - 1] / target[i])\n    }\n  }\n  P[i, i] &lt;- 1 - sum(P[i,])\n}\nround(P, 4)\n\n       [,1]   [,2]   [,3]   [,4]   [,5]  [,6]  [,7]\n[1,] 0.5000 0.5000 0.0000 0.0000 0.0000 0.000 0.000\n[2,] 0.0556 0.4444 0.5000 0.0000 0.0000 0.000 0.000\n[3,] 0.0000 0.1333 0.3667 0.5000 0.0000 0.000 0.000\n[4,] 0.0000 0.0000 0.2500 0.2500 0.5000 0.000 0.000\n[5,] 0.0000 0.0000 0.0000 0.4444 0.2556 0.300 0.000\n[6,] 0.0000 0.0000 0.0000 0.0000 0.5000 0.375 0.125\n[7,] 0.0000 0.0000 0.0000 0.0000 0.0000 0.500 0.500\n\npi_1 = c(0, 0, 0, 1, 0, 0, 0)\npi_curr &lt;- pi_1\nfor (i in 2:7) {\n  pi_next &lt;- pi_curr %*% P\n  print(round(pi_next, 4))\n  pi_curr &lt;- pi_next\n}\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]    0    0 0.25 0.25  0.5    0    0\n     [,1]   [,2]   [,3]   [,4]   [,5] [,6] [,7]\n[1,]    0 0.0333 0.1542 0.4097 0.2528 0.15    0\n       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]\n[1,] 0.0019 0.0354 0.1756 0.2919 0.3445 0.1321 0.0187\n       [,1]   [,2]  [,3]   [,4] [,5]   [,6]   [,7]\n[1,] 0.0029 0.0401 0.155 0.3139  0.3 0.1622 0.0259\n       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]\n[1,] 0.0037 0.0399 0.1553 0.2893 0.3147 0.1638 0.0332\n       [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]\n[1,] 0.0041 0.0403 0.1493 0.2899 0.307 0.1724 0.0371\n\neig_vectors &lt;- eigen(t(P))$vectors\nstationary &lt;- t(eig_vectors[,1])\nstationary &lt;- as.vector(stationary / (sum(stationary)))\nall.equal(target, stationary)\n\n[1] TRUE\n\n\nMultiply \\(\\boldsymbol{\\pi}^{(1)}\\) by \\(\\mathbb{P}\\) 500 times to get \\(\\boldsymbol{\\pi}^{(501)}\\). Show the results after the final iteration. Do NOT show the steps in between. Did the distribution converge to the stationary distribution?\n\npi_curr &lt;- pi_1\npi_next &lt;- pi_1\nfor (i in 2:500) {\n  pi_next &lt;- pi_curr %*% P\n  pi_curr &lt;- pi_next\n}\nprint(round(pi_next, 4))\n\n       [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]\n[1,] 0.0041 0.0369 0.1382 0.2765 0.311 0.1866 0.0467\n\n\nThe distribution does converge towards the stationary distribution. The values are not identical, but they are very similar."
  },
  {
    "objectID": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-3---mcmc-metropolis-algorithm-for-the-island-hopping",
    "href": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-3---mcmc-metropolis-algorithm-for-the-island-hopping",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 3 - MCMC (Metropolis Algorithm) for the island hopping",
    "text": "Problem 3 - MCMC (Metropolis Algorithm) for the island hopping\nWrite code to create a Markov chain using the Metropolis Algorithm for the same island nation in problem 2.\n\ntarget &lt;- function(x){ \n  ifelse(x %in% 0:6, dbinom(x, 6, 0.6), 0)\n}\npropose &lt;- function(x) { \n  current + sample(c(1, -1), size = 1)\n}\nn &lt;- 10^5\nresults1 &lt;- c(0, rep(NA, n - 1))\nset.seed(1)\nfor (t in 1:(n-1)) {\n  current &lt;- results1[t]\n  proposed &lt;- propose(current)\n  pmove &lt;- target(proposed) / target(current)\n  u &lt;- runif(1)\n  if(u &lt; pmove) {\n    results1[t + 1] &lt;- proposed\n  } else {\n    results1[t + 1] &lt;- current\n  }\n}\ncounts &lt;- table(results1)\nres &lt;- rbind(counts/n , dbinom(0:6, 6, 0.6)) \nrownames(res) &lt;- c(\"empirical\", \"target\")\nbarplot(res, beside = TRUE, legend.text = row.names(res), \n        args.legend = list(x = 7))\n\n\n\n\n\n\n\nchisq.test(counts, p = dbinom(0:6, 6, 0.6))\n\n\n    Chi-squared test for given probabilities\n\ndata:  counts\nX-squared = 17.037, df = 6, p-value = 0.009149\n\n\nWhile it is not strongly reflected in the barplot, the chi-squared goodness-of-fit test with p-value less than 0.05 provides evidence to say that the values produced by the Markov chain do not come from the target distribution.\n\nresults2 &lt;- c(6, rep(NA, n - 1))\nset.seed(2)\nfor (t in 1:(n-1)) {\n  current &lt;- results2[t]\n  proposed &lt;- propose(current)\n  pmove &lt;- target(proposed) / target(current)\n  u &lt;- runif(1)\n  if(u &lt; pmove) {\n    results2[t + 1] &lt;- proposed\n  } else {\n    results2[t + 1] &lt;- current\n  }\n}\ncounts &lt;- table(results2)\nres &lt;- rbind(counts/n , dbinom(0:6, 6, 0.6)) \nrownames(res) &lt;- c(\"empirical\", \"target\")\nbarplot(res, beside = TRUE, legend.text = row.names(res), \n        args.legend = list(x = 7))\n\n\n\n\n\n\n\nchisq.test(counts, p = dbinom(0:6, 6, 0.6))\n\n\n    Chi-squared test for given probabilities\n\ndata:  counts\nX-squared = 6.9061, df = 6, p-value = 0.3296\n\n\nBased on the barplot and the chi-squared goodness-of-fit test with p-value greater than 0.05, we do not have enough evidence to say that the values produced by the Markov chain do not come from the target distribution.\nRun the Metropolis Algorithm to create two Markov chains, each of length 10^5. For the first chain, start at x = 0 and use set.seed(1). For the second, start at x = 6 and use set.seed(2).\nFor each completed chain, print out a table of the resulting relative frequencies. Make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution. Use a chi-squared goodness-of-fit test to see if the generated values fit the expected probabilities. Be sure to comment on the graph and results of the test."
  },
  {
    "objectID": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-4---mcmc-metropolis-algorithm-for-a-single-continuous-random-variable",
    "href": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-4---mcmc-metropolis-algorithm-for-a-single-continuous-random-variable",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 4 - MCMC (Metropolis Algorithm) for a single continuous random variable",
    "text": "Problem 4 - MCMC (Metropolis Algorithm) for a single continuous random variable\nThe logisitic distribution is a unimodal and symmetric distribution, where the CDF is a logistic curve. The shape is similar to a normal distribution, but has heavier tails (though not as heavy as a Cauchy distribution).\nWe will compare Rejection Sampling to the Metropolis Algorithm for producing a sample from a distribution.\nThe PDF is:\n\\[f(x; \\mu, s) = \\frac{1}{s} \\frac{e^{-(\\frac{x-\\mu}{s})} }{\\left( 1 + e^{-(\\frac{x-\\mu}{s})} \\right)^2}\\]\nLuckily, this is implemented for us in R with dlogis(), which you are allowed to use to calculate the probability density of a (proposed) value.\nWe will generate two samples drawn from a logistic distribution with mean = 0 and scale = 1.\n\\[f(x; \\mu = 0, s=1) = \\frac{e^{-x} }{\\left( 1 + e^{-x} \\right)^2} = \\texttt{dlogis(x)}\\]\n\nTask 4A:\nFirst generate a sample from the logistic distribution using rejection sampling. Propose 10^5 values from a random uniform distribution from -20 to 20. Calculate the necessary constant M, and implement rejection sampling. If you propose 10^5 values, how many values do you end up accepting?\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your accepted values to the same plot (in a different color). Use the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions.\n\nset.seed(4)\nn &lt;- 10^5\nproposed &lt;- runif(n, -20, 20)\nf &lt;- function(x) { dlogis(x) }\ng &lt;- function(x) { 1/40 }\nM &lt;- f(0) / g(0)\nr_x &lt;- f(proposed) / (M * g(proposed))\nu &lt;- runif(n)\naccept &lt;- u &lt; r_x\naccepted_vals &lt;- proposed[accept]\nprint(paste(\"Number of values accepted:\", length(accepted_vals)))\n\n[1] \"Number of values accepted: 9999\"\n\nt_cdf &lt;- function(x) { plogis(x) }\ne_cdf &lt;- ecdf(accepted_vals)\ncurve(t_cdf(x), from = -10, to = 10, col = \"black\", lwd = 2, \n      ylab = \"CDF\")\nlines(e_cdf, col = \"blue\")\nlegend(\"topleft\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"), \n       col = c(\"black\", \"blue\"), lwd = 1)\n\n\n\n\n\n\n\nks.test(accepted_vals, plogis)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  accepted_vals\nD = 0.0069769, p-value = 0.7151\nalternative hypothesis: two-sided\n\n\n\n\nTask 4B:\nUse the Metropolis algorithm to generate values from the logisitic distribution.\nFor your proposal distribution, use a random uniform distribution that draws a random value between \\(X_{current} - 1\\) and \\(X_{current} + 1\\).\nAs a reminder, the steps of the algorithm are as follows:\n\nPropose a single value from the proposal distribution.\nCalculate the probability of moving = min(1, P(proposed)/P(current))\nDraw a random value to decide if you will move or not. If you move, update the current position. If you do not move, keep the current position for another iteration.\nRepeat.\n\nStart at the terrible location \\(x^{(1)} = -19\\).\nRun the Markov Chain for 10,000 iterations. Plot the first 1000 values of the chain and eyeball where you think the chain starts has finished ‘burning-in’ and is now drawing values from the target distribution. Throw away those initial values.\nPlot a density histogram of the remaining values and add the density of the logistic distribution to the histogram.\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your values (after removing burn-in) to the same plot. Use the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions.\n\nset.seed(10)\ntarget &lt;- function(x) {\n  dlogis(x)\n}\npropose &lt;- function(x) {\n  runif(1, x - 1, x + 1)\n}\nresults &lt;- rep(NA, 10^4)\nresults[1] &lt;- -19 \nfor(i in 1:(10^4 - 1)){\n  current &lt;- results[i]\n  proposed &lt;- propose(current)\n  p_move &lt;- min(1, target(proposed) / target(current))\n  results[i + 1] &lt;- ifelse(runif(1) &lt; p_move, proposed, current)\n}\nplot(results[1:1000], type = \"l\")\n\n\n\n\n\n\n\nkept_vals &lt;- results[-(1:150)]\nhist(kept_vals, probability = TRUE, breaks = 30)\ncurve(dlogis(x), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nt_cdf &lt;- function(x) { plogis(x) }\ne_cdf &lt;- ecdf(kept_vals)\ncurve(t_cdf(x), from = -10, to = 10, col = \"black\", lwd = 2, \n      ylab = \"CDF\")\nlines(e_cdf, col = \"blue\")\nlegend(\"topleft\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"), \n       col = c(\"black\", \"blue\"), lwd = 1)\n\n\n\n\n\n\n\nks.test(kept_vals, plogis)\n\nWarning in ks.test.default(kept_vals, plogis): ties should not be present for\nthe Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  kept_vals\nD = 0.008648, p-value = 0.4528\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-5---mcmc---the-effect-of-sigma-in-the-proposal-distribution",
    "href": "projects/102c_hw4_output_Amaeya_Deshpande.html#problem-5---mcmc---the-effect-of-sigma-in-the-proposal-distribution",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 5 - MCMC - the effect of sigma in the proposal distribution",
    "text": "Problem 5 - MCMC - the effect of sigma in the proposal distribution\nWrite code to perform 50,000 iterations of the Metropolis Algorithm for a single continuous random variable.\nLet the PDF of the target distribution be:\n\\[f(x) = c \\cdot ( sin(x) + 2 )\\]\nfor \\(0 \\le x \\le 3 * \\pi\\), where c is some constant so that \\(\\int_0^{3\\pi} f(x) dx = 1\\).\nFor your proposal distribution, use a normal distribution, centered at the current value, with a standard deviation of \\(\\sigma\\), which we will adjust in this problem.\nBegin your Markov Chain at the location x = 2.\nKeep in mind that the probability of a value greater than \\(3 \\pi\\) or less than 0 is 0.\nGather 50,000 samples using MCMC three different times.\nThe first time, use a sigma of 0.1 for the proposal distribution.\nThe second time, use a sigma of 2.5 for the proposal distribution.\nThe third time, use a sigma = 20.\nKeep track of whether your proposed values are accepted or rejected, and print out the acceptance ratio.\nFor each MCMC run, print out the acceptance ratio, create a histogram of the sampled values, and plot the first 500 values of the chain plot(x[1:500], type = \"l\").\n\nset.seed(11)\nc &lt;- 1 / integrate(function(x) {sin(x) + 2}, 0, 3 * pi)$value\nf &lt;- function(x) {  \n  ifelse(x &lt; 0 || x &gt; 3 * pi, 0, c * (sin(x) + 2))\n}\nn &lt;- 50000\nresults &lt;- rep(NA, n)\nresults[1] &lt;- 2\naccepted &lt;- 0\nfor(i in 1:(10^4 - 1)){\n  current &lt;- results[i]\n  proposed &lt;- rnorm(1, current, 0.1)\n  p_move &lt;- min(1, f(proposed) / f(current))\n  if (runif(1) &lt; p_move) {\n    results[i + 1] &lt;- proposed\n    accepted &lt;- accepted + 1\n  }\n  else {\n    results[i + 1] &lt;- current\n  }\n}\nprint(paste(\"Acceptance ratio is:\", accepted / n))\n\n[1] \"Acceptance ratio is: 0.1965\"\n\nhist(results, probability = TRUE, breaks = 30)\ncurve(c * (sin(x) + 2), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nplot(results[1:500], type = \"l\")\n\n\n\n\n\n\n\nresults &lt;- rep(NA, n)\nresults[1] &lt;- 2\naccepted &lt;- 0\nfor(i in 1:(10^4 - 1)){\n  current &lt;- results[i]\n  proposed &lt;- rnorm(1, current, 2.5)\n  p_move &lt;- min(1, f(proposed) / f(current))\n  if (runif(1) &lt; p_move) {\n    results[i + 1] &lt;- proposed\n    accepted &lt;- accepted + 1\n  }\n  else {\n    results[i + 1] &lt;- current\n  }\n}\nprint(paste(\"Acceptance ratio is:\", accepted / n))\n\n[1] \"Acceptance ratio is: 0.12832\"\n\nhist(results, probability = TRUE, breaks = 30)\ncurve(c * (sin(x) + 2), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nplot(results[1:500], type = \"l\")\n\n\n\n\n\n\n\nresults &lt;- rep(NA, n)\nresults[1] &lt;- 2\naccepted &lt;- 0\nfor(i in 1:(10^4 - 1)){\n  current &lt;- results[i]\n  proposed &lt;- rnorm(1, current, 20)\n  p_move &lt;- min(1, f(proposed) / f(current))\n  if (runif(1) &lt; p_move) {\n    results[i + 1] &lt;- proposed\n    accepted &lt;- accepted + 1\n  }\n  else {\n    results[i + 1] &lt;- current\n  }\n}\nprint(paste(\"Acceptance ratio is:\", accepted / n))\n\n[1] \"Acceptance ratio is: 0.03018\"\n\nhist(results, probability = TRUE, breaks = 30)\ncurve(c * (sin(x) + 2), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nplot(results[1:500], type = \"l\")"
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "",
    "text": "Homework Questions and text copyright Miles Chen. For personal use only. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported."
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html#academic-integrity",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html#academic-integrity",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nModifying the following statement with your name.\n“By including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students.”"
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html#reading-and-viewing",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html#reading-and-viewing",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Reading and Viewing:",
    "text": "Reading and Viewing:\n\nIntroducing Monte Carlo Methods with R: Section 2.1, Section 2.2, and Section 2.3\nKolmogorov-Smirnov Test on Youtube: https://www.youtube.com/watch?v=ZO2RmSkXK3c (This video covers the two-sample test, but we will conduct a one-sample test against a reference distribution)"
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-1---inverse-cdf-exponential-distribution",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-1---inverse-cdf-exponential-distribution",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 1 - Inverse CDF Exponential Distribution",
    "text": "Problem 1 - Inverse CDF Exponential Distribution\nWrite a function my_rexp(n, rate), that will generate n random values drawn from an exponential distribution with lambda = “rate” by using the inverse CDF method. Use runif() as your sole source of randomness.\nYou are not allowed to use any of the functions dexp(), pexp(), qexp(), or rexp() in your generating function.\nUse your function to generate 10,000 random values from an exponential distribution with lambda = 1. Do not print out the 10,000 values.\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your data to the same plot (in a different color). You can use R’s pexp() function when plotting the theoretic CDF.\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic exponential distribution. Be sure to print out the resulting p-value and comment on the sample produced by your function.\nOnce you complete the exercise for lambda = 1, repeat the exercise, this time generating 10000 values with lambda = 0.3. Plot the theoretic CDF vs empirical CDF and use the KS test.\n\nmy_rexp &lt;- function(n, rate){\n  u &lt;- runif(n)\n  - (log(1 - u)) / rate\n}\nset.seed(1)\nx &lt;- my_rexp(10^4, rate = 1)\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pexp(x_vals, 1)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pexp\", 1)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.388604591146829\"\n\n\nGiven that our p-value is larger than 0.05, we have no reason to believe that the sample provided by the function does not fit the expected exponential distribution.\n\nset.seed(2)\nx &lt;- my_rexp(10^4, rate = 0.3)\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pexp(x_vals, 0.3)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pexp\", 0.3)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.233370274376852\""
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-2---inverse-cdf---discrete-case",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-2---inverse-cdf---discrete-case",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 2 - Inverse CDF - Discrete Case",
    "text": "Problem 2 - Inverse CDF - Discrete Case\nWrite a function my_rbinom(n, size, prob), that will generate n random values drawn from a binomial distribution with size = size and probability of success = prob by using the inverse CDF method. Use runif() as your sole source of randomness.\nYou must use inverse CDF method. You will not get credit if you use a convolution.\nDo not use any of R’s binom functions. Do not use dbinom, pbinom, qbinom(), or rbinom()\nUse your function my_rbinom() to generate 10,000 values from a binomial distribution with n = 6, and p = 0.4.\nAfter generating 10,000 samples, make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution. (See lecture 3-2, slides 25 and 26)\nUse a chi-squared goodness-of-fit test to see if the generated values fit the expected probabilities. Be sure to comment on the graph and results of the test.\nUse your function my_rbinom() again. This time generate 10,000 values from a binomial distribution with n = 12, and p = 0.55. Make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution. No need to run a chi-squared test for this data.\n\n# write your code here\nmy_rbinom &lt;- function(n, size, prob){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    cum_prob &lt;- 0\n    u &lt;- runif(1)\n    for (k in 0:size) {\n      cum_prob &lt;- cum_prob + (choose(size, k) * (prob^k) * ((1 - prob)^(size - k)))\n      if (u &lt;= cum_prob) {\n        samp[i] &lt;- k\n        break\n      }\n    }\n  }\n  samp\n}\nset.seed(3)\nmy_samp &lt;- my_rbinom(10^4, 6, 0.4)\ne_pmf &lt;- table(my_samp) / 10^4\nt_pmf &lt;- dbinom(0:6, 6, 0.4)\ncomp &lt;- rbind(e_pmf, t_pmf)\nbarplot(comp, beside = TRUE, legend.text = c(\"Empirical\", \"Theoretical\"))\n\n\n\n\n\n\n\ncs_test &lt;- chisq.test(x = table(my_samp), p = t_pmf)\nprint(paste(\"Chi-squared goodness-of-fit test p-value is:\", cs_test$p.value))\n\n[1] \"Chi-squared goodness-of-fit test p-value is: 0.358065469530775\"\n\n\nGiven the similarity of the bars in the barplot that our p-value is greater than 0.05, we have no reason to believe that the observed values do not match the theoretical probabilities.\n\nset.seed(4)\nmy_samp &lt;- my_rbinom(10^4, 12, 0.55)\ne_pmf &lt;- table(my_samp) / 10^4\nt_pmf &lt;- dbinom(0:12, 12, 0.55)\ncomp &lt;- rbind(e_pmf, t_pmf)\nbarplot(comp, beside = TRUE, legend.text = c(\"Empirical\", \"Theoretical\"))"
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-3---rng-based-on-inverse-cdf-and-convolutions",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-3---rng-based-on-inverse-cdf-and-convolutions",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 3 - RNG based on inverse CDF and convolutions",
    "text": "Problem 3 - RNG based on inverse CDF and convolutions\nUsing only runif() and/or rnorm() as sources of randomness, generate 10,000 (\\(10^4\\)) random samples from each of the following distributions. You are not allowed to use any of R’s other distribution functions for the generation of random values. Do NOT print out the actual values in your random sample.\nFor each distribution:\n\nAfter generating your 10000 samples, plot a density histogram of the resulting sample (breaks = 30, freq = FALSE). Plot the theoretic density in another color on top of the histogram. Comment on the plot. You are allowed use R’s density functions dchisq(), dt(), etc. when plotting the density function over the histogram\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your data to the same plot (in a different color). You can use R’s CDF functions pchisq(), pt(), etc. when plotting the CDF.\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions. Be sure to print out the resulting p-value and comment on the sample produced by your function.\n\n\nProblem 3a:\n\nBeta distribution with shape parameters 4 and 2\n\n\nmy_rbeta &lt;- function(n, alpha, beta){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    u &lt;- runif(1)\n    samp[i] &lt;- qbeta(u, alpha, beta)\n  }\n  samp\n}\nset.seed(5)\nx &lt;- my_rbeta(10^4, 4, 2)\nhist(x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Beta Distribution with Parameters 4 and 2\")\ncurve(dbeta(x, 4, 2), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pbeta(x_vals, 4, 2)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"topleft\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pbeta\", 4, 2)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.701120392284348\"\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are very similar. The shape of the sample generated from my function matches the shape of the beta distribution with parameters 4 and 2.\n\n\nProblem 3b:\n\nChi-squared distribution with 4 degrees of freedom\n\n\nmy_rchisq &lt;- function(n, df){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    u &lt;- runif(1)\n    samp[i] &lt;- qchisq(u, df)\n  }\n  samp\n}\nset.seed(6)\nx &lt;- my_rchisq(10^4, 4)\nhist(x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Chi-Squared Distribution with 4 DF\")\ncurve(dchisq(x, 4), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pchisq(x_vals, 4)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pchisq\", 4)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.663163223641714\"\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are pretty similar. The shape of the sample generated from my function matches the shape of the chi-squared distribution with 4 degrees of freedom.\n\n\nProblem 3c:\n\nt-distribution with 4 degrees of freedom\n\n\nmy_rt &lt;- function(n, df){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    u &lt;- runif(1)\n    samp[i] &lt;- qt(u, df)\n  }\n  samp\n}\nset.seed(7)\nx &lt;- my_rt(10^4, 4)\nhist(x, breaks = 30, freq = FALSE, ylim = c(0, 0.4), col = \"gray\",\n     main = \"t Distribution with 4 DF\")\ncurve(dt(x, 4), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pt(x_vals, 4)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pt\", 4)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.938900082969244\"\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are very similar. The shape of the sample generated from my function matches the shape of the t distribution with 4 degrees of freedom.\n\n\nProblem 3d:\n\nGamma distribution with shape parameter 4 and rate parameter 2.\n\n\nmy_rgamma &lt;- function(n, alpha, beta){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    u &lt;- runif(1)\n    samp[i] &lt;- qgamma(u, alpha, beta)\n  }\n  samp\n}\nset.seed(8)\nx &lt;- my_rgamma(10^4, 4, 2)\nhist(x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Gamma Distribution with Parameters 4 and 2\")\ncurve(dgamma(x, 4, 2), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pgamma(x_vals, 4, 2)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pgamma\", 4, 2)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.906083018017777\"\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are very similar. The shape of the sample generated from my function matches the shape of the gamma distribution with parameters 4 and 2."
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-4---rejection-sampling",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-4---rejection-sampling",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 4 - Rejection Sampling",
    "text": "Problem 4 - Rejection Sampling\nLet \\(f(x)\\) and \\(g(x)\\) be the target and candidate (proposal) distributions, respectively, in acceptance-rejection sampling.\n\\(f(x) = \\frac{1}{2} \\sin(x)\\) for \\(0 \\le x \\le \\pi\\)\n\\(g(x) = \\mbox{Unif}(0, \\pi)\\)\nFind the optimal constant \\(M = \\max \\frac{f(x)}{g(x)}\\).\nImplement the rejection sampling design, using runif(n, 0, pi) as your source of randomness. Generate a sample of at least 10,000 accepted values.\n\nf &lt;- function(x) { 0.5 * sin(x) }\ng &lt;- function(x) { 1 / pi }\nM &lt;- pi / 2\nset.seed(9)\nproposed_x &lt;- runif(10^5, 0, pi)\nr_x &lt;- f(proposed_x) / (M * g(proposed_x))\nU &lt;- runif(10^5)\naccepted &lt;- U &lt; r_x\naccepted_x &lt;- proposed_x[accepted]\nprint(paste(\"The empirical acceptance rate is:\", length(accepted_x) / 10^5))\n\n[1] \"The empirical acceptance rate is: 0.637\"\n\n\nWhat is your empirical acceptance rate?\nCreate a histogram of your generated (accepted) sample (breaks = 30, freq = FALSE). Add the theoretic PDF to the histogram.\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your data to the same plot (in a different color).\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions. Be sure to print out the resulting p-value and comment on the sample produced by your function.\nI have written a vectorized PDF and CDF function for you.\n\nsin_pdf &lt;- function(x) {\n  ifelse(x &gt; 0 & x &lt; pi, 0.5 * sin(x), 0)\n}\n\nsin_cdf &lt;- function(x) {\n  ifelse(x &lt; 0, 0, ifelse(x &lt; pi, 0.5 - 0.5 * cos(x), 1))\n}\n\n\nx_vals &lt;- x_vals &lt;- seq(0, max(accepted_x), length.out = 1000)\nhist(accepted_x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Accepted Sample Distribution\")\nlines(x_vals, sin_pdf(x_vals), col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"), cex = 0.5)\n\n\n\n\n\n\n\nplot(x_vals, sin_cdf(x_vals), type = \"l\", xlab = \"x\", ylab = \"CDF\", \n     col = \"black\", lwd = 2)\nlines(ecdf(accepted_x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(accepted_x, \"sin_cdf\")\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.200407310985682\"\n\n\nBased on the plots above and the p-value being greater than 0.05, we have no reason to believe that the sample we produced is significantly different from the theoretical distribution."
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-5---rejection-sampling",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-5---rejection-sampling",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 5 - Rejection Sampling",
    "text": "Problem 5 - Rejection Sampling\nUse rejection sampling to generate samples from the Beta distribution with shape parameters \\(a = 4\\) and \\(b = 6\\).\nThe PDF of this Beta distribution is:\n\\[f(x) = \\frac{1}{B(a,b)} x^{a - 1}(1-x)^{b - 1} = 504 x^{3}(1-x)^{5}\\]\nUse the Uniform (0,1) distribution as your trial distribution.\nUse calculus to solve for \\(M = \\max \\frac{f(x)}{g(x)}\\). Show your work. (The derivative is easy to find and can be easily factored to find the roots.)\nImplement the rejection sampling design, using runif(n) as your source of randomness. Generate a sample of at least 10,000 accepted values.\nTo find M, I first found f(x) / g(x) which is \\[504 x^{3}(1-x)^{5}\\]. The derivative is simplified to \\[-504 x^{2}(x-1)^{4}(8x-3)\\]. Thus, the roots are x = 1 and x = 3/8. Evaluating points of the function leads us to confirm that the global maximum is at x = 3/8. The function evaluated at x = 3/8 is approximately 2.535.\n\nf &lt;- function(x) { 504 * x^3 * (1 - x)^5 }\ng &lt;- function(x) { rep(1, length(x)) }\nM &lt;- 504 * (3/8)^3 * (1 - (3/8))^5\nset.seed(10)\nproposed_x &lt;- runif(10^5)\nr_x &lt;- f(proposed_x) / (M * g(proposed_x))\nU &lt;- runif(10^5)\naccepted &lt;- U &lt; r_x\naccepted_x &lt;- proposed_x[accepted]\nks_test &lt;- ks.test(accepted_x, \"pbeta\", 4, 6)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.722405755833897\"\n\n\nBased on our outputted p-value which is very large, we have no reason to believe that the sample produced in the code is significantly different from a sample taken from Beta(4, 6).\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions. You may use pbeta for the CDF. Be sure to print out the resulting p-value and comment on the sample produced by your function. (No additional plots necessary)"
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-6---empirical-supremum-rejection-sampling",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-6---empirical-supremum-rejection-sampling",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 6 - Empirical Supremum Rejection Sampling",
    "text": "Problem 6 - Empirical Supremum Rejection Sampling\nOne challenge of rejection sampling is finding the constant \\(M = \\max \\frac{f(x)}{g(x)}\\). Empirical Supremum rejection sampling estimates the quantity \\(M\\) with a value \\(\\hat c\\). The algorithm works much in the same was as rejection sampling, but it continually updates the value \\(\\hat c\\) if a new \\(x\\) is produced where \\(\\frac{f(x)}{g(x)}\\) is larger than the current estimate \\(\\hat c\\).\nRead: 6.3.3 Empirical Supremum Rejection Sampling from the following website:\nhttps://bookdown.org/rdpeng/advstatcomp/rejection-sampling.html#empirical-supremum-rejection-sampling\n(side note: Roger Peng earned his PhD in Statistics from UCLA and hosts the data-science podcast “Not so standard deviations” with Hilary Parker.)\nUse Empirical Supremum Rejection Sampling to generate samples from the normal distribution.\nThe target distribution f(x) will be the positive half of the standard normal distribution, which will have PDF:\n\\[f(x) = 2 \\times \\frac{1}{\\sqrt{2\\pi}} \\exp{(-x^2/2)}\\mbox{,   for } x \\ge 0\\]\nUse an exponential distribution with lambda = 1 as your trial (proposal) distribution.\n\\[g(x) = e^{-x} \\mbox{,   for } x \\ge 0\\]\nUnlike the example from lecture, DO NOT find the optimal constant M that will maximize the acceptance rates for the rejection sampling design.\nImplement Empirical Supremum Rejection Sampling. While the webpage describes a process that looks like it should be implemented with a loop, it is possible to achieve the same result in a more efficient manner without the need for any loops:\n\nPropose \\(n = 10000\\) values. The accepted sample will be smaller.\nUse runif and inverse CDF to generate \\(n\\) proposal values \\(X\\) from the exponential distribution.\nCalculate the ratio for all \\(n\\) values: \\(\\frac{f(X)}{g(X)}\\)\nEstimate \\(\\hat c\\) as the maximum \\(\\frac{f(X)}{g(X)}\\) you encounter.\nUse runif to generate \\(n\\) values of \\(U\\) to decide whether to accept or reject the proposed \\(X\\).\nreject all proposed X values that do not meet the rejection criteria.\n\nOnce you have generated samples from the folded normal distribution using rejection sampling, turn the accepted values into values from the standard normal distribution. Use runif to generate \\(S\\) to decide the sign of the accepted \\(X\\). The accepted \\(X\\) values will be positive or negative with probably 0.5.\nCreate a QQ-norm plot of your accepted sample or normally distributed values. Comment on the plot.\nSubset your accepted values to the first 1000. Perform a Shapiro-Wilk test shapiro.test() to test normality of these 1000 values. Comment on the results.\n\nf &lt;- function(x) { (2 / sqrt(2 * pi)) * exp(-x^2 / 2) }\ng &lt;- function(x) { exp(-x) }\nset.seed(11)\nproposed_x &lt;- runif(10^4)\nX &lt;- - log(proposed_x)\nratios &lt;- f(X) / g(X)\nc_hat &lt;- max(ratios)\nU &lt;- runif(10^4)\naccepted &lt;- U &lt; ratios / c_hat\naccepted_x &lt;- X[accepted]\nS &lt;- runif(length(accepted_x))\nsigns &lt;- ifelse(S &lt; 0.5, -1, 1)\naccepted_x &lt;- signs * accepted_x\nqqnorm(accepted_x)\nqqline(accepted_x, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nshapiro.test(accepted_x[1:1000])\n\n\n    Shapiro-Wilk normality test\n\ndata:  accepted_x[1:1000]\nW = 0.99873, p-value = 0.7092\n\n\nBased on the QQ plot, we see that the accepted sample conforms well to the normal distribution, as a majority of the sample falls on the 45 degree line. The p-value given by the Shapiro-Wilk test also supports this, as we have no reason to believe that our sample significantly differs from teh normal distribution."
  },
  {
    "objectID": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-7---bivariate-normal-distribution",
    "href": "projects/102c_hw3_output_Amaeya_Deshpande.html#problem-7---bivariate-normal-distribution",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 7 - Bivariate Normal Distribution",
    "text": "Problem 7 - Bivariate Normal Distribution\nGenerate 1000 random observations from a bivariate normal distribution.\n\\[\n\\mathbf{X} \\sim \\mathcal{N}_2 \\left (\n\\boldsymbol{\\mu}= \\left(\\begin{array}{c}\n   2 \\\\\n   -1 \\\\\n  \\end{array}\n  \\right),\n  \\boldsymbol{\\Sigma} = \\left( {\\begin{array}{cc}\n   3 & -1.5 \\\\\n   -1.5 & 3 \\\\\n  \\end{array} } \\right) \\right )\n\\]\nImplement the Box-Muller transform to generate standard normal values. Use runif() as your only source of randomness.\nOnce you have standard normal values, apply the necessary transform to get the desired bivariate distribution. You may use chol to find the Cholesky decomposition of a matrix.\nCreate a plot the resulting generated data.\n\nmu &lt;- c(2, -1)\nsigma &lt;- cbind(c(3, -1.5), c(-1.5, 3))\nset.seed(12)\nU1 &lt;- runif(2 * 10^3)\nU2 &lt;- runif(2 * 10^3)\nZ1 &lt;- sqrt(-2 * log(U1)) * cos(2 * pi * U2)\nZ2 &lt;- sqrt(-2 * log(U1)) * sin(2 * pi * U2)\nZ &lt;- rbind(Z1, Z2)\nA &lt;- t(chol(sigma))\nX &lt;- mu + A %*% Z\nplot(X[1,], X[2,], pch = 19, cex = 0.5)"
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "",
    "text": "Homework Questions and text copyright Miles Chen. For personal use only. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported."
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html#academic-integrity",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html#academic-integrity",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nModifying the following statement with your name.\n“By including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students.”"
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html#reading",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html#reading",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Reading",
    "text": "Reading\nReading is important!\n\nChapter 2 section 2 in Introducing Monte Carlo Methods with R\nChapter 3 sections 1-3 in Introducing Monte Carlo Methods with R"
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-1---estimate-pi-poorly",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-1---estimate-pi-poorly",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 1 - Estimate pi (poorly)",
    "text": "Problem 1 - Estimate pi (poorly)\nA simple Monte Carlo Exercise … Get an estimate of pi by using random uniform numbers. (It won’t be a very good estimate.)\nIn this first exercise, we can see how a simple source of randomness (in our case, R’s runif() function) can be used to estimate tough quantities.\nWe will find an estimate of pi by estimating the ratio between the area of a circle and its encompassing square.\n\ns &lt;- seq(-1, 1, by = 0.001)\nposf &lt;- sqrt(1 - s ^ 2)\nplot(s, posf, type = \"l\", asp = 1, ylim = c(-1, 1))\nlines(s, -1 * posf)\nsegments(-1, -1, -1, 1)\nsegments(-1, -1, 1, -1)\nsegments(1, 1, -1, 1)\nsegments(1, 1, 1, -1)\n\n\n\n\n\n\n\n\nTo calculate the area of the circle analytically, we would need to integrate the function drawing the upper semi-circle and then multiply that by 2. This process requires the use of trig substitutions, and while doable, can illustrate a time where the analytic solution is not easy.\n\\[Area = 2 \\times \\int_{-1}^1 \\sqrt{1 - x^2} dx\\]\nFor the Monte-Carlo approach, we will use runif(n, min = -1, max=1) to generate a bunch of random pairs of x and y coordinates. We will see how many of those random uniform points fall within the circle. This is easy - just see if \\(x^2 + y^2 \\le 1\\). The total area of the square is 4. The total area of the circle is pi. Thus, the proportion of coordinates that satisfy the inequality \\(x^2 + y^2 \\le 1 \\approx \\pi/4\\).\nInstructions:\n\ncreate a vector x of n random values between -1 and 1. I suggest starting with n = 500\ncreate a vector y of n random values between -1 and 1. Use the two vectors to make coordinate pairs.\ncalculate which of points satisfy the inequality for falling inside the circle.\nPrint out your estimate of pi by multiplying the proportion by 4.\nplot each of those (x,y) coordinate pairs. Use pch = 20. Color the points based on whether they fall in the circle or not.\n\n\nset.seed(123)\nx &lt;- runif(500, min = -1, max = 1)\ny &lt;- runif(500, min = -1, max = 1)\ninside &lt;- x^2 + y^2 &lt;= 1\nprop_inside &lt;- sum(inside) / 500\npi_est &lt;- prop_inside * 4\nprint(paste(\"Estimate for pi: \", pi_est))\n\n[1] \"Estimate for pi:  3.2\"\n\ns &lt;- seq(-1, 1, by = 0.001)\nposf &lt;- sqrt(1 - s ^ 2)\nplot(s, posf, type = \"l\", asp = 1, ylim = c(-1, 1))\nlines(s, -1 * posf)\nsegments(-1, -1, -1, 1)\nsegments(-1, -1, 1, -1)\nsegments(1, 1, -1, 1)\nsegments(1, 1, 1, -1)\npoints(x, y, pch = 20, col = ifelse(inside, \"green\", \"red\"))"
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-2",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-2",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 2",
    "text": "Problem 2\nEstimate the integral of the following function by using Monte Carlo integration.\n\\[I = \\int_0^5 h(x) dx = \\int_0^5 \\exp(-0.5 (x-2)^2 - 0.1 |\\sin(2x)|) dx\\]\nWe want to estimate the value of I, the area under the curve from 0 to 5. We can say that area is equal to the average value of \\(h(x)\\) times the width:\n\\[I = 5 \\cdot \\mathbb{E}_f[h(X)]\\]\n\n# what the function looks like\nh &lt;- function(x) {exp(-0.5 * (x - 2) ^ 2 - 0.1 * abs( sin(2 * x) ) )}\nv &lt;- seq(0, 5, by = 0.01)\nplot(v, h(v), type = \"l\", xlim = c(-0.5, 5.5))\npolygon(c(0, v, 5), c(0, h(v), 0), col = 'khaki')\nrect_ht = 0.458394 # height of rectangle is the Expected value of h(x)\npolygon(c(0, 0, 5, 5), c(0, rect_ht, rect_ht, 0), col = rgb(0, 0, 1, .2))\n\n\n\n\n\n\n\n\nFor MC integration, we estimate\n\\[\\mathbb{E}_f[h(X)] = \\int_\\mathcal{X} h(x)f(x) dx\\]\nFor this first problem, we will use the uniform distribution on the interval (0,5) to draw samples of x. Thus, the PDF, \\(f(x)\\) is\n\\[f(x) = 1/5\\]\nThus,\n\\[ I  = 5 \\cdot \\mathbb{E}_f[h(X)] = 5 \\cdot \\int_0^5 h(x) f(x) dx \\approx \\frac{5}{N}\\sum_{j = 1}^N h(x_j) = \\hat I\\]\nWhere \\(x_j \\sim \\text{Unif}(0,5)\\)\nGenerate a sample of 5000 values of x to estimate \\(\\hat{I}\\). Use runif() to generate the random uniform values.\nCreate a plot using the cumulative mean (aka running mean) of the first 500 values in the sample to show how the estimate of the integral ‘settles’ over the course of the sampling.\nDo this two more times (each with a different starting seed), for a total of three samples of 5000 values each. Plot the cumulative mean of the first 500 values of each sample. There will be three different lines on this plot.\nIn your plot also include the ‘true’ value of the integral, 2.29583, as a horizontal line.\nPrint out your three estimates of \\(I\\).\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nh &lt;- function(x) {exp(-0.5 * (x - 2) ^ 2 - 0.1 * abs( sin(2 * x) ) )}\n\n## Monte Carlo Integration using uniform random sampling\nn = 5000 # total number of values to generate\nk = 500  # number of values to plot\n\n# First series\nset.seed(1)\nx1 &lt;- runif(n, min = 0, max = 5)\nh_vals1 &lt;- h(x1)\nI_est1 &lt;- (5 / n) * sum(h_vals1)\ncum_mean1 &lt;- cummean(h_vals1[1:k]) * 5\n\n# second series\nset.seed(2)\nx2 &lt;- runif(n, min = 0, max = 5)\nh_vals2 &lt;- h(x2)\nI_est2 &lt;- (5 / n) * sum(h_vals2)\ncum_mean2 &lt;- cummean(h_vals2[1:k]) * 5\n\n# third series\nset.seed(3)\nx3 &lt;- runif(n, min = 0, max = 5)\nh_vals3 &lt;- h(x3)\nI_est3 &lt;- (5 / n) * sum(h_vals3)\ncum_mean3 &lt;- cummean(h_vals3[1:k]) * 5\n\nprint(paste(\"Estimate for I with seed 1:\", I_est1))\n\n[1] \"Estimate for I with seed 1: 2.27626784048365\"\n\nprint(paste(\"Estimate for I with seed 2:\", I_est2))\n\n[1] \"Estimate for I with seed 2: 2.26402222241221\"\n\nprint(paste(\"Estimate for I with seed 3:\", I_est3))\n\n[1] \"Estimate for I with seed 3: 2.29369518890943\"\n\n\n\nplot(x = c(1, 500), y = c(2.29583, 2.29583), type = \"l\", ylim = c(0, 5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:500, y = cum_mean1, col = \"blue\")\nlines(x = 1:500, y = cum_mean2, col = \"red\")\nlines(x = 1:500, y = cum_mean3, col = \"green\")\nlegend(\"topright\", legend = c(\"Estimate 1\", \"Estimate 2\", \"Estimate 3\", \n                  \"True Value\"), col = c(\"blue\", \"red\", \"green\", \"black\"), lty = 1)"
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-2b",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-2b",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 2b",
    "text": "Problem 2b\nWe can estimate the variance of \\(\\bar h_n\\) with \\(v_n = \\frac{1}{n^2}\\sum_{j=1}^N [ h(x_j) - \\bar h_n]^2\\).\nWhat is variance of the estimate \\(\\hat I\\)?\nWrite the formula for estimated variance of I-hat with latex:\n\\[Var(\\hat I) = \\frac{25}{n^2}\\sum_{j=1}^N [ h(x_j) - \\bar h_n]^2\\]\nLook at your first Monte Carlo series and estimate the running variance of \\(\\hat I\\) as n goes from 1 to 500.\nCreate a plot of the \\(\\hat I\\) with 95% confidence bounds above and below.\n\ncum_est &lt;- cum_mean1[1:500]\ncum_var &lt;- numeric(500)\nfor (i in 1:500) {\n  cum_var[i] &lt;- var(h_vals1[1:i]) * 25\n}\nupper_bound &lt;- cum_est + qnorm(0.975) * sqrt(cum_var / 1:500)\nlower_bound &lt;- cum_est + qnorm(0.025) * sqrt(cum_var / 1:500)\nplot(x = 1:500, y = cum_est, type = \"l\", ylim = c(0, 5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:500, y = upper_bound, col = \"blue\")\nlines(x = 1:500, y = lower_bound, col = \"blue\")"
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-3a",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-3a",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 3a",
    "text": "Problem 3a\nFind \\(Z_r\\) so that \\(\\int_{0}^5 \\frac{1}{Z_r} \\text{Normal PDF (2,1)} = 1\\). Hint: figure out how much of the distribution is ‘cut off’ at 0 and 5, and find \\(Z_r\\) accordingly.\n\nz_upper &lt;- (5 - 2) / 1\nz_lower &lt;- (0 - 2) / 1\nz_r &lt;- pnorm(z_upper) - pnorm(z_lower)\nz_r\n\n[1] 0.9759"
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-3b",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-3b",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 3b",
    "text": "Problem 3b\nPerform Importance sampling.\nGenerate a sample of 5000 values of x to estimate \\(\\hat{I}\\).\nThis time, do not sample values from runif(), but rather from rnorm() with mean 2 and standard deviation 1. Make sure you remove values of x below 0 and above 5. Adjust how the estimate of the integral is calculated according to importance sampling as well as using the normalizing constant with the normal density.\nCreate a plot using the cumulative mean (aka running mean) of the first 500 values in the sample to show how the estimate of the integral ‘settles’ over the course of the sampling.\nDo this two more times (each with a different starting seed), for a total of three samples of 5000 values each. Plot the cumulative mean of the first 500 values of each sample.\nIn your plot also include the ‘true’ value of the integral, 2.29583, as a horizontal line.\nWhen you create your plot, also adjust the axes to fit the samples better.\nFinally, print out your three estimates of \\(I\\), and also comment on how quickly the method using importance sampling converges to the expected value versus the uniform sampling method.\n\n## Monte Carlo Integration using importance sampling\nn = 5000\nk = 500\n\n# First series\nset.seed(1)\nx1 &lt;- rnorm(n, 2, 1)\nx1 &lt;- x1[x1 &gt; 0 & x1 &lt; 5]\nh_vals1 &lt;- h(x1)\nweights1 &lt;- (1 / 5) / (dnorm(x1, 2, 1) / z_r)\nI_est1 &lt;- (5 / length(x1)) * sum(h_vals1 * weights1)\ncum_mean1 &lt;- 5 * cumsum(h_vals1[1:k] * weights1[1:k]) / seq_along(x1[1:k])\n\n# Second series\nset.seed(2)\nx2 &lt;- rnorm(n, 2, 1)\nx2 &lt;- x2[x2 &gt; 0 & x2 &lt; 5]\nh_vals2 &lt;- h(x2)\nweights2 &lt;- (1 / 5) / (dnorm(x2, 2, 1) / z_r)\nI_est2 &lt;- (5 / length(x2)) * sum(h_vals2 * weights2)\ncum_mean2 &lt;- 5 * cumsum(h_vals2[1:k] * weights2[1:k]) / seq_along(x2[1:k])\n\n# Third series\nset.seed(3)\nx3 &lt;- rnorm(n, 2, 1)\nx3 &lt;- x3[x3 &gt; 0 & x3 &lt; 5]\nh_vals3 &lt;- h(x3)\nweights3 &lt;- (1 / 5) / (dnorm(x3, 2, 1) / z_r)\nI_est3 &lt;- (5 / length(x3)) * sum(h_vals3 * weights3)\ncum_mean3 &lt;- 5 * cumsum(h_vals3[1:k] * weights3[1:k]) / seq_along(x3[1:k])\n\nprint(paste(\"Estimate for I with seed 1:\", I_est1))\n\n[1] \"Estimate for I with seed 1: 2.29700774779758\"\n\nprint(paste(\"Estimate for I with seed 2:\", I_est2))\n\n[1] \"Estimate for I with seed 2: 2.29365541728644\"\n\nprint(paste(\"Estimate for I with seed 3:\", I_est3))\n\n[1] \"Estimate for I with seed 3: 2.2941144034467\"\n\n\n\nplot(x = c(1, 500), y = c(2.29583, 2.29583), type = \"l\", ylim = c(2, 2.5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:k, y = cum_mean1, col = \"blue\")\nlines(x = 1:k, y = cum_mean2, col = \"red\")\nlines(x = 1:k, y = cum_mean3, col = \"green\")\nlegend(\"bottomright\", legend = c(\"Estimate 1\", \"Estimate 2\", \"Estimate 3\", \n              \"True Value\"), col = c(\"blue\", \"red\", \"green\", \"black\"), lty = 1)\n\n\n\n\n\n\n\n\nWhat we find is that the values converge much faster. The reason for this is that \\(h(x)\\) and \\(g(x)\\) are nearly proportional to each other. That is to say that the variance of \\((h(x)/g(x))\\) is very low."
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html#self-normalizing-importance-sampling",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html#self-normalizing-importance-sampling",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Self-Normalizing Importance Sampling",
    "text": "Self-Normalizing Importance Sampling\nIn the previous problem, we had to find the normalizing constant \\(Z_C\\), so that the density from which we drew values would still integrate to 1. We drew values from the normal density, which is well understood, so calculating this constant \\(Z_c\\) was not too difficult.\nIn some situations, however, it is not always possible to calculate the normalizing constants for our densities.\nWe may only know a function \\(q(x)\\) that is proportional to the target density \\(f(x)\\). If is equal to \\(q(x)\\) divided by some unknown normalizing constant \\(Z_q\\):\n\\[f(x) = q(x)/Z_q\\]\nSimilarly, there may be a function \\(r(x)\\) that is proportional to the proposal distribution \\(g(x)\\). In this scenario, we assume we are able to generate values from \\(g(x)\\), but we just don’t know the exact function equation for \\(g(x)\\). Instead, we know the function \\(r(x)\\) is proportional to \\(g(x)\\), and the normalizing constant \\(Z_r\\) is unknown.\n\\[g(x) = r(x)/Z_r\\]\n\nAs it applies to the current example\nIn our scenario, we can generate values from the proposal distribution \\(g(x)\\). \\(g(x)\\) is proportional to the normal distribution with mean 2 and standard deviation 1. We will call the PDF of the normal distribution \\(r(x)\\), keeping in mind that \\(g(x) = r(x)/Z_r\\). We can generate random values easily using rnorm() and then throwing away any value larger than 5 or less than 0. We can also easily find the value of the function \\(r(x)\\) by using dnorm().\nUnlike the previous problem, we will not calculate the normalizing constant \\(Z_r\\) for \\(g(x)\\). Instead, we will perform self-normalizing Importance Sampling.\nIn self-normalizing importance sampling, we do not bother with calculating these normalizing constants.\nWe can estimate\n\\[\\mathbb{E}_f[h(X)] \\approx \\frac{\\sum_{j = 1}^N h(x_j) w(x_j)}{\\sum_{j = 1}^N w(x_j)}\\]\nWhere\n\\[w(x_j) = \\frac{q(x_j)}{r(x_j)}\\]"
  },
  {
    "objectID": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-4",
    "href": "projects/102c_hw2_output_Amaeya_Deshpande.html#problem-4",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 4",
    "text": "Problem 4\nPerform Self-normalizing Importance sampling.\nGenerate a sample of 5000 values of x to estimate \\(\\hat{I}\\).\nLet \\(f(x)\\) be the uniform distribution from 0 to 5. You can let \\(q(x)\\) be 0.2, and \\(Z_q = 1\\).\nLet \\(g(x)\\) be proportional to the normal distribution with mean 2 and sd 1. We can let \\(r(x)\\) be the normal PDF, and leave \\(Z_r\\) unknown.\nAdjust how the estimate of the integral is calculated according to self-normalized importance sampling.\nCreate a plot using the cumulative mean (aka running mean) of the first 500 values in the sample to show how the estimate of the integral ‘settles’ over the course of the sampling.\nDo this two more times (each with a different starting seed), for a total of three samples of 5000 values each. Plot the cumulative mean of the first 500 values of each sample.\nIn your plot also include the ‘true’ value of the integral, 2.29583, as a horizontal line.\nWhen you create your plot, also adjust the axes to fit the samples better.\nFinally, print out your three estimates of \\(I\\), and also comment on how quickly the method using importance sampling converges to the expected value versus the uniform sampling method.\nKeep in mind that because we are estimating the normalizing constant \\(Z_r\\) using our samples, the performance of the self-normalizing method will not be as good as when we knew \\(Z_r\\) directly.\n\n## Monte Carlo Integration using self-normalizing importance sampling\nn = 5000\nk = 500\nq &lt;- 0.2\n\n# First series\nset.seed(1)\nx1 &lt;- rnorm(n, 2, 1)\nx1 &lt;- x1[x1 &gt; 0 & x1 &lt; 5]\nh_vals1 &lt;- h(x1)\nr1 &lt;- dnorm(x1, 2, 1)\nweights1 &lt;- q / r1\nI_est1 &lt;- 5 * sum(h_vals1 * weights1) / sum(weights1)\ncum_mean1 &lt;- 5 * cumsum(h_vals1[1:k] * weights1[1:k]) / cumsum(weights1[1:k])\n\n# Second series\nset.seed(2)\nx2 &lt;- rnorm(n, 2, 1)\nx2 &lt;- x2[x2 &gt; 0 & x2 &lt; 5]\nh_vals2 &lt;- h(x2)\nr2 &lt;- dnorm(x2, 2, 1)\nweights2 &lt;- q / r2\nI_est2 &lt;- 5 * sum(h_vals2 * weights2) / sum(weights2)\ncum_mean2 &lt;- 5 * cumsum(h_vals2[1:k] * weights2[1:k]) / cumsum(weights2[1:k])\n\n# Third series\nset.seed(3)\nx3 &lt;- rnorm(n, 2, 1)\nx3 &lt;- x3[x3 &gt; 0 & x3 &lt; 5]\nh_vals3 &lt;- h(x3)\nr3 &lt;- dnorm(x3, 2, 1)\nweights3 &lt;- q / r3\nI_est3 &lt;- 5 * sum(h_vals3 * weights3) / sum(weights3)\ncum_mean3 &lt;- 5 * cumsum(h_vals3[1:k] * weights3[1:k]) / cumsum(weights3[1:k])\n\nprint(paste(\"Estimate for I with seed 1:\", I_est1))\n\n[1] \"Estimate for I with seed 1: 2.2834276456526\"\n\nprint(paste(\"Estimate for I with seed 2:\", I_est2))\n\n[1] \"Estimate for I with seed 2: 2.24511503235841\"\n\nprint(paste(\"Estimate for I with seed 3:\", I_est3))\n\n[1] \"Estimate for I with seed 3: 2.33700377077445\"\n\n\n\nplot(x = c(1, 500), y = c(2.29583, 2.29583), type = \"l\", ylim = c(0, 5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:k, y = cum_mean1, col = \"blue\")\nlines(x = 1:k, y = cum_mean2, col = \"red\")\nlines(x = 1:k, y = cum_mean3, col = \"green\")\nlegend(\"topright\", legend = c(\"Estimate 1\", \"Estimate 2\", \"Estimate 3\", \n          \"True Value\"), col = c(\"blue\", \"red\", \"green\", \"black\"), lty = 1)"
  },
  {
    "objectID": "index.html#stats-102b-assignments",
    "href": "index.html#stats-102b-assignments",
    "title": "Projects",
    "section": "Stats 102B Assignments",
    "text": "Stats 102B Assignments"
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "",
    "text": "Homework Questions and text copyright Miles Chen. For personal use only. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported."
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#academic-integrity",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#academic-integrity",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nModifying the following statement with your name.\n“By including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students.”"
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#reading",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#reading",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Reading",
    "text": "Reading\nReading is important!\nDoing Bayesian Data Analysis Textbook is available at: https://www.sciencedirect.com/science/book/9780124058880\n\nChapter 2 of Doing Bayesian Data Analysis\nSkim Chapter 5 of Doing Bayesian Data Analysis\nChapter 6 of Doing Bayesian Data Analysis\nhttp://varianceexplained.org/statistics/beta_distribution_and_baseball/\nhttp://varianceexplained.org/r/credible_intervals_baseball/"
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-1-doing-bayesian-data-analysis-exercise-5.1-iterative-application-of-bayes-rule",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-1-doing-bayesian-data-analysis-exercise-5.1-iterative-application-of-bayes-rule",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 1: Doing Bayesian Data Analysis: Exercise 5.1 [Iterative application of Bayes’ Rule]",
    "text": "Problem 1: Doing Bayesian Data Analysis: Exercise 5.1 [Iterative application of Bayes’ Rule]\nThe textbook already gives us the probability of having the disease given a positive test. We can continue using Bayes’ Theorem with this probability as the prior probability, now considering a false negative test.\n\ndisease_given_pos &lt;- (0.99 * 0.001) / (0.99 * 0.001 + 0.05 * 0.999)\ndisease_given_pos_neg &lt;- (0.01 * disease_given_pos) / (0.95 * 0.999 + 0.01 * 0.001)\ndisease_given_pos_neg\n\n[1] 0.0002047777"
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-2-doing-bayesian-data-analysis-exercise-5.3-data-order-invariance",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-2-doing-bayesian-data-analysis-exercise-5.3-data-order-invariance",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 2: Doing Bayesian Data Analysis: Exercise 5.3 [data-order invariance]",
    "text": "Problem 2: Doing Bayesian Data Analysis: Exercise 5.3 [data-order invariance]"
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#a",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#a",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "(A)",
    "text": "(A)\n\ndisease_given_neg &lt;- (0.01 * 0.001) / (0.95 * 0.999 + 0.01 * 0.001)\ndisease_given_neg\n\n[1] 1.053674e-05"
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#b",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#b",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "(B)",
    "text": "(B)\n\ndisease_given_neg_pos &lt;- (0.99 * disease_given_neg) / (0.99 * 0.001 + 0.05 * 0.999)\ndisease_given_neg_pos\n\n[1] 0.0002047777\n\n\nThis is equivalent to the probability from Exercise 5.1, demonstrating that the probability of having the disease is the same given one positive and one negative test, regardless of the order."
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#summary-of-ch-6-of-doing-bayesian-data-analysis",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#summary-of-ch-6-of-doing-bayesian-data-analysis",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Summary of Ch 6 of Doing Bayesian Data Analysis",
    "text": "Summary of Ch 6 of Doing Bayesian Data Analysis\nIf the Beta distribution prior has distribution \\(\\text{Beta}(\\alpha, \\beta)\\)\nAnd our data has \\(z\\) successes, and \\(N - z\\) failures, the posterior distribution will have distribution:\n\\[\\text{Beta}(z + \\alpha, N - z + \\beta)\\]\nLet’s further explore the relationship between the prior, the likelihood, and the posterior distributions."
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#the-beta-prior-for-baseball-batting-average",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#the-beta-prior-for-baseball-batting-average",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "The beta prior for baseball batting average",
    "text": "The beta prior for baseball batting average\nRead: http://varianceexplained.org/statistics/beta_distribution_and_baseball/\nAs seen in the blog article, we will model the prior distribution of baseball batters’ batting average as \\(\\text{Beta}(81, 219)\\). These values have been arbitrarily selected, but were chosen because the author of the article ‘felt like’ batters generally have a batting average between 0.2 and 0.35.\n\ns &lt;- seq(0.0, 1, by = 0.005)\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density')\narrows(qbeta(0.025, 81, 219), 0.5, qbeta(0.975, 81, 219), 0.5, col = 'red', code = 3, angle = 90, length = 0.05) # adding an 'arrow' to display a credibility interval at the level y = 0.5\n\n\n\n\n\n\n\n\nCredibility interval:\n\nprint( c( qbeta(0.025, 81, 219), qbeta(0.975, 81, 219) ) )  # equal tailed Credibility interval\n\n[1] 0.2213490 0.3215554\n\n\nBefore seeing any data, my prior distribution tells me that there is a 95% probability that the batter’s batting average is between 0.2213 and 0.3216."
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-3",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-3",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 3",
    "text": "Problem 3\n\nOn gradescope begin tagging Problem 3 here\nLet’s say you observe a player who had 10 at bats and has 4 base hits (batting average = 0.400).\nPlot the likelihood of the data for values of p between 0.0 and 1. Use the same vector s for the locations.\n\nlikelihood &lt;- choose(10, 4) * s^4 * (1 - s)^(6)\nplot(s, likelihood, type = 'l', ylab = 'probability')\n\n\n\n\n\n\n\n\nUse the known results for the posterior distribution: \\(\\text{Beta}(z + \\alpha, N - z + \\beta)\\). Plot the posterior distribution of p after considering the data. Use red for the posterior. Also plot the prior distribution in black. You will see just a slight shift between the prior and the posterior.\n\ncred_int &lt;- c(qbeta(0.025, 85, 225), qbeta(0.975, 85, 225))\nconf_int &lt;- prop.test(4, 10, conf.level = 0.95)$conf.int[1:2]\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density')\nlines(s, dbeta(s, 85, 225), col = \"red\")\narrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)\narrows(conf_int[1], 0.6, conf_int[2], 0.6, col = 'blue', code = 3, angle = 90, length = 0.05)\n\n\n\n\n\n\n\n\nUse qbeta() to create a 95% credibility interval based on the posterior distribution.\nUse classical statistics to create a 95% confidence interval for p based on the fact that you had 4 successful hits out of 10 trials. (Even though the large sample condition is not met, assume you can use the central limit theorem for the creation of the confidence interval.)\nUsing the function arrow(), add both the credibility interval (in red at the level y = 0.5) and the confidence interval (in blue at the level y = 0.6) to the plot so you can make a visual comparison."
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-4a",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-4a",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 4a",
    "text": "Problem 4a\nLet’s say you observe a player who had 100 at bats and has 35 base hits (batting average = 0.350). Use a prior distribution of Beta(81, 219).\nPlot the posterior distribution of p after considering the data (in red). Also plot the prior (in black). Comment on the difference between the prior and the posterior.\nFind a 95% credibility interval based on the posterior. Create a classical 95% confidence interval. Compare the two intervals.\nAdd both the credibility interval (in red, at y = 0.5) and the confidence interval (in blue, at y = 0.6) to the plot so you can make a visual comparison.\n\ncred_int &lt;- c( qbeta(0.025, 116, 284), qbeta(0.975, 116, 284))\nconf_int &lt;- prop.test(35, 100, conf.level = 0.95)$conf.int[1:2]\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density', ylim = c(0, 20))\nlines(s, dbeta(s, 116, 284), col = \"red\")\narrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)\narrows(conf_int[1], 0.6, conf_int[2], 0.6, col = 'blue', code = 3, angle = 90, length = 0.05)\n\n\n\n\n\n\n\n\nThe prior and posterior distributions have a notable and visible difference unlike in question 3. The posterior distribution is shifted more to teh right towards a higher batting average. The posterior distribution is also narrower as it considers more data. Both distributions flatten out on the right tail around the same proportion. The credibility interval and confidence interval are also notably different. The confidence interval is much wider and spans to much higher batting averages."
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-4b",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-4b",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 4b",
    "text": "Problem 4b\nLet’s say you observe a player who had 500 at bats and has 175 base hits (batting average = 0.350).\nPlot the posterior distribution of p after considering the data. Also plot the prior (use a prior distribution of Beta(81, 219)). Comment on the difference between the prior and the posterior.\nFind a 95% credibility interval based on the posterior. Create a classical 95% confidence interval. Compare the two intervals.\nAdd both the credibility interval (in red, at y = 0.5) and the confidence interval (in blue, at y = 0.8) to the plot so you can make a visual comparison.\n\ncred_int &lt;- c( qbeta(0.025, 256, 544), qbeta(0.975, 256, 544))\nconf_int &lt;- prop.test(175, 500, conf.level = 0.95)$conf.int[1:2]\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density', ylim = c(0, 25))\nlines(s, dbeta(s, 256, 544), col = \"red\")\narrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)\narrows(conf_int[1], 0.8, conf_int[2], 0.8, col = 'blue', code = 3, angle = 90, length = 0.05)\n\n\n\n\n\n\n\n\nThe prior and posterior distributions are now very different from one another. Their modes are at least 0.05 away from one another. The posterior distribution is much narrower due to the added data. The credibility and confidence intervals are now very similar in range, but the confidence interval still spans to higher batting avergaes."
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-4c",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-4c",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 4c",
    "text": "Problem 4c\nFinally, let’s say you observe a player who had 5000 at bats and has 1750 base hits.\nPlot the posterior distribution of p after considering the data. Also plot the prior Beta(81, 219).\nAdd both the credibility interval (in red, at y = 0.5) and the confidence interval (in blue, at y = 1) to the plot so you can make a visual comparison.\n\ncred_int &lt;- c( qbeta(0.025, 1831, 3469), qbeta(0.975, 256, 544))\nconf_int &lt;- prop.test(1831, 5300, conf.level = 0.95)$conf.int[1:2]\nplot(s, dbeta(s, 81, 219), type = 'l', ylab = 'density', ylim = c(0, 60))\nlines(s, dbeta(s, 1831, 3469), col = \"red\")\narrows(cred_int[1], 0.5, cred_int[2], 0.5, col = 'red', code = 3, angle = 90, length = 0.05)\narrows(conf_int[1], 1, conf_int[2], 1, col = 'blue', code = 3, angle = 90, length = 0.05)\n\n\n\n\n\n\n\n\n\nAs the amount of data increases, how do the results of the Bayesian credibility interval compare to the results of the classical confidence interval?\nAs the amount of data increases, the results of the Bayesian credibility interval and the classical confidence interval become more and more similar. This can be seen in the red and blue intervals above. This happens because as the amount of data increases, the posterior data used for the credibility distribution becomes more and more similar to the sampled data used for the confidence interval."
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-5-application-of-monte-carlo-integration-bayesian-statistics",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-5-application-of-monte-carlo-integration-bayesian-statistics",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 5: Application of Monte Carlo Integration: Bayesian Statistics",
    "text": "Problem 5: Application of Monte Carlo Integration: Bayesian Statistics\nIn problem 4b, the player’s data that we observed was 175 hits out of 500 at-bats. In the problem, you updated the posterior distribution. The posterior distribution is: Beta(256, 544)\nSomeone asks, if this player has four at-bats in the next game, what is the probability that the player will get at least one hit?\nThe answer to this question is 1 - Probability(0 hits in 4 at bats). So we need to find the probability of 0 hits in 4 at bats.\nIf we knew the value of p, it’s easy:\n\\[P(x = 0) = {4 \\choose 0} p^0 (1-p)^4 = (1-p)^4\\]\nIn Bayesian statistics, however, we do not know the value of p. It can be one of the infinite values between 0 and 1. After seeing our data, p is described by the distribution Beta(256, 544).\nThus, the answer to our question will be found by integrating the probability of 0 hits multiplied by the probability of p, considered across every possible value of p.\n\\[\\int_0^1 (1 - p)^4 f(p) dp\\]\nwhere f(p) is the pdf of the distribution Beta(256, 544).\nUsing Monte Carlo Estimation (use n = 50,000), estimate the value of the above integral. (hint: it’s already in the form \\(E_f[h(X)] = \\int_\\mathcal{X} h(x)f(x) dx\\))\nYou are allowed to use rbeta().\n\nset.seed(1)\nsamp &lt;- rbeta(50000, 256, 544)\ng_samp &lt;- (1 - samp)^4\n1 - mean(g_samp)\n\n[1] 0.7854513"
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-6-bayesian-inference-for-the-rate-parameter-of-the-exponential-distribution",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-6-bayesian-inference-for-the-rate-parameter-of-the-exponential-distribution",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 6: Bayesian Inference for the rate parameter of the Exponential distribution",
    "text": "Problem 6: Bayesian Inference for the rate parameter of the Exponential distribution\nThe gamma distribution has a PDF of the form:\n\\[f(x; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\\]\nIt has two shape parameters \\(\\alpha\\) and \\(\\beta\\). Many distributions (including the exponential and chi-squared distributions) can be written in the form of a gamma distribution. We will take a look at the gamma distribution because it serves as a conjugate-prior for many distributions.\nWhen looking at the PDF of the gamma distribution, you can ignore the scary looking constant in the front \\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\), as its only purpose is to make sure the PDF integrates to 1.\nThe exponential distribution has the following PDF, defined by the rate parameter \\(\\lambda\\).\n\\[f(x;\\lambda) = \\lambda e^{-\\lambda x}\\]\nThe exponential distribution can be used to model the time between events, such as the time between customers entering a store. The \\(\\lambda\\) parameter is the rate (the number of arrivals per time block). Let’s say we are trying to model customers entering a store each hour. If \\(\\lambda = 2\\), that means the average rate is two arrivals per hour. The expected time between customers is \\(1/\\lambda = 0.5\\), meaning the mean time between customers is half an hour.\nIn this problem, we are trying to model customers entering a small business and will use Bayesian inference to create a distribution for the rate parameter. You talk to the business owner who tells you that sometimes the business gets busy and will see 20 customers in an hour. Other times, it’s slow, and maybe only 3 or 4 customers come. But overall, the owner estimates the average is something like 8 customers per hour, give or take a few.\nTaking this into account, you decide to use a Gamma distribution with shape parameters \\(\\alpha = 8\\) and \\(\\beta = 1\\) as the prior distribution for the rate \\(\\lambda\\).\n\ns &lt;- seq(0, 30, by = 0.01)\nplot(s, dgamma(s, 8, 1), type = 'l')\n\n\n\n\n\n\n\n\nYou decide to collect data by timing how long you wait between customer arrivals.\nYou gather the following values, measured in fractions of an hour:\n\ny &lt;- c(0.131, 0.078, 0.297, 0.024, 0.016, 0.057, 0.070, 0.148, 0.070, 0.109)\n# after you started the stop watch, the first customer arrived after 7 minutes and 52 seconds (0.131 of an hour)\n# the next customer came 4 minutes and 41 seconds after that (0.078 of an hour). etc. etc.\n# You gathered values for 10 customers total.\n# Conveniently, they add up to exactly one hour!\n\nI have written a simple function l() to calculate the likelihood of the data for a given lambda. It simply takes the pdf of each data point and returns the product.\n\ns &lt;- seq(0, 30, by = 0.01)\nl &lt;- function(lambda){\n  y &lt;- c(0.131, 0.078, 0.297, 0.024, 0.016, 0.057, 0.070, 0.148, 0.070, 0.109)\n  prod(lambda * exp(-lambda * y))\n}\n\nres &lt;- rep(NA, length(s))\nfor(i in 1:length(s)){\n  res[i] &lt;- l(s[i])\n}\n\nplot(s, res, type = 'l', main = 'likelihood of given data as a function of lambda')\n\n\n\n\n\n\n\n\nCalculate the likelihood function for lambda mathematically. The total likelihood of the data (which is assumed to be iid) is the product of each point’s probability. You can take advantage of the fact that the sum of the y’s is 1.\nWrite down your equation of the likelihood function.\nThe equation for the likelihood function of lambda is\n\\[L(\\lambda) = \\lambda^{10} e^{-\\lambda}\\]\nThis was found by using the PDF of the exponential distribution and the fact that n = 10 in our data and that the sum of all y’s is 1.\nCreate a plot of your mathematical likelihood function for values of lambda between 0 and 30. Is it identical to the plot I have provided above?\n\nL &lt;- function(lambda){\n  prod(lambda^10 * exp(-lambda))\n}\n\nres &lt;- rep(NA, length(s))\nfor(i in 1:length(s)){\n  res[i] &lt;- L(s[i])\n}\n\nplot(s, res, type = 'l', main = 'likelihood of given data as a function of lambda')\n\n\n\n\n\n\n\n\nThe plot is seemingly identical to the plot provided using the function l().\nMathematically, find the posterior distribution of lambda given the data.\nHints: We know that the posterior distribution is proportional to the likelihood times the prior. We also know that the gamma distribution is the conjugate prior for the exponential distribution. This means that the posterior distribution of lambda will be a gamma distribution.\n\\[p(\\lambda | y) \\propto p(y | \\lambda) p(\\lambda)\\]\nStart by multiplying the likelihood by the prior (a gamma distribution). Then, using algebra, rearrange terms so that the posterior is in the form of a gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\). If you temporarily ignore the normalizing constant in the gamma distribution, it is in the form \\(x^{\\text{constant1}}e^{\\text{-constant2}\\cdot x}\\)\nYour answer: The posterior distribution of lambda given the data is a gamma distribution with parameters \\(\\alpha = 18\\) and \\(\\beta = 2\\). This was determined using the information above and ultimately reaching the form \\(x^{\\text{17}}e^{\\text{-2}\\cdot x}\\)\nGraph the posterior distribution.\n\nplot(s, dgamma(s, 18, 2), type = 'l')"
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-7-bayesian-inference-for-the-mean-of-a-normal-distribution",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-7-bayesian-inference-for-the-mean-of-a-normal-distribution",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 7: Bayesian Inference for the mean of a normal distribution",
    "text": "Problem 7: Bayesian Inference for the mean of a normal distribution\nLet’s say X comes from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The variance \\(\\sigma^2\\) is known and is equal to 9. The mean, however, is unknown and has its own probability distribution.\nThe prior belief for the mean of the population is that \\(\\mu\\) comes from a normal distribution with mean \\(\\mu_0\\) and variance \\(\\sigma^2_0\\). Both \\(\\mu_0\\) and \\(\\sigma^2_0\\) are given.\n\\[\\mu \\sim N(\\mu_0, \\sigma^2_0)\\]\nDerive the posterior distribution of \\(\\mu\\) given that we have observed a single observation x.\nThat is:\n\\[X \\sim N(\\mu, \\sigma^2)\\]\n\\[f(x | \\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\frac{-(x - \\mu)^2}{2 \\sigma^2}}\\]\nWhere \\(\\mu\\) itself has the pdf:\n\\[f(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma_0^2}} \\exp{\\frac{-(\\mu - \\mu_0)^2}{2 \\sigma_0^2}}\\]\nTo get you started, keep in mind Bayes’ Rule. We also remember that \\(f(x)\\) is a constant that exists only to ensure that \\(f(\\mu|x)\\) integrates to 1.\n\\[f(\\mu | x) = \\frac{\\text{likelihood}\\times\\text{prior}}{\\text{marginal}} = \\frac{f(x|\\mu)f(\\mu)}{f(x)} \\propto f(x|\\mu)f(\\mu)\\]\nThus:\n\\[f(\\mu | x) \\propto f(x|\\mu)f(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\frac{-(x - \\mu)^2}{2 \\sigma^2}} \\cdot \\frac{1}{\\sqrt{2 \\pi \\sigma_0^2}} \\exp{\\frac{-(\\mu - \\mu_0)^2}{2 \\sigma_0^2}}\\]\nYour job: combine and rearrange terms as necessary to get the result to be in the form of the normal PDF.\nThat is find \\(\\mu_1\\) and \\(\\sigma_1\\) so that the above product can be expressed as:\n\\[f(\\mu | x) = \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp{\\frac{-(\\mu - \\mu_1)^2}{2 \\sigma_1^2}}\\]\nYour answer: The posterior distribution of \\(\\mu\\) given the data is a normal distribution with parameters \\[\\mu_1 = \\frac{\\sigma_0^2 x + \\sigma^2 \\mu_0} {\\sigma_0^2 + \\sigma^2}\\] and\n\\[\\sigma_1^2 = \\frac{\\sigma_0^2 \\sigma^2} {\\sigma_0^2 + \\sigma^2}\\]\nThis was found by expanding and combining the terms in the equation provided above and then rearranging terms to return to the form of the normal distribution. From there, I was able to identify the parameters of the distribution."
  },
  {
    "objectID": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-7b",
    "href": "102c/102c_hw1_output_Amaeya_Deshpande.html#problem-7b",
    "title": "Stats 102C, Homework 1 - Intro to Bayesian Statistics",
    "section": "Problem 7b:",
    "text": "Problem 7b:\nUsing your result from above, give the posterior distribution for the mean height of adult males in the US.\nPrior to seeing any data, it is believed that the mean height of adult males in the US is about 69 inches. We express our prior beliefs by saying \\(\\mu\\) random variable that follows a Normal distribution with mean 69 and sd = 0.5.\nWe randomly select one adult male in the US, and find his height to be 71 inches.\nWith this observation, what is the posterior distribution of the mean \\(\\mu\\)?\nThe posterior distribution of the mean \\(\\mu\\) is \\[\\mu \\sim N(69.054, 0.243)\\]\nThis was found by inputting the known values into the answers we found in question 7a. By solving for the parameters, I was able to find the posterior distribution given the single observation."
  },
  {
    "objectID": "102c/102c_hw4_output_Amaeya_Deshpande.html",
    "href": "102c/102c_hw4_output_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "",
    "text": "Homework Questions, copyright Miles Chen. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported to the Dean of Students.\nModify this file with your answers and responses."
  },
  {
    "objectID": "102c/102c_hw4_output_Amaeya_Deshpande.html#academic-integrity-statement",
    "href": "102c/102c_hw4_output_Amaeya_Deshpande.html#academic-integrity-statement",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nBy including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students."
  },
  {
    "objectID": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-1---transition-matrix-and-stationary-distribution-two-state-case",
    "href": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-1---transition-matrix-and-stationary-distribution-two-state-case",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 1 - Transition Matrix and Stationary Distribution (Two state case)",
    "text": "Problem 1 - Transition Matrix and Stationary Distribution (Two state case)\nImagine a two-state Markov chain. With state 1 representing CA and state 2 representing TX.\nLet’s pretend that each year, 9% of Californians move to TX and that 12% of Texans move to CA.\nCreate and display a 2x2 transition matrix \\(\\mathbb{P}\\) in R to represent the transition probabilities.\nUsing algebra, find the stationary distribution \\(\\boldsymbol{\\pi}\\), so that \\(\\boldsymbol{\\pi}\\mathbb{P} = \\boldsymbol{\\pi}\\).\n\nP &lt;- rbind(c(0.91, 0.09), c(0.12, 0.88))\nw &lt;- c(4/7, 3/7)\n\nWe can use algebra to find the stationary distribution by creating a system of equations:\n0.91w1 + 0.12w2 = w1 0.09w1 + 0.88w2 = w2 -&gt; 0.09w1 = 0.12w2 -&gt; w2 = 0.75w1\nWe also know that w1 + w2 = 1 so…\n0.91w1 + 0.12(0.75w1) + 0.09w1 + 0.88(0.75w1) = 1 -&gt; 1.75w1 = 1\nWhen we divide, we get w1 = 4/7 so knowing that w1 + w2 = 1, we get that w2 = 3/7\nFind the left eigenvector of \\(\\mathbb{P}\\) and normalize it (so it sums to 1). Does it match the stationary distribution you found?\n\neig_vectors &lt;- eigen(t(P))$vectors\nstationary &lt;- t(eig_vectors[,1])\nstationary / (sum(stationary))\n\n          [,1]      [,2]\n[1,] 0.5714286 0.4285714\n\n\nYes, this matches the stationary distribution found using algebra."
  },
  {
    "objectID": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-2---transition-matrix-and-stationary-distribution-island-example",
    "href": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-2---transition-matrix-and-stationary-distribution-island-example",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 2 - Transition Matrix and Stationary Distribution (island example)",
    "text": "Problem 2 - Transition Matrix and Stationary Distribution (island example)\nLook at the example with the politician visiting the island chain in chapter 7 of the textbook, Doing Bayesian Data Analysis. Also see Lecture 5-3.\nImagine another chain of 7 islands where the target distribution is equal to the probabilities of a binomial distribution with n = 6 and p = 0.6.\nThis ‘nation’ has 7 islands in a chain numbered from 0 to 6. Island 0 has prob = \\(\\binom{6}{0}(.6)^0(.4)^6\\) = dbinom(0, 6, 0.6), Island 1 has prob = \\(\\binom{6}{1}(.6)^1(.4)^5\\) = dbinom(1, 6, 0.6), Island 6 has prob = dbinom(6, 6, 0.6), etc.\nUse the same algorithm as the politician to figure out the transition probabilities. Create and print out the full 7 x 7 transition matrix \\(\\mathbb{P}\\). Populate the matrix with actual decimal values, and not symbols (round to 4 decimal places for display purposes).\nStart with the initial distribution: \\(\\boldsymbol{\\pi}^{(1)}\\) = c(0, 0, 0, 1, 0, 0, 0)\nMultiply \\(\\boldsymbol{\\pi}^{(n)}\\) by \\(\\mathbb{P}\\) 6 times and print the results after each iteration. (Print the distribution of \\(\\boldsymbol{\\pi}^{(2)}\\), \\(\\boldsymbol{\\pi}^{(3)}\\), … \\(\\boldsymbol{\\pi}^{(7)}\\))\nFind the stationary distribution of the chain by finding the left eigenvector of the transition matrix and normalizing it. Check (using all.equal()) to see if it is equal to the target distribution (a binomial distribution with \\(n = 6\\) and \\(p = 0.6\\))\n\ntarget &lt;- dbinom(0:6, 6, 0.6)\nP &lt;- matrix(rep(0, 49), nrow = 7)\nfor (i in 1:7) {\n  if (i &lt; 7) {\n    if (target[i] &lt; target[i + 1]) {\n      P[i, i + 1] &lt;- 0.5\n    }\n    else {\n      P[i, i + 1] &lt;- 0.5 * (target[i + 1] / target[i])\n    }\n  }\n  if (i &gt; 1) {\n    if (target[i] &lt; target[i - 1]) {\n      P[i, i - 1] &lt;- 0.5\n    }\n    else {\n      P[i, i - 1] &lt;- 0.5 * (target[i - 1] / target[i])\n    }\n  }\n  P[i, i] &lt;- 1 - sum(P[i,])\n}\nround(P, 4)\n\n       [,1]   [,2]   [,3]   [,4]   [,5]  [,6]  [,7]\n[1,] 0.5000 0.5000 0.0000 0.0000 0.0000 0.000 0.000\n[2,] 0.0556 0.4444 0.5000 0.0000 0.0000 0.000 0.000\n[3,] 0.0000 0.1333 0.3667 0.5000 0.0000 0.000 0.000\n[4,] 0.0000 0.0000 0.2500 0.2500 0.5000 0.000 0.000\n[5,] 0.0000 0.0000 0.0000 0.4444 0.2556 0.300 0.000\n[6,] 0.0000 0.0000 0.0000 0.0000 0.5000 0.375 0.125\n[7,] 0.0000 0.0000 0.0000 0.0000 0.0000 0.500 0.500\n\npi_1 = c(0, 0, 0, 1, 0, 0, 0)\npi_curr &lt;- pi_1\nfor (i in 2:7) {\n  pi_next &lt;- pi_curr %*% P\n  print(round(pi_next, 4))\n  pi_curr &lt;- pi_next\n}\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]    0    0 0.25 0.25  0.5    0    0\n     [,1]   [,2]   [,3]   [,4]   [,5] [,6] [,7]\n[1,]    0 0.0333 0.1542 0.4097 0.2528 0.15    0\n       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]\n[1,] 0.0019 0.0354 0.1756 0.2919 0.3445 0.1321 0.0187\n       [,1]   [,2]  [,3]   [,4] [,5]   [,6]   [,7]\n[1,] 0.0029 0.0401 0.155 0.3139  0.3 0.1622 0.0259\n       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]\n[1,] 0.0037 0.0399 0.1553 0.2893 0.3147 0.1638 0.0332\n       [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]\n[1,] 0.0041 0.0403 0.1493 0.2899 0.307 0.1724 0.0371\n\neig_vectors &lt;- eigen(t(P))$vectors\nstationary &lt;- t(eig_vectors[,1])\nstationary &lt;- as.vector(stationary / (sum(stationary)))\nall.equal(target, stationary)\n\n[1] TRUE\n\n\nMultiply \\(\\boldsymbol{\\pi}^{(1)}\\) by \\(\\mathbb{P}\\) 500 times to get \\(\\boldsymbol{\\pi}^{(501)}\\). Show the results after the final iteration. Do NOT show the steps in between. Did the distribution converge to the stationary distribution?\n\npi_curr &lt;- pi_1\npi_next &lt;- pi_1\nfor (i in 2:500) {\n  pi_next &lt;- pi_curr %*% P\n  pi_curr &lt;- pi_next\n}\nprint(round(pi_next, 4))\n\n       [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]\n[1,] 0.0041 0.0369 0.1382 0.2765 0.311 0.1866 0.0467\n\n\nThe distribution does converge towards the stationary distribution. The values are not identical, but they are very similar."
  },
  {
    "objectID": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-3---mcmc-metropolis-algorithm-for-the-island-hopping",
    "href": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-3---mcmc-metropolis-algorithm-for-the-island-hopping",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 3 - MCMC (Metropolis Algorithm) for the island hopping",
    "text": "Problem 3 - MCMC (Metropolis Algorithm) for the island hopping\nWrite code to create a Markov chain using the Metropolis Algorithm for the same island nation in problem 2.\n\ntarget &lt;- function(x){ \n  ifelse(x %in% 0:6, dbinom(x, 6, 0.6), 0)\n}\npropose &lt;- function(x) { \n  current + sample(c(1, -1), size = 1)\n}\nn &lt;- 10^5\nresults1 &lt;- c(0, rep(NA, n - 1))\nset.seed(1)\nfor (t in 1:(n-1)) {\n  current &lt;- results1[t]\n  proposed &lt;- propose(current)\n  pmove &lt;- target(proposed) / target(current)\n  u &lt;- runif(1)\n  if(u &lt; pmove) {\n    results1[t + 1] &lt;- proposed\n  } else {\n    results1[t + 1] &lt;- current\n  }\n}\ncounts &lt;- table(results1)\nres &lt;- rbind(counts/n , dbinom(0:6, 6, 0.6)) \nrownames(res) &lt;- c(\"empirical\", \"target\")\nbarplot(res, beside = TRUE, legend.text = row.names(res), \n        args.legend = list(x = 7))\n\n\n\n\n\n\n\nchisq.test(counts, p = dbinom(0:6, 6, 0.6))\n\n\n    Chi-squared test for given probabilities\n\ndata:  counts\nX-squared = 17.037, df = 6, p-value = 0.009149\n\n\nWhile it is not strongly reflected in the barplot, the chi-squared goodness-of-fit test with p-value less than 0.05 provides evidence to say that the values produced by the Markov chain do not come from the target distribution.\n\nresults2 &lt;- c(6, rep(NA, n - 1))\nset.seed(2)\nfor (t in 1:(n-1)) {\n  current &lt;- results2[t]\n  proposed &lt;- propose(current)\n  pmove &lt;- target(proposed) / target(current)\n  u &lt;- runif(1)\n  if(u &lt; pmove) {\n    results2[t + 1] &lt;- proposed\n  } else {\n    results2[t + 1] &lt;- current\n  }\n}\ncounts &lt;- table(results2)\nres &lt;- rbind(counts/n , dbinom(0:6, 6, 0.6)) \nrownames(res) &lt;- c(\"empirical\", \"target\")\nbarplot(res, beside = TRUE, legend.text = row.names(res), \n        args.legend = list(x = 7))\n\n\n\n\n\n\n\nchisq.test(counts, p = dbinom(0:6, 6, 0.6))\n\n\n    Chi-squared test for given probabilities\n\ndata:  counts\nX-squared = 6.9061, df = 6, p-value = 0.3296\n\n\nBased on the barplot and the chi-squared goodness-of-fit test with p-value greater than 0.05, we do not have enough evidence to say that the values produced by the Markov chain do not come from the target distribution.\nRun the Metropolis Algorithm to create two Markov chains, each of length 10^5. For the first chain, start at x = 0 and use set.seed(1). For the second, start at x = 6 and use set.seed(2).\nFor each completed chain, print out a table of the resulting relative frequencies. Make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution. Use a chi-squared goodness-of-fit test to see if the generated values fit the expected probabilities. Be sure to comment on the graph and results of the test."
  },
  {
    "objectID": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-4---mcmc-metropolis-algorithm-for-a-single-continuous-random-variable",
    "href": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-4---mcmc-metropolis-algorithm-for-a-single-continuous-random-variable",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 4 - MCMC (Metropolis Algorithm) for a single continuous random variable",
    "text": "Problem 4 - MCMC (Metropolis Algorithm) for a single continuous random variable\nThe logisitic distribution is a unimodal and symmetric distribution, where the CDF is a logistic curve. The shape is similar to a normal distribution, but has heavier tails (though not as heavy as a Cauchy distribution).\nWe will compare Rejection Sampling to the Metropolis Algorithm for producing a sample from a distribution.\nThe PDF is:\n\\[f(x; \\mu, s) = \\frac{1}{s} \\frac{e^{-(\\frac{x-\\mu}{s})} }{\\left( 1 + e^{-(\\frac{x-\\mu}{s})} \\right)^2}\\]\nLuckily, this is implemented for us in R with dlogis(), which you are allowed to use to calculate the probability density of a (proposed) value.\nWe will generate two samples drawn from a logistic distribution with mean = 0 and scale = 1.\n\\[f(x; \\mu = 0, s=1) = \\frac{e^{-x} }{\\left( 1 + e^{-x} \\right)^2} = \\texttt{dlogis(x)}\\]\n\nTask 4A:\nFirst generate a sample from the logistic distribution using rejection sampling. Propose 10^5 values from a random uniform distribution from -20 to 20. Calculate the necessary constant M, and implement rejection sampling. If you propose 10^5 values, how many values do you end up accepting?\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your accepted values to the same plot (in a different color). Use the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions.\n\nset.seed(4)\nn &lt;- 10^5\nproposed &lt;- runif(n, -20, 20)\nf &lt;- function(x) { dlogis(x) }\ng &lt;- function(x) { 1/40 }\nM &lt;- f(0) / g(0)\nr_x &lt;- f(proposed) / (M * g(proposed))\nu &lt;- runif(n)\naccept &lt;- u &lt; r_x\naccepted_vals &lt;- proposed[accept]\nprint(paste(\"Number of values accepted:\", length(accepted_vals)))\n\n[1] \"Number of values accepted: 9999\"\n\nt_cdf &lt;- function(x) { plogis(x) }\ne_cdf &lt;- ecdf(accepted_vals)\ncurve(t_cdf(x), from = -10, to = 10, col = \"black\", lwd = 2, \n      ylab = \"CDF\")\nlines(e_cdf, col = \"blue\")\nlegend(\"topleft\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"), \n       col = c(\"black\", \"blue\"), lwd = 1)\n\n\n\n\n\n\n\nks.test(accepted_vals, plogis)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  accepted_vals\nD = 0.0069769, p-value = 0.7151\nalternative hypothesis: two-sided\n\n\n\n\nTask 4B:\nUse the Metropolis algorithm to generate values from the logisitic distribution.\nFor your proposal distribution, use a random uniform distribution that draws a random value between \\(X_{current} - 1\\) and \\(X_{current} + 1\\).\nAs a reminder, the steps of the algorithm are as follows:\n\nPropose a single value from the proposal distribution.\nCalculate the probability of moving = min(1, P(proposed)/P(current))\nDraw a random value to decide if you will move or not. If you move, update the current position. If you do not move, keep the current position for another iteration.\nRepeat.\n\nStart at the terrible location \\(x^{(1)} = -19\\).\nRun the Markov Chain for 10,000 iterations. Plot the first 1000 values of the chain and eyeball where you think the chain starts has finished ‘burning-in’ and is now drawing values from the target distribution. Throw away those initial values.\nPlot a density histogram of the remaining values and add the density of the logistic distribution to the histogram.\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your values (after removing burn-in) to the same plot. Use the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions.\n\nset.seed(10)\ntarget &lt;- function(x) {\n  dlogis(x)\n}\npropose &lt;- function(x) {\n  runif(1, x - 1, x + 1)\n}\nresults &lt;- rep(NA, 10^4)\nresults[1] &lt;- -19 \nfor(i in 1:(10^4 - 1)){\n  current &lt;- results[i]\n  proposed &lt;- propose(current)\n  p_move &lt;- min(1, target(proposed) / target(current))\n  results[i + 1] &lt;- ifelse(runif(1) &lt; p_move, proposed, current)\n}\nplot(results[1:1000], type = \"l\")\n\n\n\n\n\n\n\nkept_vals &lt;- results[-(1:150)]\nhist(kept_vals, probability = TRUE, breaks = 30)\ncurve(dlogis(x), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nt_cdf &lt;- function(x) { plogis(x) }\ne_cdf &lt;- ecdf(kept_vals)\ncurve(t_cdf(x), from = -10, to = 10, col = \"black\", lwd = 2, \n      ylab = \"CDF\")\nlines(e_cdf, col = \"blue\")\nlegend(\"topleft\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"), \n       col = c(\"black\", \"blue\"), lwd = 1)\n\n\n\n\n\n\n\nks.test(kept_vals, plogis)\n\nWarning in ks.test.default(kept_vals, plogis): ties should not be present for\nthe Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  kept_vals\nD = 0.008648, p-value = 0.4528\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-5---mcmc---the-effect-of-sigma-in-the-proposal-distribution",
    "href": "102c/102c_hw4_output_Amaeya_Deshpande.html#problem-5---mcmc---the-effect-of-sigma-in-the-proposal-distribution",
    "title": "Stats 102C, Homework 4 - Intro to MCMC",
    "section": "Problem 5 - MCMC - the effect of sigma in the proposal distribution",
    "text": "Problem 5 - MCMC - the effect of sigma in the proposal distribution\nWrite code to perform 50,000 iterations of the Metropolis Algorithm for a single continuous random variable.\nLet the PDF of the target distribution be:\n\\[f(x) = c \\cdot ( sin(x) + 2 )\\]\nfor \\(0 \\le x \\le 3 * \\pi\\), where c is some constant so that \\(\\int_0^{3\\pi} f(x) dx = 1\\).\nFor your proposal distribution, use a normal distribution, centered at the current value, with a standard deviation of \\(\\sigma\\), which we will adjust in this problem.\nBegin your Markov Chain at the location x = 2.\nKeep in mind that the probability of a value greater than \\(3 \\pi\\) or less than 0 is 0.\nGather 50,000 samples using MCMC three different times.\nThe first time, use a sigma of 0.1 for the proposal distribution.\nThe second time, use a sigma of 2.5 for the proposal distribution.\nThe third time, use a sigma = 20.\nKeep track of whether your proposed values are accepted or rejected, and print out the acceptance ratio.\nFor each MCMC run, print out the acceptance ratio, create a histogram of the sampled values, and plot the first 500 values of the chain plot(x[1:500], type = \"l\").\n\nset.seed(11)\nc &lt;- 1 / integrate(function(x) {sin(x) + 2}, 0, 3 * pi)$value\nf &lt;- function(x) {  \n  ifelse(x &lt; 0 || x &gt; 3 * pi, 0, c * (sin(x) + 2))\n}\nn &lt;- 50000\nresults &lt;- rep(NA, n)\nresults[1] &lt;- 2\naccepted &lt;- 0\nfor(i in 1:(10^4 - 1)){\n  current &lt;- results[i]\n  proposed &lt;- rnorm(1, current, 0.1)\n  p_move &lt;- min(1, f(proposed) / f(current))\n  if (runif(1) &lt; p_move) {\n    results[i + 1] &lt;- proposed\n    accepted &lt;- accepted + 1\n  }\n  else {\n    results[i + 1] &lt;- current\n  }\n}\nprint(paste(\"Acceptance ratio is:\", accepted / n))\n\n[1] \"Acceptance ratio is: 0.1965\"\n\nhist(results, probability = TRUE, breaks = 30)\ncurve(c * (sin(x) + 2), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nplot(results[1:500], type = \"l\")\n\n\n\n\n\n\n\nresults &lt;- rep(NA, n)\nresults[1] &lt;- 2\naccepted &lt;- 0\nfor(i in 1:(10^4 - 1)){\n  current &lt;- results[i]\n  proposed &lt;- rnorm(1, current, 2.5)\n  p_move &lt;- min(1, f(proposed) / f(current))\n  if (runif(1) &lt; p_move) {\n    results[i + 1] &lt;- proposed\n    accepted &lt;- accepted + 1\n  }\n  else {\n    results[i + 1] &lt;- current\n  }\n}\nprint(paste(\"Acceptance ratio is:\", accepted / n))\n\n[1] \"Acceptance ratio is: 0.12832\"\n\nhist(results, probability = TRUE, breaks = 30)\ncurve(c * (sin(x) + 2), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nplot(results[1:500], type = \"l\")\n\n\n\n\n\n\n\nresults &lt;- rep(NA, n)\nresults[1] &lt;- 2\naccepted &lt;- 0\nfor(i in 1:(10^4 - 1)){\n  current &lt;- results[i]\n  proposed &lt;- rnorm(1, current, 20)\n  p_move &lt;- min(1, f(proposed) / f(current))\n  if (runif(1) &lt; p_move) {\n    results[i + 1] &lt;- proposed\n    accepted &lt;- accepted + 1\n  }\n  else {\n    results[i + 1] &lt;- current\n  }\n}\nprint(paste(\"Acceptance ratio is:\", accepted / n))\n\n[1] \"Acceptance ratio is: 0.03018\"\n\nhist(results, probability = TRUE, breaks = 30)\ncurve(c * (sin(x) + 2), add = TRUE, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nplot(results[1:500], type = \"l\")"
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "",
    "text": "Homework Questions and text copyright Miles Chen. For personal use only. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported."
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html#academic-integrity",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html#academic-integrity",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nModifying the following statement with your name.\n“By including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students.”"
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html#reading",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html#reading",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Reading",
    "text": "Reading\nReading is important!\n\nChapter 2 section 2 in Introducing Monte Carlo Methods with R\nChapter 3 sections 1-3 in Introducing Monte Carlo Methods with R"
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-1---estimate-pi-poorly",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-1---estimate-pi-poorly",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 1 - Estimate pi (poorly)",
    "text": "Problem 1 - Estimate pi (poorly)\nA simple Monte Carlo Exercise … Get an estimate of pi by using random uniform numbers. (It won’t be a very good estimate.)\nIn this first exercise, we can see how a simple source of randomness (in our case, R’s runif() function) can be used to estimate tough quantities.\nWe will find an estimate of pi by estimating the ratio between the area of a circle and its encompassing square.\n\ns &lt;- seq(-1, 1, by = 0.001)\nposf &lt;- sqrt(1 - s ^ 2)\nplot(s, posf, type = \"l\", asp = 1, ylim = c(-1, 1))\nlines(s, -1 * posf)\nsegments(-1, -1, -1, 1)\nsegments(-1, -1, 1, -1)\nsegments(1, 1, -1, 1)\nsegments(1, 1, 1, -1)\n\n\n\n\n\n\n\n\nTo calculate the area of the circle analytically, we would need to integrate the function drawing the upper semi-circle and then multiply that by 2. This process requires the use of trig substitutions, and while doable, can illustrate a time where the analytic solution is not easy.\n\\[Area = 2 \\times \\int_{-1}^1 \\sqrt{1 - x^2} dx\\]\nFor the Monte-Carlo approach, we will use runif(n, min = -1, max=1) to generate a bunch of random pairs of x and y coordinates. We will see how many of those random uniform points fall within the circle. This is easy - just see if \\(x^2 + y^2 \\le 1\\). The total area of the square is 4. The total area of the circle is pi. Thus, the proportion of coordinates that satisfy the inequality \\(x^2 + y^2 \\le 1 \\approx \\pi/4\\).\nInstructions:\n\ncreate a vector x of n random values between -1 and 1. I suggest starting with n = 500\ncreate a vector y of n random values between -1 and 1. Use the two vectors to make coordinate pairs.\ncalculate which of points satisfy the inequality for falling inside the circle.\nPrint out your estimate of pi by multiplying the proportion by 4.\nplot each of those (x,y) coordinate pairs. Use pch = 20. Color the points based on whether they fall in the circle or not.\n\n\nset.seed(123)\nx &lt;- runif(500, min = -1, max = 1)\ny &lt;- runif(500, min = -1, max = 1)\ninside &lt;- x^2 + y^2 &lt;= 1\nprop_inside &lt;- sum(inside) / 500\npi_est &lt;- prop_inside * 4\nprint(paste(\"Estimate for pi: \", pi_est))\n\n[1] \"Estimate for pi:  3.2\"\n\ns &lt;- seq(-1, 1, by = 0.001)\nposf &lt;- sqrt(1 - s ^ 2)\nplot(s, posf, type = \"l\", asp = 1, ylim = c(-1, 1))\nlines(s, -1 * posf)\nsegments(-1, -1, -1, 1)\nsegments(-1, -1, 1, -1)\nsegments(1, 1, -1, 1)\nsegments(1, 1, 1, -1)\npoints(x, y, pch = 20, col = ifelse(inside, \"green\", \"red\"))"
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-2",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-2",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 2",
    "text": "Problem 2\nEstimate the integral of the following function by using Monte Carlo integration.\n\\[I = \\int_0^5 h(x) dx = \\int_0^5 \\exp(-0.5 (x-2)^2 - 0.1 |\\sin(2x)|) dx\\]\nWe want to estimate the value of I, the area under the curve from 0 to 5. We can say that area is equal to the average value of \\(h(x)\\) times the width:\n\\[I = 5 \\cdot \\mathbb{E}_f[h(X)]\\]\n\n# what the function looks like\nh &lt;- function(x) {exp(-0.5 * (x - 2) ^ 2 - 0.1 * abs( sin(2 * x) ) )}\nv &lt;- seq(0, 5, by = 0.01)\nplot(v, h(v), type = \"l\", xlim = c(-0.5, 5.5))\npolygon(c(0, v, 5), c(0, h(v), 0), col = 'khaki')\nrect_ht = 0.458394 # height of rectangle is the Expected value of h(x)\npolygon(c(0, 0, 5, 5), c(0, rect_ht, rect_ht, 0), col = rgb(0, 0, 1, .2))\n\n\n\n\n\n\n\n\nFor MC integration, we estimate\n\\[\\mathbb{E}_f[h(X)] = \\int_\\mathcal{X} h(x)f(x) dx\\]\nFor this first problem, we will use the uniform distribution on the interval (0,5) to draw samples of x. Thus, the PDF, \\(f(x)\\) is\n\\[f(x) = 1/5\\]\nThus,\n\\[ I  = 5 \\cdot \\mathbb{E}_f[h(X)] = 5 \\cdot \\int_0^5 h(x) f(x) dx \\approx \\frac{5}{N}\\sum_{j = 1}^N h(x_j) = \\hat I\\]\nWhere \\(x_j \\sim \\text{Unif}(0,5)\\)\nGenerate a sample of 5000 values of x to estimate \\(\\hat{I}\\). Use runif() to generate the random uniform values.\nCreate a plot using the cumulative mean (aka running mean) of the first 500 values in the sample to show how the estimate of the integral ‘settles’ over the course of the sampling.\nDo this two more times (each with a different starting seed), for a total of three samples of 5000 values each. Plot the cumulative mean of the first 500 values of each sample. There will be three different lines on this plot.\nIn your plot also include the ‘true’ value of the integral, 2.29583, as a horizontal line.\nPrint out your three estimates of \\(I\\).\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nh &lt;- function(x) {exp(-0.5 * (x - 2) ^ 2 - 0.1 * abs( sin(2 * x) ) )}\n\n## Monte Carlo Integration using uniform random sampling\nn = 5000 # total number of values to generate\nk = 500  # number of values to plot\n\n# First series\nset.seed(1)\nx1 &lt;- runif(n, min = 0, max = 5)\nh_vals1 &lt;- h(x1)\nI_est1 &lt;- (5 / n) * sum(h_vals1)\ncum_mean1 &lt;- cummean(h_vals1[1:k]) * 5\n\n# second series\nset.seed(2)\nx2 &lt;- runif(n, min = 0, max = 5)\nh_vals2 &lt;- h(x2)\nI_est2 &lt;- (5 / n) * sum(h_vals2)\ncum_mean2 &lt;- cummean(h_vals2[1:k]) * 5\n\n# third series\nset.seed(3)\nx3 &lt;- runif(n, min = 0, max = 5)\nh_vals3 &lt;- h(x3)\nI_est3 &lt;- (5 / n) * sum(h_vals3)\ncum_mean3 &lt;- cummean(h_vals3[1:k]) * 5\n\nprint(paste(\"Estimate for I with seed 1:\", I_est1))\n\n[1] \"Estimate for I with seed 1: 2.27626784048365\"\n\nprint(paste(\"Estimate for I with seed 2:\", I_est2))\n\n[1] \"Estimate for I with seed 2: 2.26402222241221\"\n\nprint(paste(\"Estimate for I with seed 3:\", I_est3))\n\n[1] \"Estimate for I with seed 3: 2.29369518890943\"\n\n\n\nplot(x = c(1, 500), y = c(2.29583, 2.29583), type = \"l\", ylim = c(0, 5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:500, y = cum_mean1, col = \"blue\")\nlines(x = 1:500, y = cum_mean2, col = \"red\")\nlines(x = 1:500, y = cum_mean3, col = \"green\")\nlegend(\"topright\", legend = c(\"Estimate 1\", \"Estimate 2\", \"Estimate 3\", \n                  \"True Value\"), col = c(\"blue\", \"red\", \"green\", \"black\"), lty = 1)"
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-2b",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-2b",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 2b",
    "text": "Problem 2b\nWe can estimate the variance of \\(\\bar h_n\\) with \\(v_n = \\frac{1}{n^2}\\sum_{j=1}^N [ h(x_j) - \\bar h_n]^2\\).\nWhat is variance of the estimate \\(\\hat I\\)?\nWrite the formula for estimated variance of I-hat with latex:\n\\[Var(\\hat I) = \\frac{25}{n^2}\\sum_{j=1}^N [ h(x_j) - \\bar h_n]^2\\]\nLook at your first Monte Carlo series and estimate the running variance of \\(\\hat I\\) as n goes from 1 to 500.\nCreate a plot of the \\(\\hat I\\) with 95% confidence bounds above and below.\n\ncum_est &lt;- cum_mean1[1:500]\ncum_var &lt;- numeric(500)\nfor (i in 1:500) {\n  cum_var[i] &lt;- var(h_vals1[1:i]) * 25\n}\nupper_bound &lt;- cum_est + qnorm(0.975) * sqrt(cum_var / 1:500)\nlower_bound &lt;- cum_est + qnorm(0.025) * sqrt(cum_var / 1:500)\nplot(x = 1:500, y = cum_est, type = \"l\", ylim = c(0, 5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:500, y = upper_bound, col = \"blue\")\nlines(x = 1:500, y = lower_bound, col = \"blue\")"
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-3a",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-3a",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 3a",
    "text": "Problem 3a\nFind \\(Z_r\\) so that \\(\\int_{0}^5 \\frac{1}{Z_r} \\text{Normal PDF (2,1)} = 1\\). Hint: figure out how much of the distribution is ‘cut off’ at 0 and 5, and find \\(Z_r\\) accordingly.\n\nz_upper &lt;- (5 - 2) / 1\nz_lower &lt;- (0 - 2) / 1\nz_r &lt;- pnorm(z_upper) - pnorm(z_lower)\nz_r\n\n[1] 0.9759"
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-3b",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-3b",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 3b",
    "text": "Problem 3b\nPerform Importance sampling.\nGenerate a sample of 5000 values of x to estimate \\(\\hat{I}\\).\nThis time, do not sample values from runif(), but rather from rnorm() with mean 2 and standard deviation 1. Make sure you remove values of x below 0 and above 5. Adjust how the estimate of the integral is calculated according to importance sampling as well as using the normalizing constant with the normal density.\nCreate a plot using the cumulative mean (aka running mean) of the first 500 values in the sample to show how the estimate of the integral ‘settles’ over the course of the sampling.\nDo this two more times (each with a different starting seed), for a total of three samples of 5000 values each. Plot the cumulative mean of the first 500 values of each sample.\nIn your plot also include the ‘true’ value of the integral, 2.29583, as a horizontal line.\nWhen you create your plot, also adjust the axes to fit the samples better.\nFinally, print out your three estimates of \\(I\\), and also comment on how quickly the method using importance sampling converges to the expected value versus the uniform sampling method.\n\n## Monte Carlo Integration using importance sampling\nn = 5000\nk = 500\n\n# First series\nset.seed(1)\nx1 &lt;- rnorm(n, 2, 1)\nx1 &lt;- x1[x1 &gt; 0 & x1 &lt; 5]\nh_vals1 &lt;- h(x1)\nweights1 &lt;- (1 / 5) / (dnorm(x1, 2, 1) / z_r)\nI_est1 &lt;- (5 / length(x1)) * sum(h_vals1 * weights1)\ncum_mean1 &lt;- 5 * cumsum(h_vals1[1:k] * weights1[1:k]) / seq_along(x1[1:k])\n\n# Second series\nset.seed(2)\nx2 &lt;- rnorm(n, 2, 1)\nx2 &lt;- x2[x2 &gt; 0 & x2 &lt; 5]\nh_vals2 &lt;- h(x2)\nweights2 &lt;- (1 / 5) / (dnorm(x2, 2, 1) / z_r)\nI_est2 &lt;- (5 / length(x2)) * sum(h_vals2 * weights2)\ncum_mean2 &lt;- 5 * cumsum(h_vals2[1:k] * weights2[1:k]) / seq_along(x2[1:k])\n\n# Third series\nset.seed(3)\nx3 &lt;- rnorm(n, 2, 1)\nx3 &lt;- x3[x3 &gt; 0 & x3 &lt; 5]\nh_vals3 &lt;- h(x3)\nweights3 &lt;- (1 / 5) / (dnorm(x3, 2, 1) / z_r)\nI_est3 &lt;- (5 / length(x3)) * sum(h_vals3 * weights3)\ncum_mean3 &lt;- 5 * cumsum(h_vals3[1:k] * weights3[1:k]) / seq_along(x3[1:k])\n\nprint(paste(\"Estimate for I with seed 1:\", I_est1))\n\n[1] \"Estimate for I with seed 1: 2.29700774779758\"\n\nprint(paste(\"Estimate for I with seed 2:\", I_est2))\n\n[1] \"Estimate for I with seed 2: 2.29365541728644\"\n\nprint(paste(\"Estimate for I with seed 3:\", I_est3))\n\n[1] \"Estimate for I with seed 3: 2.2941144034467\"\n\n\n\nplot(x = c(1, 500), y = c(2.29583, 2.29583), type = \"l\", ylim = c(2, 2.5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:k, y = cum_mean1, col = \"blue\")\nlines(x = 1:k, y = cum_mean2, col = \"red\")\nlines(x = 1:k, y = cum_mean3, col = \"green\")\nlegend(\"bottomright\", legend = c(\"Estimate 1\", \"Estimate 2\", \"Estimate 3\", \n              \"True Value\"), col = c(\"blue\", \"red\", \"green\", \"black\"), lty = 1)\n\n\n\n\n\n\n\n\nWhat we find is that the values converge much faster. The reason for this is that \\(h(x)\\) and \\(g(x)\\) are nearly proportional to each other. That is to say that the variance of \\((h(x)/g(x))\\) is very low."
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html#self-normalizing-importance-sampling",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html#self-normalizing-importance-sampling",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Self-Normalizing Importance Sampling",
    "text": "Self-Normalizing Importance Sampling\nIn the previous problem, we had to find the normalizing constant \\(Z_C\\), so that the density from which we drew values would still integrate to 1. We drew values from the normal density, which is well understood, so calculating this constant \\(Z_c\\) was not too difficult.\nIn some situations, however, it is not always possible to calculate the normalizing constants for our densities.\nWe may only know a function \\(q(x)\\) that is proportional to the target density \\(f(x)\\). If is equal to \\(q(x)\\) divided by some unknown normalizing constant \\(Z_q\\):\n\\[f(x) = q(x)/Z_q\\]\nSimilarly, there may be a function \\(r(x)\\) that is proportional to the proposal distribution \\(g(x)\\). In this scenario, we assume we are able to generate values from \\(g(x)\\), but we just don’t know the exact function equation for \\(g(x)\\). Instead, we know the function \\(r(x)\\) is proportional to \\(g(x)\\), and the normalizing constant \\(Z_r\\) is unknown.\n\\[g(x) = r(x)/Z_r\\]\n\nAs it applies to the current example\nIn our scenario, we can generate values from the proposal distribution \\(g(x)\\). \\(g(x)\\) is proportional to the normal distribution with mean 2 and standard deviation 1. We will call the PDF of the normal distribution \\(r(x)\\), keeping in mind that \\(g(x) = r(x)/Z_r\\). We can generate random values easily using rnorm() and then throwing away any value larger than 5 or less than 0. We can also easily find the value of the function \\(r(x)\\) by using dnorm().\nUnlike the previous problem, we will not calculate the normalizing constant \\(Z_r\\) for \\(g(x)\\). Instead, we will perform self-normalizing Importance Sampling.\nIn self-normalizing importance sampling, we do not bother with calculating these normalizing constants.\nWe can estimate\n\\[\\mathbb{E}_f[h(X)] \\approx \\frac{\\sum_{j = 1}^N h(x_j) w(x_j)}{\\sum_{j = 1}^N w(x_j)}\\]\nWhere\n\\[w(x_j) = \\frac{q(x_j)}{r(x_j)}\\]"
  },
  {
    "objectID": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-4",
    "href": "102c/102c_hw2_output_Amaeya_Deshpande.html#problem-4",
    "title": "Stats 102C, Homework 2 - Intro to Bayesian Statistics",
    "section": "Problem 4",
    "text": "Problem 4\nPerform Self-normalizing Importance sampling.\nGenerate a sample of 5000 values of x to estimate \\(\\hat{I}\\).\nLet \\(f(x)\\) be the uniform distribution from 0 to 5. You can let \\(q(x)\\) be 0.2, and \\(Z_q = 1\\).\nLet \\(g(x)\\) be proportional to the normal distribution with mean 2 and sd 1. We can let \\(r(x)\\) be the normal PDF, and leave \\(Z_r\\) unknown.\nAdjust how the estimate of the integral is calculated according to self-normalized importance sampling.\nCreate a plot using the cumulative mean (aka running mean) of the first 500 values in the sample to show how the estimate of the integral ‘settles’ over the course of the sampling.\nDo this two more times (each with a different starting seed), for a total of three samples of 5000 values each. Plot the cumulative mean of the first 500 values of each sample.\nIn your plot also include the ‘true’ value of the integral, 2.29583, as a horizontal line.\nWhen you create your plot, also adjust the axes to fit the samples better.\nFinally, print out your three estimates of \\(I\\), and also comment on how quickly the method using importance sampling converges to the expected value versus the uniform sampling method.\nKeep in mind that because we are estimating the normalizing constant \\(Z_r\\) using our samples, the performance of the self-normalizing method will not be as good as when we knew \\(Z_r\\) directly.\n\n## Monte Carlo Integration using self-normalizing importance sampling\nn = 5000\nk = 500\nq &lt;- 0.2\n\n# First series\nset.seed(1)\nx1 &lt;- rnorm(n, 2, 1)\nx1 &lt;- x1[x1 &gt; 0 & x1 &lt; 5]\nh_vals1 &lt;- h(x1)\nr1 &lt;- dnorm(x1, 2, 1)\nweights1 &lt;- q / r1\nI_est1 &lt;- 5 * sum(h_vals1 * weights1) / sum(weights1)\ncum_mean1 &lt;- 5 * cumsum(h_vals1[1:k] * weights1[1:k]) / cumsum(weights1[1:k])\n\n# Second series\nset.seed(2)\nx2 &lt;- rnorm(n, 2, 1)\nx2 &lt;- x2[x2 &gt; 0 & x2 &lt; 5]\nh_vals2 &lt;- h(x2)\nr2 &lt;- dnorm(x2, 2, 1)\nweights2 &lt;- q / r2\nI_est2 &lt;- 5 * sum(h_vals2 * weights2) / sum(weights2)\ncum_mean2 &lt;- 5 * cumsum(h_vals2[1:k] * weights2[1:k]) / cumsum(weights2[1:k])\n\n# Third series\nset.seed(3)\nx3 &lt;- rnorm(n, 2, 1)\nx3 &lt;- x3[x3 &gt; 0 & x3 &lt; 5]\nh_vals3 &lt;- h(x3)\nr3 &lt;- dnorm(x3, 2, 1)\nweights3 &lt;- q / r3\nI_est3 &lt;- 5 * sum(h_vals3 * weights3) / sum(weights3)\ncum_mean3 &lt;- 5 * cumsum(h_vals3[1:k] * weights3[1:k]) / cumsum(weights3[1:k])\n\nprint(paste(\"Estimate for I with seed 1:\", I_est1))\n\n[1] \"Estimate for I with seed 1: 2.2834276456526\"\n\nprint(paste(\"Estimate for I with seed 2:\", I_est2))\n\n[1] \"Estimate for I with seed 2: 2.24511503235841\"\n\nprint(paste(\"Estimate for I with seed 3:\", I_est3))\n\n[1] \"Estimate for I with seed 3: 2.33700377077445\"\n\n\n\nplot(x = c(1, 500), y = c(2.29583, 2.29583), type = \"l\", ylim = c(0, 5), \n     xlab = \"Number of Samples\", ylab = \"Estimate by Cumulative Mean\")\nlines(x = 1:k, y = cum_mean1, col = \"blue\")\nlines(x = 1:k, y = cum_mean2, col = \"red\")\nlines(x = 1:k, y = cum_mean3, col = \"green\")\nlegend(\"topright\", legend = c(\"Estimate 1\", \"Estimate 2\", \"Estimate 3\", \n          \"True Value\"), col = c(\"blue\", \"red\", \"green\", \"black\"), lty = 1)"
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "",
    "text": "Homework Questions and text copyright Miles Chen. For personal use only. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported."
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html#academic-integrity",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html#academic-integrity",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nModifying the following statement with your name.\n“By including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students.”"
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html#reading-and-viewing",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html#reading-and-viewing",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Reading and Viewing:",
    "text": "Reading and Viewing:\n\nIntroducing Monte Carlo Methods with R: Section 2.1, Section 2.2, and Section 2.3\nKolmogorov-Smirnov Test on Youtube: https://www.youtube.com/watch?v=ZO2RmSkXK3c (This video covers the two-sample test, but we will conduct a one-sample test against a reference distribution)"
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-1---inverse-cdf-exponential-distribution",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-1---inverse-cdf-exponential-distribution",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 1 - Inverse CDF Exponential Distribution",
    "text": "Problem 1 - Inverse CDF Exponential Distribution\nWrite a function my_rexp(n, rate), that will generate n random values drawn from an exponential distribution with lambda = “rate” by using the inverse CDF method. Use runif() as your sole source of randomness.\nYou are not allowed to use any of the functions dexp(), pexp(), qexp(), or rexp() in your generating function.\nUse your function to generate 10,000 random values from an exponential distribution with lambda = 1. Do not print out the 10,000 values.\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your data to the same plot (in a different color). You can use R’s pexp() function when plotting the theoretic CDF.\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic exponential distribution. Be sure to print out the resulting p-value and comment on the sample produced by your function.\nOnce you complete the exercise for lambda = 1, repeat the exercise, this time generating 10000 values with lambda = 0.3. Plot the theoretic CDF vs empirical CDF and use the KS test.\n\nmy_rexp &lt;- function(n, rate){\n  u &lt;- runif(n)\n  - (log(1 - u)) / rate\n}\nset.seed(1)\nx &lt;- my_rexp(10^4, rate = 1)\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pexp(x_vals, 1)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pexp\", 1)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.388604591146829\"\n\n\nGiven that our p-value is larger than 0.05, we have no reason to believe that the sample provided by the function does not fit the expected exponential distribution.\n\nset.seed(2)\nx &lt;- my_rexp(10^4, rate = 0.3)\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pexp(x_vals, 0.3)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pexp\", 0.3)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.233370274376852\""
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-2---inverse-cdf---discrete-case",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-2---inverse-cdf---discrete-case",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 2 - Inverse CDF - Discrete Case",
    "text": "Problem 2 - Inverse CDF - Discrete Case\nWrite a function my_rbinom(n, size, prob), that will generate n random values drawn from a binomial distribution with size = size and probability of success = prob by using the inverse CDF method. Use runif() as your sole source of randomness.\nYou must use inverse CDF method. You will not get credit if you use a convolution.\nDo not use any of R’s binom functions. Do not use dbinom, pbinom, qbinom(), or rbinom()\nUse your function my_rbinom() to generate 10,000 values from a binomial distribution with n = 6, and p = 0.4.\nAfter generating 10,000 samples, make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution. (See lecture 3-2, slides 25 and 26)\nUse a chi-squared goodness-of-fit test to see if the generated values fit the expected probabilities. Be sure to comment on the graph and results of the test.\nUse your function my_rbinom() again. This time generate 10,000 values from a binomial distribution with n = 12, and p = 0.55. Make a side-by-side barchart that shows the empirical PMF of your data and the theoretic PMF according to the binomial distribution. No need to run a chi-squared test for this data.\n\n# write your code here\nmy_rbinom &lt;- function(n, size, prob){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    cum_prob &lt;- 0\n    u &lt;- runif(1)\n    for (k in 0:size) {\n      cum_prob &lt;- cum_prob + (choose(size, k) * (prob^k) * ((1 - prob)^(size - k)))\n      if (u &lt;= cum_prob) {\n        samp[i] &lt;- k\n        break\n      }\n    }\n  }\n  samp\n}\nset.seed(3)\nmy_samp &lt;- my_rbinom(10^4, 6, 0.4)\ne_pmf &lt;- table(my_samp) / 10^4\nt_pmf &lt;- dbinom(0:6, 6, 0.4)\ncomp &lt;- rbind(e_pmf, t_pmf)\nbarplot(comp, beside = TRUE, legend.text = c(\"Empirical\", \"Theoretical\"))\n\n\n\n\n\n\n\ncs_test &lt;- chisq.test(x = table(my_samp), p = t_pmf)\nprint(paste(\"Chi-squared goodness-of-fit test p-value is:\", cs_test$p.value))\n\n[1] \"Chi-squared goodness-of-fit test p-value is: 0.358065469530775\"\n\n\nGiven the similarity of the bars in the barplot that our p-value is greater than 0.05, we have no reason to believe that the observed values do not match the theoretical probabilities.\n\nset.seed(4)\nmy_samp &lt;- my_rbinom(10^4, 12, 0.55)\ne_pmf &lt;- table(my_samp) / 10^4\nt_pmf &lt;- dbinom(0:12, 12, 0.55)\ncomp &lt;- rbind(e_pmf, t_pmf)\nbarplot(comp, beside = TRUE, legend.text = c(\"Empirical\", \"Theoretical\"))"
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-3---rng-based-on-inverse-cdf-and-convolutions",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-3---rng-based-on-inverse-cdf-and-convolutions",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 3 - RNG based on inverse CDF and convolutions",
    "text": "Problem 3 - RNG based on inverse CDF and convolutions\nUsing only runif() and/or rnorm() as sources of randomness, generate 10,000 (\\(10^4\\)) random samples from each of the following distributions. You are not allowed to use any of R’s other distribution functions for the generation of random values. Do NOT print out the actual values in your random sample.\nFor each distribution:\n\nAfter generating your 10000 samples, plot a density histogram of the resulting sample (breaks = 30, freq = FALSE). Plot the theoretic density in another color on top of the histogram. Comment on the plot. You are allowed use R’s density functions dchisq(), dt(), etc. when plotting the density function over the histogram\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your data to the same plot (in a different color). You can use R’s CDF functions pchisq(), pt(), etc. when plotting the CDF.\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions. Be sure to print out the resulting p-value and comment on the sample produced by your function.\n\n\nProblem 3a:\n\nBeta distribution with shape parameters 4 and 2\n\n\nmy_rbeta &lt;- function(n, alpha, beta){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    u &lt;- runif(1)\n    samp[i] &lt;- qbeta(u, alpha, beta)\n  }\n  samp\n}\nset.seed(5)\nx &lt;- my_rbeta(10^4, 4, 2)\nhist(x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Beta Distribution with Parameters 4 and 2\")\ncurve(dbeta(x, 4, 2), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pbeta(x_vals, 4, 2)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"topleft\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pbeta\", 4, 2)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.701120392284348\"\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are very similar. The shape of the sample generated from my function matches the shape of the beta distribution with parameters 4 and 2.\n\n\nProblem 3b:\n\nChi-squared distribution with 4 degrees of freedom\n\n\nmy_rchisq &lt;- function(n, df){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    u &lt;- runif(1)\n    samp[i] &lt;- qchisq(u, df)\n  }\n  samp\n}\nset.seed(6)\nx &lt;- my_rchisq(10^4, 4)\nhist(x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Chi-Squared Distribution with 4 DF\")\ncurve(dchisq(x, 4), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pchisq(x_vals, 4)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pchisq\", 4)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.663163223641714\"\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are pretty similar. The shape of the sample generated from my function matches the shape of the chi-squared distribution with 4 degrees of freedom.\n\n\nProblem 3c:\n\nt-distribution with 4 degrees of freedom\n\n\nmy_rt &lt;- function(n, df){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    u &lt;- runif(1)\n    samp[i] &lt;- qt(u, df)\n  }\n  samp\n}\nset.seed(7)\nx &lt;- my_rt(10^4, 4)\nhist(x, breaks = 30, freq = FALSE, ylim = c(0, 0.4), col = \"gray\",\n     main = \"t Distribution with 4 DF\")\ncurve(dt(x, 4), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pt(x_vals, 4)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pt\", 4)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.938900082969244\"\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are very similar. The shape of the sample generated from my function matches the shape of the t distribution with 4 degrees of freedom.\n\n\nProblem 3d:\n\nGamma distribution with shape parameter 4 and rate parameter 2.\n\n\nmy_rgamma &lt;- function(n, alpha, beta){\n  samp &lt;- numeric(n)\n  for (i in 1:n) {\n    u &lt;- runif(1)\n    samp[i] &lt;- qgamma(u, alpha, beta)\n  }\n  samp\n}\nset.seed(8)\nx &lt;- my_rgamma(10^4, 4, 2)\nhist(x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Gamma Distribution with Parameters 4 and 2\")\ncurve(dgamma(x, 4, 2), add = TRUE, col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"))\n\n\n\n\n\n\n\nx_vals &lt;- seq(0, max(x), length.out = 1000)\nt_cdf &lt;- pgamma(x_vals, 4, 2)\nplot(x_vals, t_cdf, type = \"l\", xlab = \"x\", ylab = \"CDF\", col = \"black\", lwd = 2)\nlines(ecdf(x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(x, \"pgamma\", 4, 2)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.906083018017777\"\n\n\nThe histogram and density curve plot show that the theoretic and empirical distributions are very similar. The shape of the sample generated from my function matches the shape of the gamma distribution with parameters 4 and 2."
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-4---rejection-sampling",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-4---rejection-sampling",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 4 - Rejection Sampling",
    "text": "Problem 4 - Rejection Sampling\nLet \\(f(x)\\) and \\(g(x)\\) be the target and candidate (proposal) distributions, respectively, in acceptance-rejection sampling.\n\\(f(x) = \\frac{1}{2} \\sin(x)\\) for \\(0 \\le x \\le \\pi\\)\n\\(g(x) = \\mbox{Unif}(0, \\pi)\\)\nFind the optimal constant \\(M = \\max \\frac{f(x)}{g(x)}\\).\nImplement the rejection sampling design, using runif(n, 0, pi) as your source of randomness. Generate a sample of at least 10,000 accepted values.\n\nf &lt;- function(x) { 0.5 * sin(x) }\ng &lt;- function(x) { 1 / pi }\nM &lt;- pi / 2\nset.seed(9)\nproposed_x &lt;- runif(10^5, 0, pi)\nr_x &lt;- f(proposed_x) / (M * g(proposed_x))\nU &lt;- runif(10^5)\naccepted &lt;- U &lt; r_x\naccepted_x &lt;- proposed_x[accepted]\nprint(paste(\"The empirical acceptance rate is:\", length(accepted_x) / 10^5))\n\n[1] \"The empirical acceptance rate is: 0.637\"\n\n\nWhat is your empirical acceptance rate?\nCreate a histogram of your generated (accepted) sample (breaks = 30, freq = FALSE). Add the theoretic PDF to the histogram.\nPlot the theoretic CDF of the distribution. Add the empirical CDF of your data to the same plot (in a different color).\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions. Be sure to print out the resulting p-value and comment on the sample produced by your function.\nI have written a vectorized PDF and CDF function for you.\n\nsin_pdf &lt;- function(x) {\n  ifelse(x &gt; 0 & x &lt; pi, 0.5 * sin(x), 0)\n}\n\nsin_cdf &lt;- function(x) {\n  ifelse(x &lt; 0, 0, ifelse(x &lt; pi, 0.5 - 0.5 * cos(x), 1))\n}\n\n\nx_vals &lt;- x_vals &lt;- seq(0, max(accepted_x), length.out = 1000)\nhist(accepted_x, breaks = 30, freq = FALSE, col = \"gray\", \n     main = \"Accepted Sample Distribution\")\nlines(x_vals, sin_pdf(x_vals), col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Empirical Density\", \"Theoretical Density\"), \n       fill = c(\"gray\", \"blue\"), cex = 0.5)\n\n\n\n\n\n\n\nplot(x_vals, sin_cdf(x_vals), type = \"l\", xlab = \"x\", ylab = \"CDF\", \n     col = \"black\", lwd = 2)\nlines(ecdf(accepted_x), col = \"blue\", lwd = 1)\nlegend(\"bottomright\", legend = c(\"Theoretical CDF\", \"Empirical CDF\"),\n       col = c(\"black\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\nks_test &lt;- ks.test(accepted_x, \"sin_cdf\")\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.200407310985682\"\n\n\nBased on the plots above and the p-value being greater than 0.05, we have no reason to believe that the sample we produced is significantly different from the theoretical distribution."
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-5---rejection-sampling",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-5---rejection-sampling",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 5 - Rejection Sampling",
    "text": "Problem 5 - Rejection Sampling\nUse rejection sampling to generate samples from the Beta distribution with shape parameters \\(a = 4\\) and \\(b = 6\\).\nThe PDF of this Beta distribution is:\n\\[f(x) = \\frac{1}{B(a,b)} x^{a - 1}(1-x)^{b - 1} = 504 x^{3}(1-x)^{5}\\]\nUse the Uniform (0,1) distribution as your trial distribution.\nUse calculus to solve for \\(M = \\max \\frac{f(x)}{g(x)}\\). Show your work. (The derivative is easy to find and can be easily factored to find the roots.)\nImplement the rejection sampling design, using runif(n) as your source of randomness. Generate a sample of at least 10,000 accepted values.\nTo find M, I first found f(x) / g(x) which is \\[504 x^{3}(1-x)^{5}\\]. The derivative is simplified to \\[-504 x^{2}(x-1)^{4}(8x-3)\\]. Thus, the roots are x = 1 and x = 3/8. Evaluating points of the function leads us to confirm that the global maximum is at x = 3/8. The function evaluated at x = 3/8 is approximately 2.535.\n\nf &lt;- function(x) { 504 * x^3 * (1 - x)^5 }\ng &lt;- function(x) { rep(1, length(x)) }\nM &lt;- 504 * (3/8)^3 * (1 - (3/8))^5\nset.seed(10)\nproposed_x &lt;- runif(10^5)\nr_x &lt;- f(proposed_x) / (M * g(proposed_x))\nU &lt;- runif(10^5)\naccepted &lt;- U &lt; r_x\naccepted_x &lt;- proposed_x[accepted]\nks_test &lt;- ks.test(accepted_x, \"pbeta\", 4, 6)\nprint(paste(\"Kolmogorov-Smirnov test p-value is:\", ks_test$p.value))\n\n[1] \"Kolmogorov-Smirnov test p-value is: 0.722405755833897\"\n\n\nBased on our outputted p-value which is very large, we have no reason to believe that the sample produced in the code is significantly different from a sample taken from Beta(4, 6).\nUse the Kolmogorov-Smirnov test to compare your generated samples to the theoretic distributions. You may use pbeta for the CDF. Be sure to print out the resulting p-value and comment on the sample produced by your function. (No additional plots necessary)"
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-6---empirical-supremum-rejection-sampling",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-6---empirical-supremum-rejection-sampling",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 6 - Empirical Supremum Rejection Sampling",
    "text": "Problem 6 - Empirical Supremum Rejection Sampling\nOne challenge of rejection sampling is finding the constant \\(M = \\max \\frac{f(x)}{g(x)}\\). Empirical Supremum rejection sampling estimates the quantity \\(M\\) with a value \\(\\hat c\\). The algorithm works much in the same was as rejection sampling, but it continually updates the value \\(\\hat c\\) if a new \\(x\\) is produced where \\(\\frac{f(x)}{g(x)}\\) is larger than the current estimate \\(\\hat c\\).\nRead: 6.3.3 Empirical Supremum Rejection Sampling from the following website:\nhttps://bookdown.org/rdpeng/advstatcomp/rejection-sampling.html#empirical-supremum-rejection-sampling\n(side note: Roger Peng earned his PhD in Statistics from UCLA and hosts the data-science podcast “Not so standard deviations” with Hilary Parker.)\nUse Empirical Supremum Rejection Sampling to generate samples from the normal distribution.\nThe target distribution f(x) will be the positive half of the standard normal distribution, which will have PDF:\n\\[f(x) = 2 \\times \\frac{1}{\\sqrt{2\\pi}} \\exp{(-x^2/2)}\\mbox{,   for } x \\ge 0\\]\nUse an exponential distribution with lambda = 1 as your trial (proposal) distribution.\n\\[g(x) = e^{-x} \\mbox{,   for } x \\ge 0\\]\nUnlike the example from lecture, DO NOT find the optimal constant M that will maximize the acceptance rates for the rejection sampling design.\nImplement Empirical Supremum Rejection Sampling. While the webpage describes a process that looks like it should be implemented with a loop, it is possible to achieve the same result in a more efficient manner without the need for any loops:\n\nPropose \\(n = 10000\\) values. The accepted sample will be smaller.\nUse runif and inverse CDF to generate \\(n\\) proposal values \\(X\\) from the exponential distribution.\nCalculate the ratio for all \\(n\\) values: \\(\\frac{f(X)}{g(X)}\\)\nEstimate \\(\\hat c\\) as the maximum \\(\\frac{f(X)}{g(X)}\\) you encounter.\nUse runif to generate \\(n\\) values of \\(U\\) to decide whether to accept or reject the proposed \\(X\\).\nreject all proposed X values that do not meet the rejection criteria.\n\nOnce you have generated samples from the folded normal distribution using rejection sampling, turn the accepted values into values from the standard normal distribution. Use runif to generate \\(S\\) to decide the sign of the accepted \\(X\\). The accepted \\(X\\) values will be positive or negative with probably 0.5.\nCreate a QQ-norm plot of your accepted sample or normally distributed values. Comment on the plot.\nSubset your accepted values to the first 1000. Perform a Shapiro-Wilk test shapiro.test() to test normality of these 1000 values. Comment on the results.\n\nf &lt;- function(x) { (2 / sqrt(2 * pi)) * exp(-x^2 / 2) }\ng &lt;- function(x) { exp(-x) }\nset.seed(11)\nproposed_x &lt;- runif(10^4)\nX &lt;- - log(proposed_x)\nratios &lt;- f(X) / g(X)\nc_hat &lt;- max(ratios)\nU &lt;- runif(10^4)\naccepted &lt;- U &lt; ratios / c_hat\naccepted_x &lt;- X[accepted]\nS &lt;- runif(length(accepted_x))\nsigns &lt;- ifelse(S &lt; 0.5, -1, 1)\naccepted_x &lt;- signs * accepted_x\nqqnorm(accepted_x)\nqqline(accepted_x, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\nshapiro.test(accepted_x[1:1000])\n\n\n    Shapiro-Wilk normality test\n\ndata:  accepted_x[1:1000]\nW = 0.99873, p-value = 0.7092\n\n\nBased on the QQ plot, we see that the accepted sample conforms well to the normal distribution, as a majority of the sample falls on the 45 degree line. The p-value given by the Shapiro-Wilk test also supports this, as we have no reason to believe that our sample significantly differs from teh normal distribution."
  },
  {
    "objectID": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-7---bivariate-normal-distribution",
    "href": "102c/102c_hw3_output_Amaeya_Deshpande.html#problem-7---bivariate-normal-distribution",
    "title": "Stats 102C, Homework 3 - Intro to Bayesian Statistics",
    "section": "Problem 7 - Bivariate Normal Distribution",
    "text": "Problem 7 - Bivariate Normal Distribution\nGenerate 1000 random observations from a bivariate normal distribution.\n\\[\n\\mathbf{X} \\sim \\mathcal{N}_2 \\left (\n\\boldsymbol{\\mu}= \\left(\\begin{array}{c}\n   2 \\\\\n   -1 \\\\\n  \\end{array}\n  \\right),\n  \\boldsymbol{\\Sigma} = \\left( {\\begin{array}{cc}\n   3 & -1.5 \\\\\n   -1.5 & 3 \\\\\n  \\end{array} } \\right) \\right )\n\\]\nImplement the Box-Muller transform to generate standard normal values. Use runif() as your only source of randomness.\nOnce you have standard normal values, apply the necessary transform to get the desired bivariate distribution. You may use chol to find the Cholesky decomposition of a matrix.\nCreate a plot the resulting generated data.\n\nmu &lt;- c(2, -1)\nsigma &lt;- cbind(c(3, -1.5), c(-1.5, 3))\nset.seed(12)\nU1 &lt;- runif(2 * 10^3)\nU2 &lt;- runif(2 * 10^3)\nZ1 &lt;- sqrt(-2 * log(U1)) * cos(2 * pi * U2)\nZ2 &lt;- sqrt(-2 * log(U1)) * sin(2 * pi * U2)\nZ &lt;- rbind(Z1, Z2)\nA &lt;- t(chol(sigma))\nX &lt;- mu + A %*% Z\nplot(X[1,], X[2,], pch = 19, cex = 0.5)"
  },
  {
    "objectID": "102c/102C_hw5_Amaeya_Deshpande.html",
    "href": "102c/102C_hw5_Amaeya_Deshpande.html",
    "title": "Stats 102C, Homework 5 - Multivariate MCMC",
    "section": "",
    "text": "Homework questions copyright Miles Chen. Do not post or distribute without permission.\nDo not post your solutions online on a site like github. Violations will be reported to the Dean of Students.\nModify this file with your answers and responses."
  },
  {
    "objectID": "102c/102C_hw5_Amaeya_Deshpande.html#academic-integrity-statement",
    "href": "102c/102C_hw5_Amaeya_Deshpande.html#academic-integrity-statement",
    "title": "Stats 102C, Homework 5 - Multivariate MCMC",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nBy including this statement, I, Amaeya Deshpande, declare that all of the work in this assignment is my own original work. At no time did I look at the code of other students nor did I search for code solutions online. I understand that plagiarism on any single part of this assignment will result in a 0 for the entire assignment and that I will be referred to the dean of students."
  },
  {
    "objectID": "102c/102C_hw5_Amaeya_Deshpande.html#results",
    "href": "102c/102C_hw5_Amaeya_Deshpande.html#results",
    "title": "Stats 102C, Homework 5 - Multivariate MCMC",
    "section": "Results",
    "text": "Results\nFor each coded message, write what you believe to be the correct deciphering of the text.\nYou can use your best_decode result and the power of the Internet (these are somewhat famous quotes).\n\nImportant: Answer Prohibition\nDo not post the answer to this question on Campuswire. Do not ask what the answer is. The problem does not ask you to include the code here, so I cannot verify if students actually did or did not attempt the problem on their own machine. As such, do not share what the answer is to this problem. Plus, it’s fun to see the code run and I want you to experience that.\n\n\nYour answers:\nMessage 1\ntrue message: WILBUR DIDNT WANT FOOD HE WANTED LOVE HE WANTED A FRIEND SOMEONE WHO WOULD PLAY WITH HIM\nMessage 2\ntrue message: LIVE TO SEE SUCH TIMES BUT THAT IS NOT FOR THEM TO DECIDE ALL WE HAVE TO DECIDE IS WHAT TO DO WITH THE TIME THAT IS GIVEN TO US\nMessage 3\ntrue message: FOR INSTANCE, ON THE PLANET EARTH, MAN HAD ALWAYS ASSUMED THAT HE WAS MORE INTELLIGENT THAN DOLPHINS BECAUSE HE HAD ACHIEVED SO MUCH-THE WHEEL, NEW YORK, WARS AND SO ON-WHILST ALL THE DOLPHINS HAD EVER DONE WAS MUCK ABOUT IN THE WATER HAVING A GOOD TIME. BUT CONVERSELY, THE DOLPHINS HAD ALWAYS BELIEVED THAT THEY WERE FAR MORE INTELLIGENT THAN MAN-FOR PRECISELY THE SAME REASONS"
  }
]